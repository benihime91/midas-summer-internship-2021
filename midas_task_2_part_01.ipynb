{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "midas-task-2-part-01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1lEHy0eoIwCS_z-iehTA5vlq6MLntmxXm",
      "authorship_tag": "ABX9TyMBlFe/6LN9vXaIBjuoqVGO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benihime91/midas-summer-internship-2021/blob/main/midas_task_2_part_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjEbaEVaWQj9"
      },
      "source": [
        "# > Uncomment and run this cell if running on Google Colab\n",
        "# install required dependencies for google colab\n",
        "!git clone https://github.com/benihime91/midas-summer-internship-2021.git\n",
        "!pip install --upgrade -r \"/content/midas-summer-internship-2021/requirements.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxmvU0RruGhb"
      },
      "source": [
        "# > for Goggle Colab\n",
        "import sys\n",
        "sys.path.append(\"midas-summer-internship-2021/\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIttbuJAV3vv"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02qYzvKbV8fx"
      },
      "source": [
        "# Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCkK2vzKWGkW"
      },
      "source": [
        "## Part - 1\n",
        "> Use this dataset (https://www.dropbox.com/s/pan6mutc5xj5kj0/trainPart1.zip) to train a CNN. Use no other data source or pretrained networks, and explain your design choices during preprocessing, model building and training. Also, cite the sources you used to borrow techniques. A test set will be provided later to judge the performance of your classifier. Please save your model checkpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asp5xGW_WMhz"
      },
      "source": [
        "### Getting the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtkjtbGpWrQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "544ce8b3-9d6a-478c-da05-39d67d7cac42"
      },
      "source": [
        "import os\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Download the 1st Dataset using :\n",
        "!wget -P \"data/\" https://www.dropbox.com/s/pan6mutc5xj5kj0/trainPart1.zip\n",
        "!unzip --qq \"data/trainPart1.zip\" -d \"data/\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-06 17:42:36--  https://www.dropbox.com/s/pan6mutc5xj5kj0/trainPart1.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6022:18::a27d:4212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/pan6mutc5xj5kj0/trainPart1.zip [following]\n",
            "--2021-04-06 17:42:36--  https://www.dropbox.com/s/raw/pan6mutc5xj5kj0/trainPart1.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uca4c5214e6865fc9e2c0d85e8de.dl.dropboxusercontent.com/cd/0/inline/BMEHYdQKsFO-i5-wraze5u42byX3O819TUTCbRDrocjVIHYBHj2KlpBJwSxdc4gkY7OtOFWkQQeeknJTkOxjal9Y4oJGYBfoBxy9e3VCtsm171acLox3wdinfJxynTPZApQdQO_6IjSylcDVwsNSpr4x/file# [following]\n",
            "--2021-04-06 17:42:36--  https://uca4c5214e6865fc9e2c0d85e8de.dl.dropboxusercontent.com/cd/0/inline/BMEHYdQKsFO-i5-wraze5u42byX3O819TUTCbRDrocjVIHYBHj2KlpBJwSxdc4gkY7OtOFWkQQeeknJTkOxjal9Y4oJGYBfoBxy9e3VCtsm171acLox3wdinfJxynTPZApQdQO_6IjSylcDVwsNSpr4x/file\n",
            "Resolving uca4c5214e6865fc9e2c0d85e8de.dl.dropboxusercontent.com (uca4c5214e6865fc9e2c0d85e8de.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6022:15::a27d:420f\n",
            "Connecting to uca4c5214e6865fc9e2c0d85e8de.dl.dropboxusercontent.com (uca4c5214e6865fc9e2c0d85e8de.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BME4ogLJ_dRvL1GI2dgtS_k2b_5eAGzjus-ssVOl6wNOOrWdH7PMhNa4MFsLzE4c0I1pPRCq9QFfkx6lkqHMCvf3bgxMiSuuhBQs4MScx5IwV3oYFEwWr70CKFolvDp9Bb6OyEEpLus7RsitqUM3a5MmKrCze_dKWXXF5dXdBIxIwOjyb-gJ4kVoq_qiKN_PvgOfV_Eq0O2gBFlczSWcn2k3ra7tknpVxoL_0_TGvp6lWbJcHhHZwiVnBeFqQ0zLaxZr60-DHn9TQxm9KJ2xAut34lFWCktkdJW9BfSiguThzqKByCIUx9WJemTnlC_s_SkC0PI1AKOc0DKxUH7dyuo6xsGR2aTjHFdPbzs0Aa8VniQZbbHKEPNJBtapGCS40II/file [following]\n",
            "--2021-04-06 17:42:37--  https://uca4c5214e6865fc9e2c0d85e8de.dl.dropboxusercontent.com/cd/0/inline2/BME4ogLJ_dRvL1GI2dgtS_k2b_5eAGzjus-ssVOl6wNOOrWdH7PMhNa4MFsLzE4c0I1pPRCq9QFfkx6lkqHMCvf3bgxMiSuuhBQs4MScx5IwV3oYFEwWr70CKFolvDp9Bb6OyEEpLus7RsitqUM3a5MmKrCze_dKWXXF5dXdBIxIwOjyb-gJ4kVoq_qiKN_PvgOfV_Eq0O2gBFlczSWcn2k3ra7tknpVxoL_0_TGvp6lWbJcHhHZwiVnBeFqQ0zLaxZr60-DHn9TQxm9KJ2xAut34lFWCktkdJW9BfSiguThzqKByCIUx9WJemTnlC_s_SkC0PI1AKOc0DKxUH7dyuo6xsGR2aTjHFdPbzs0Aa8VniQZbbHKEPNJBtapGCS40II/file\n",
            "Reusing existing connection to uca4c5214e6865fc9e2c0d85e8de.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10059590 (9.6M) [application/zip]\n",
            "Saving to: ‘data/trainPart1.zip’\n",
            "\n",
            "trainPart1.zip      100%[===================>]   9.59M  12.5MB/s    in 0.8s    \n",
            "\n",
            "2021-04-06 17:42:38 (12.5 MB/s) - ‘data/trainPart1.zip’ saved [10059590/10059590]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMBr1LfVXSEW"
      },
      "source": [
        "### Analyzing the Dataset\n",
        "> In this part we will analyze the get familiar with the given dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSkpCrcLXaRp"
      },
      "source": [
        "# imports\n",
        "import os\n",
        "import random\n",
        "from typing import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from fastcore.all import *\n",
        "from torchvision.datasets.folder import IMG_EXTENSIONS\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqblcJPnXd7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fca5272-f760-45d5-97c3-63ab33f19c6c"
      },
      "source": [
        "DATASET_01_PATH = Path(\"data/train/\")\n",
        "DATASET_01_PATH.ls()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#62) [Path('data/train/Sample033'),Path('data/train/Sample025'),Path('data/train/Sample053'),Path('data/train/Sample026'),Path('data/train/Sample050'),Path('data/train/Sample044'),Path('data/train/Sample035'),Path('data/train/Sample060'),Path('data/train/Sample024'),Path('data/train/Sample020')...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g4-FfRrXf_i"
      },
      "source": [
        "Perform some one-off data manipulations to get all the files and filenames in an easy to use format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcAjvjblXn9Q"
      },
      "source": [
        "def folder2df(directory: Union[str, Path], extensions: list = IMG_EXTENSIONS,\n",
        "              shuffle: bool = False, seed: int = 42):\n",
        "    \"\"\"\n",
        "    Parses all the Images in `directory` and puts them in a `DataFrame` object.\n",
        "    \"\"\"\n",
        "\n",
        "    random.seed(seed)\n",
        "\n",
        "    image_list = L()\n",
        "    target_list = L()\n",
        "\n",
        "    if not isinstance(directory, Path):\n",
        "        directory = Path(directory)\n",
        "\n",
        "    for label in directory.ls():\n",
        "        label = Path(label)\n",
        "        if os.path.isdir(label):\n",
        "            for img in label.ls():\n",
        "                if str(img).lower().endswith(extensions):\n",
        "                    image_list.append(img)\n",
        "                    target_list.append(str(label).split(os.path.sep)[-1])\n",
        "\n",
        "    print(f\"Found {len(image_list)} files belonging to {len(set(target_list))} classes.\")\n",
        "\n",
        "    dataframe: pd.DataFrame = pd.DataFrame()\n",
        "    dataframe[\"image_id\"] = image_list.map(str)\n",
        "    dataframe[\"target\"] = target_list\n",
        "    if shuffle:\n",
        "        dataframe = (dataframe.sample(frac=1, random_state=seed)\n",
        "                     .reset_index(inplace=False, drop=True))\n",
        "    return dataframe"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDgGDQ_fivrx"
      },
      "source": [
        "Create the a `pandas Dataframe` which contains all the file info we need to get started"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI-aXyU-XpbQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "b9a444b2-e000-41f2-9d05-beca359ea97d"
      },
      "source": [
        "DATASET_01 = folder2df(directory=DATASET_01_PATH, shuffle=True)\n",
        "DATASET_01.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2480 files belonging to 62 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/train/Sample004/img004-001.png</td>\n",
              "      <td>Sample004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/train/Sample035/img035-004.png</td>\n",
              "      <td>Sample035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/train/Sample043/img043-008.png</td>\n",
              "      <td>Sample043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/train/Sample046/img046-005.png</td>\n",
              "      <td>Sample046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/train/Sample025/img025-050.png</td>\n",
              "      <td>Sample025</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              image_id     target\n",
              "0  data/train/Sample004/img004-001.png  Sample004\n",
              "1  data/train/Sample035/img035-004.png  Sample035\n",
              "2  data/train/Sample043/img043-008.png  Sample043\n",
              "3  data/train/Sample046/img046-005.png  Sample046\n",
              "4  data/train/Sample025/img025-050.png  Sample025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXDt8fOwiyoA"
      },
      "source": [
        "Lets look closer at the data, how many `class_ids` do we have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmIk-P2AXrbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06696ecb-229e-4a34-8b4d-4932e10c83c4"
      },
      "source": [
        "unq_cls = DATASET_01.target.unique()\n",
        "tot_itm = len(DATASET_01)\n",
        "print(\"Total number of Images in the Dataset: \", tot_itm)\n",
        "print(\"Number of unique classes in the Dataset: \",len(unq_cls))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of Images in the Dataset:  2480\n",
            "Number of unique classes in the Dataset:  62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD9BffaOXtT2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "outputId": "2c7e600a-1f08-4f7e-9705-69948a2c004b"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "_, ax = plt.subplots(1, 1, figsize=(22, 8))\n",
        "sns.countplot(data=DATASET_01, x=\"target\", ax=ax);\n",
        "plt.xticks(rotation=90);"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABPsAAAIOCAYAAADOawinAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAA6GElEQVR4nO3deZhsZ1ku/PvJgIDoSZDNmITw4YgDEbeAEwqKAgoEwqho9IBRnBU9etTzCcfhqJ8CgohfZIqCIiYMYTZiFBUZQgghgAgoo4FsFARBkeE9f1Rt6N7unXRXr+p+e72/33XVtatWdd1117BWVT+7ula11gIAAAAA7H/H7XUBAAAAAGAahn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwEyfsdYGtuMENbtBOP/30va4BAAAAAF149atf/b7W2oEjl++LYd/pp5+eSy65ZK9rAAAAAEAXqurtR1vuz3gBAAAAYCYM+wAAAABgJgz7AAAAAGAmDPsAAAAAYCYM+wAAAABgJgz7AAAAAGAmDPsAAAAAYCYM+wAAAABgJgz7AAAAAGAmDPsAAAAAYCYM+wAAAABgJgz7AAAAAGAmDPsAAAAAYCYM+wAAAABgJgz7AAAAAGAm1j7sq6rjq+o1VfW85elbVNUrquotVfXHVXWtdXcAAAAAgBHsxif7fjTJGzec/rUkj2qtfW6S9yd58C50AAAAAIDZW+uwr6pOSfKtSZ6wPF1J7pTk/OWPnJfkzHV2AAAAAIBRnLDm/Ecn+R9JPmt5+nOSfKC19vHl6XcludnRLlhV5yQ5J0lOO+20JMmhxz91R2UOPPRBm04f+t0n7Czv+x+y6fRVv/uYlbNu+P0/sun0ex//yytnJcmNHvpzm07/0+N+ckd5N/3B39h0+p2P/fYd5Z36w3+46fTf//Y9d5T3+T/0nE2nX/v4e6ycdeuHXrjp9Cv+/29bOStJbvd9z9t0+qW/9607yrvD9z5/0+k/e8LddpT3TQ95wabTz3/iXXeU960PfuGm08968l1WzrrX97xo0+mn7yArSR5wRN7vP+VbdpT3Xd/94k2nn/D7O8t7yHdtzvudp+4s7wcetDnvkX+4et5PfPvmrF95+s66/ewDNuf9/J/s7LH9pftufmx/5IKd5T3mrM15D3j2zvKefubmvLs+50HH+Mlr9sJ7bn4tvOuzf2zlrCR54ZmP3nT6bs/++R3lveDMX9qc96ydvZ694F6bX8++9Zm/uaO859/7YUfkPXYHWT+8OeuC3105K0mef9b3H5H3xB3mbf7jiW+74Lwd5T3vrLM3553/tJ3l3ec7jsj74x1k3X/T6buf/8yVs5Lkufe596bT9zj/wmP85NZceJ/N70vuef4Lj/GTW/Oc+2x+rT7z/JfsKO/Z9/nGTafvdcFLV8561ll32HT6rAtesXJWklxw1u02nb7PBZftKO/8s87YdPp+F7xpR3nPOOsLNp1+yDPfsaO8J9z7tE2n/9ez/mnlrF+81003nX7Us96zclaS/Pi9brzp9BOfedWO8h587xtuOv3HF7xvR3n3P+sGm04/9xk7y7v7/TbnXfRHh1bOuvMDD2w6/Vd/sHpWknzdd27Oe9WTd/ZYfOX3bH4sLj93Z3lfds7mvL9/3Ht3lPf5P3ijTaff+cjVn8un/sTm5/GVv/7OlbOS5Cb/49RNp9/zm3+/o7wbP+zzN+c98nU7y/uJL910+r2PvmRHeTf6sYOb837rZatn/ehXb8567MUrZyXJjX74jptOX/XbLz7GT27NDX9o8+87Vz3uuTvL+8G7b877nfOP8ZNbzPuB+1zt+Wv7ZF9VfVuSq1prr17l8q21c1trB1trBw8cOHDNFwAAAACAwa3zk31fk+QeVXW3JNdO8tlJfivJSVV1wvLTfackefcaOwAAAADAMNb2yb7W2v9srZ3SWjs9yQOS/Hlr7TuSXJzk8OcNz07ynGNEAAAAAADbsBt74z3STyf5iap6Sxbf4bezL6QBAAAAAJKsfwcdSZLW2l8k+Yvl8X9IctvduF4AAAAAGMlefLIPAAAAAFgDwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJiJtQ37quraVfXKqnptVb2+qh6xXP6UqvrHqrpseThjXR0AAAAAYCQnrDH7o0nu1Fr7t6o6MclfV9ULl+f9VGvt/DVeNwAAAAAMZ23DvtZaS/Jvy5MnLg9tXdcHAAAAAKNb63f2VdXxVXVZkquSXNRae8XyrF+uqsur6lFV9RnHuOw5VXVJVV1y6NChddYEAAAAgFlY67CvtfaJ1toZSU5Jctuq+pIk/zPJFyb5yiTXT/LTx7jsua21g621gwcOHFhnTQAAAACYhV3ZG29r7QNJLk5yl9balW3ho0menOS2u9EBAAAAAOZunXvjPVBVJy2PXyfJnZP8XVXdZLmskpyZ5Ip1dQAAAACAkaxzb7w3SXJeVR2fxVDxGa2151XVn1fVgSSV5LIk37/GDgAAAAAwjHXujffyJF9+lOV3Wtd1AgAAAMDIduU7+wAAAACA9TPsAwAAAICZMOwDAAAAgJkw7AMAAACAmTDsAwAAAICZMOwDAAAAgJkw7AMAAACAmTDsAwAAAICZMOwDAAAAgJkw7AMAAACAmTDsAwAAAICZMOwDAAAAgJkw7AMAAACAmTDsAwAAAICZMOwDAAAAgJkw7AMAAACAmTDsAwAAAICZMOwDAAAAgJkw7AMAAACAmTDsAwAAAICZMOwDAAAAgJkw7AMAAACAmTDsAwAAAICZMOwDAAAAgJkw7AMAAACAmTDsAwAAAICZMOwDAAAAgJkw7AMAAACAmTDsAwAAAICZMOwDAAAAgJkw7AMAAACAmTDsAwAAAICZMOwDAAAAgJkw7AMAAACAmTDsAwAAAICZMOwDAAAAgJkw7AMAAACAmTDsAwAAAICZMOwDAAAAgJkw7AMAAACAmTDsAwAAAICZMOwDAAAAgJlY27Cvqq5dVa+sqtdW1eur6hHL5beoqldU1Vuq6o+r6lrr6gAAAAAAI1nnJ/s+muROrbVbJzkjyV2q6vZJfi3Jo1prn5vk/UkevMYOAAAAADCMtQ372sK/LU+euDy0JHdKcv5y+XlJzlxXBwAAAAAYyVq/s6+qjq+qy5JcleSiJG9N8oHW2seXP/KuJDdbZwcAAAAAGMVah32ttU+01s5IckqS2yb5wq1etqrOqapLquqSQ4cOrasiAAAAAMzGruyNt7X2gSQXJ/mqJCdV1QnLs05J8u5jXObc1trB1trBAwcO7EZNAAAAANjX1rk33gNVddLy+HWS3DnJG7MY+t1n+WNnJ3nOujoAAAAAwEhOuOYfWdlNkpxXVcdnMVR8RmvteVX1hiRPr6pfSvKaJE9cYwcAAAAAGMbahn2ttcuTfPlRlv9DFt/fBwAAAABMaFe+sw8AAAAAWD/DPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmAnDPgAAAACYCcM+AAAAAJgJwz4AAAAAmIm1Dfuq6tSquriq3lBVr6+qH10uf3hVvbuqLlse7rauDgAAAAAwkhPWmP3xJA9rrV1aVZ+V5NVVddHyvEe11n5jjdcNAAAAAMNZ27CvtXZlkiuXxz9UVW9McrN1XR8AAAAAjG5XvrOvqk5P8uVJXrFc9ENVdXlVPamqTt6NDgAAAAAwd2sf9lXV9ZJckOTHWmsfTPL4JLdMckYWn/z7zWNc7pyquqSqLjl06NC6awIAAADAvrfWYV9VnZjFoO9prbVnJklr7b2ttU+01j6Z5PeS3PZol22tndtaO9haO3jgwIF11gQAAACAWVjn3ngryROTvLG19sgNy2+y4cfuleSKdXUAAAAAgJGsc2+8X5PkO5O8rqouWy772SQPrKozkrQkb0vyfWvsAAAAAADDWOfeeP86SR3lrBes6zoBAAAAYGS7sjdeAAAAAGD9DPsAAAAAYCYM+wAAAABgJgz7AAAAAGAmDPsAAAAAYCYM+wAAAABgJgz7AAAAAGAmDPsAAAAAYCYM+wAAAABgJgz7AAAAAGAmDPsAAAAAYCYM+wAAAABgJgz7AAAAAGAmDPsAAAAAYCYM+wAAAABgJgz7AAAAAGAmDPsAAAAAYCYM+wAAAABgJgz7AAAAAGAmDPsAAAAAYCYM+wAAAABgJgz7AAAAAGAmDPsAAAAAYCYM+wAAAABgJgz7AAAAAGAmtjTsq6qXbGUZAAAAALB3Tri6M6vq2kmum+QGVXVyklqe9dlJbrbmbgAAAADANlztsC/J9yX5sSQ3TfLqfHrY98Ekv72+WgAAAADAdl3tsK+19ltJfquqfri19thd6gQAAAAArOCaPtmXJGmtPbaqvjrJ6Rsv01r7/TX1AgAAAAC2aUvDvqr6gyS3THJZkk8sF7ckhn0AAAAA0IktDfuSHExyq9ZaW2cZAAAAAGB1x23x565IcuN1FgEAAAAAdmarn+y7QZI3VNUrk3z08MLW2j3W0goAAAAA2LatDvsevs4SAAAAAMDObXVvvH+57iIAAAAAwM5sdW+8H8pi77tJcq0kJyb5cGvts9dVDAAAAADYnq1+su+zDh+vqkpyzyS3X1cpAAAAAGD7tro33k9pC89O8i3T1wEAAAAAVrXVP+O994aTxyU5mOQ/1tIIAAAAAFjJVvfGe/cNxz+e5G1Z/CkvAAAAANCJrX5n3/esuwgAAAAAsDNb+s6+qjqlqp5VVVctDxdU1SnrLgcAAAAAbN1Wd9Dx5CQXJrnp8vDc5bJjqqpTq+riqnpDVb2+qn50ufz6VXVRVb15+e/JO7kBAAAAAMDCVod9B1prT26tfXx5eEqSA9dwmY8neVhr7VZJbp/kB6vqVkl+JslLWmufl+Qly9MAAAAAwA5tddj3z1X1oKo6fnl4UJJ/vroLtNaubK1dujz+oSRvTHKzLHbscd7yx85LcuZKzQEAAACATbY67PvvSe6X5D1JrkxynyTfvdUrqarTk3x5klckuVFr7crlWe9JcqNjXOacqrqkqi45dOjQVq8KAAAAAIa11WHf/05ydmvtQGvthlkM/x6xlQtW1fWSXJDkx1prH9x4XmutJWlHu1xr7dzW2sHW2sEDB67pL4YBAAAAgK0O+76stfb+wydaa/+SxSf1rlZVnZjFoO9prbVnLhe/t6pusjz/Jkmu2l5lAAAAAOBotjrsO27jXnOr6vpJTri6C1RVJXlikje21h654awLk5y9PH52kudsvS4AAAAAcCxXO7Db4DeT/G1V/cny9H2T/PI1XOZrknxnktdV1WXLZT+b5FeTPKOqHpzk7Vl8FyAAAAAAsENbGva11n6/qi5Jcqflonu31t5wDZf56yR1jLO/cesVAQAAAICt2Oon+7Ic7l3tgA8AAAAA2Dtb/c4+AAAAAKBzhn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE2sb9lXVk6rqqqq6YsOyh1fVu6vqsuXhbuu6fgAAAAAYzTo/2feUJHc5yvJHtdbOWB5esMbrBwAAAIChrG3Y11p7aZJ/WVc+AAAAALDZXnxn3w9V1eXLP/M9+Vg/VFXnVNUlVXXJoUOHdrMfAAAAAOxLuz3se3ySWyY5I8mVSX7zWD/YWju3tXawtXbwwIEDu1QPAAAAAPavXR32tdbe21r7RGvtk0l+L8ltd/P6AQAAAGDOdnXYV1U32XDyXkmuONbPAgAAAADbc8K6gqvqj5J8Q5IbVNW7kvxCkm+oqjOStCRvS/J967p+AAAAABjN2oZ9rbUHHmXxE9d1fQAAAAAwur3YGy8AAAAAsAaGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBNrG/ZV1ZOq6qqqumLDsutX1UVV9eblvyev6/oBAAAAYDTr/GTfU5Lc5YhlP5PkJa21z0vykuVpAAAAAGACaxv2tdZemuRfjlh8zyTnLY+fl+TMdV0/AAAAAIxmt7+z70attSuXx9+T5EbH+sGqOqeqLqmqSw4dOrQ77QAAAABgH9uzHXS01lqSdjXnn9taO9haO3jgwIFdbAYAAAAA+9NuD/veW1U3SZLlv1ft8vUDAAAAwGzt9rDvwiRnL4+fneQ5u3z9AAAAADBbaxv2VdUfJfnbJF9QVe+qqgcn+dUkd66qNyf5puVpAAAAAGACJ6wruLX2wGOc9Y3ruk4AAAAAGNme7aADAAAAAJiWYR8AAAAAzIRhHwAAAADMhGEfAAAAAMyEYR8AAAAAzIRhHwAAAADMhGEfAAAAAMyEYR8AAAAAzIRhHwAAAADMhGEfAAAAAMyEYR8AAAAAzIRhHwAAAADMhGEfAAAAAMyEYR8AAAAAzIRhHwAAAADMhGEfAAAAAMyEYR8AAAAAzIRhHwAAAADMhGEfAAAAAMyEYR8AAAAAzIRhHwAAAADMhGEfAAAAAMyEYR8AAAAAzIRhHwAAAADMhGEfAAAAAMyEYR8AAAAAzIRhHwAAAADMhGEfAAAAAMyEYR8AAAAAzIRhHwAAAADMhGEfAAAAAMyEYR8AAAAAzIRhHwAAAADMhGEfAAAAAMyEYR8AAAAAzIRhHwAAAADMhGEfAAAAAMyEYR8AAAAAzIRhHwAAAADMhGEfAAAAAMyEYR8AAAAAzIRhHwAAAADMxAl7caVV9bYkH0ryiSQfb60d3IseAAAAADAnezLsW7pja+19e3j9AAAAADAr/owXAAAAAGZir4Z9LcmfVtWrq+qcPeoAAAAAALOyV3/G+7WttXdX1Q2TXFRVf9dae+nGH1gOAc9JktNOO20vOgIAAADAvrInn+xrrb17+e9VSZ6V5LZH+ZlzW2sHW2sHDxw4sNsVAQAAAGDf2fVhX1V9ZlV91uHjSb45yRW73QMAAAAA5mYv/oz3RkmeVVWHr/8PW2sv2oMeAAAAADAruz7sa639Q5Jb7/b1AgAAAMDc7dXeeAEAAACAiRn2AQAAAMBMGPYBAAAAwEwY9gEAAADATBj2AQAAAMBMGPYBAAAAwEwY9gEAAADATBj2AQAAAMBMGPYBAAAAwEwY9gEAAADATBj2AQAAAMBMGPYBAAAAwEwY9gEAAADATBj2AQAAAMBMGPYBAAAAwEwY9gEAAADATBj2AQAAAMBMGPYBAAAAwEwY9gEAAADATBj2AQAAAMBMGPYBAAAAwEwY9gEAAADATBj2AQAAAMBMGPYBAAAAwEwY9gEAAADATBj2AQAAAMBMGPYBAAAAwEwY9gEAAADATBj2AQAAAMBMGPYBAAAAwEwY9gEAAADATBj2AQAAAMBMGPYBAAAAwEwY9gEAAADATBj2AQAAAMBMGPYBAAAAwEwY9gEAAADATBj2AQAAAMBMGPYBAAAAwEwY9gEAAADATBj2AQAAAMBMGPYBAAAAwEwY9gEAAADATOzJsK+q7lJVb6qqt1TVz+xFBwAAAACYm10f9lXV8Ukel+SuSW6V5IFVdavd7gEAAAAAc7MXn+y7bZK3tNb+obX2n0menuSee9ADAAAAAGZlL4Z9N0vyzg2n37VcBgAAAADsQLXWdvcKq+6T5C6ttYcsT39nktu11n7oiJ87J8k5y5NfkORNW4i/QZL3TVh3yryeu42W13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3eaUd/PW2oEjF54wYZGteneSUzecPmW5bJPW2rlJzt1OcFVd0lo7uLN668nrudtoeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+duo+X13G2EvL34M95XJfm8qrpFVV0ryQOSXLgHPQAAAABgVnb9k32ttY9X1Q8leXGS45M8qbX2+t3uAQAAAABzsxd/xpvW2guSvGAN0dv6s99dzuu522h5PXcbLa/nbqPl9dxttLyeu42W13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbfZ5u76DDgAAAABgPfbiO/sAAAAAgDUw7AMAAACAmTDsAwAAAICZmM2wr6p+YMKs61XVbarqpB1kHFdVxy2PX2uZd/0Vs65VVbXh9B2r6mFVdddV+x2Rf4uqundVfeGKl//sqrrlUZZ/2Yp5d6iqL1ge/5qq+smq+tZVsvZJv8nyls/d+1TVj1fVj1TVXQ4/D1fMu3FV3Xh5/MDyefLFHeVN/dhOud6edngbUlWnLx+XL1kla5kx6WN7RPaOtgHLjElv7zLnYFXdq6ruscNukz3vll2uvWqXLV7Hr0ycN9nr4xG5d17xciceZdkNdt5oU962ny/L5/C1l8erqr6nqh5bVQ+tqm3v0GzV7dA1ZP63qrp/Vf3E8nD/Hb5XmWodm/S+O0r+Tt+ndLveruN5cpTr2PE2YKr1dk3rRVXV7ZbPkXsvj9c1X3Jb17HS9u6IjB3/fnE12auuG1NvUyZ7rzL1e7yruZ7eH9uV+6379baH590aXrsn/727JnqP3PNr2YbLT7Y9XsdjscyZ6r3PZL83LjMm3R4nSVpr++6Q5CeOODwsyfsOn14h73c2HP/aJO9IcnGSdya52wp5ZyZ5b5Irk9wzySuSvCTJu5LcfYW81yY5eXn8p5K8LMnPJ7koyf9ZIe/ZG47fM8k/Jnlykjcl+e5tZt0vyT8luSzJ65N85YbzLl2h26OXt++VSX5xefx/JfmzJP/fCnm995ssb3lbX5nkCUnemuQPkjwtyeVJvnSFbt+3fG68LclDl8/jJy6fJw/uIG/qx3ay9TbJzyxv698lecjy3ycue66yjZr6sX32huM72gas6fZ+fZJLluvB+5M8L8nfJPmLJKfu8fPu37N4vfmDJHdLcvx2M47Ie8wRh8cm+cDh0yvkTfr6eA3X9Y5t/vwdl+vT+5L8aZLTN5y37XV2ym7Ly1yR5LrL47+W5PwkD0rypCRPWiHvE0nenMW2/VYT3KbvWq7/j8/iPcDPJ/nd5bLv2mbWZOvYmu67Z284PsU2qtv1dg3Pk6nfI0+63q7h9n5zkrckeWEWr5FPSPKi5bJv3mn+hutZZZsy6e8Xa+g32TZlmTfZe5VM/B5vwMd2V15vO3neTf36M/Xv3ZO9R07Hr2XLvEm3x2t4LKb8/eLMTDvvmXS9+FTuTp4ge3VI8qEkf5zk/03yC8vD+w8fXyHv0g3HL05ym+Xx/yfJJSvkvSbJjZPcIskHk3zBcvnNV8y7YsPxS5JcZ3n8hCSXr9Jvw/GXJbnF8vgNkrx2m1mXJbnJ8vhtlxuwex15PdvIe32SSnLd5WN6eON94sb7YUb9JsvL4s3U4cvfIMmLl8e/LMnLVuj2umWvz0nyb0luvFx+cpLLOsib+rGdbL1dPq7XWd7WDyU5sFz+mSs+T6Z+bF+z4fiOtgFrur2v2ZBxiyTPWh6/c5I/3ePn3WuWl/3eLF7U35vFi/HXbzdrmffOJE/N4kX+7OXh0OHjK+RN/fp44TEOz03y4W1mvSrJFy+P3yeLX/hvf+Rzcht5R75J3fhm9YMr5L1hw/FXJzluw+lV1ovXJPmSJL+cxRvd12bxpv/0FZ8rb0py0lGWn5zk71foNsk6tq77bsPxKbZR3a63a3ieTL0NmHq9nfr2vvFol10+r9+4zazJtnfLvKl/v5h6mzfZNmV5ucneq2T693i9P7ZT95tsvd0Hz7upX3+m/r17svfI6fi1bJk32fZ4TY/FlL9fTPZ74/Jyk64Xhw87/tOKPfLFSX4zi5XkEa21j1TV2a21R0yQ/dmttUuTpLX2D6t+9Ly19p4kqap3tNbetFz29hXzPlhVX9JauyKLaf61s5jsn5DV/hS7bTh+QmvtH5f93ldVn9xm1vGttSuXl39lVd0xyfOq6tQjrmfL3VprbUOPwxmfzGq3tfd+U+ZVFs+LJPlwkhsur+DyqvrsFbp9rLX2kSQfqaq3Hn5Ot9beX1Wr3HdT50392E653n6itfbvVfWfWTwm/7zM+vCqn2TPtI/tlNuAZPrbe3xr7dDy+DuyeOFMa+2iqnr0NrOmft611tr7k/xekt+rxZ8H3y/Jr1bVKa21U7eZd6ssPuFylyQ/2Vr7p6r6hdbaeSt0S6Z/ffy6LP6H/N+OWF5Z/AK2Hddqrb0+SVpr51fVG5M8s6p+Oquts9+TxaeWPnqU8x64Qt47q+pOrbU/z+KToKcmeXtVfc4KWcniuXJFkp9L8nNVddskD0jy18ttzFdvM69y9Pvpk8vztmPKdSxZw3234fgU26ie19upnydTbwOmXm+nvr0nZPFJiiO9O4v/ON2OKbd3R5ri94upt3lTblMO5031XmXq93i9P7ZT95tyve39eTf168/Uv3dP+R6559eyZNrtcTL9YzHpe5+J5z1TrxdJsj+Hfa21dyS5b1XdM8lFVfWoHUZ+YVVdnsUdeXpVnbz8RfC4JNdaJbCqjmutfTLJf9+w7PgV874/ydOq6rVJrkpySVW9NMmXJlnl7+pvXVUfzOL2fkZV3aS1dmVVXSvJ8dvM+lBV3bK19tYkWeZ8Q5JnZ/GGc7ueX1V/lcXK/IQkz6iql2fxsduXrpDXe78p816Q5EXL58ZdkvxJkiy/O2CVjUSrqhNbax9L8qnvEKzFd0WsNNicOG/qx3bK9fbSqvrDLH7ZekmS86rqRUnulOQNK1Sb+rGdchuQTH97L6mqJyb58yT3yOLj9amq667Qb+rn3ab7e/lC/5gkj6mqm283rLX2oSQ/VlVfkcV2/vkr9jqcN/Xr48uTfKS19pdHnlFVb9pm1seq6sYbBq6vr6pvzOLPKP7L9zJtwauy+F/flx2l28NXyHtIkt9fXvZfk1xWVZclOSmLP4HcriOfK69M8sqqeliSO6yQ98tZrGt/msX/xifJaVn8j/QvbjNrynUsmf6+m3ob1fN6O+nzZA3bgKnX26nXiycleVVVPT2fXi9OzWKA+MRtZk25vUum//1i6m3elNuUZNr3Kkd7j3fHJM/Kau/xen9sp+435Xrb+/Nu6tefqX/vnvI9cs+vZcnRt8enJbl/tr89TqZ/LCZ97zPxvGfq9WLRp7WVPgDTjar6zCQPT3K71toqbxJylJXjytbaf9biC0zv0Fp75jbzvjLJ61pr/3HE8tOTfG1r7akrdDw+i7+D//x8emr+4tbaB7abdTXXcVKSL2qt/e02LnPrLD5e/pYjlp+Y5H6ttaet0OOrsvifi5fX4ot575XF9P385Qq1naxbZ/Hi+eYe+02dV1V3y+J/aV7bWrtouey4JCe21o72P3JXl3Vakn9qrX38iOU3y+J58md7nDfpc2/K9bYWXwh83yz+h+b8LP5X9tuzeFwf11r78Ha6LTMne2yv5jpOyja3AcvLTXp7l4/h92Z5e7P4zpVPVNV1ktywtfb2bWSdlsU2/WNHLF/1efcNrbW/2M5ltpFdSX4gyVe11h40Qd6OXx+nVFXflORQa+21Ryw/KckPttZ+eZt510/yH23xyc3JVNUXZfNr7atW3LZ/e2vtDyfudnKSb0lys+Wid2fxXuD928yZbB07IneS++5q8k/KatuobtfbdTxPNmRP8R556vV2HevFrbL4xW3jenFha22V/2yazBp+v5h8mzfVNmVD3iTvVdbx+8WUpn5spzblersfnnfLzMlef6b8vXvK98g9v5ZtyPmiLL7DbpLt8cSPxZS/X6xj3jP9ejGDYd/1k6S19i973eXqVNVt2vIj3j3mAZ+2X7YrPbPNm6da/CnW5yX5hx2+KZ90Het9ne25X8/dmEbv28/e+01hXevZCPddMt1rzzr1/lj02q/nx3a09yrr0OvzbhQ7+ZjmnqnF7qufXlWHstjzySur6qrlstNXyPvCqnphVT2/qm5ZVU+pqg9U1SuX0+nt5t3miMNXJLmwqr68qm4zQd5tdph36vK++quq+tnasFv2qnr2NrM2fmz1lKp6yfK+e1lVff5edltH3jVc1+tWuMzR7r/3r3L/Tf1YXMN1TXVbe+o32XblGNuU96+6TbmG69r2bZ06bw3bqMm2ob1vU6bcBiwzJn3uVdVTa/FJhVTVt2Sx17tfy+LPZO67zaypX7snzbuG69rTbcrU/TZ0u2qKbru0XuzkvUW3rz/HWGenfA+60+1x7++RJ+13Ndez59uANdx3G9eLm03w+jPl7xeTvfYsM6bepkz9WEy9DZ3yfdTU24B1PrZTPI+nfn2cNO9qrmeVbdS6n3e9vf5MlreGbuuZWbQV9+yxl4ckf5vF334fv2HZ8Vl8P8fLV8h7aZK7Z/Elo29f5tRy2UtWyPtkFnuPu3jD4d+X//55B3kXZfE38GdksSellyX5nOV5r9lm1sa9UT0jyTlZDJHvteJ9N1m3NeXd+xiHs7L4uPx28ya7/9bwWHR7W9fUb7Ltyhq2KVPf1qnzut3m7YNtytTrxdTPvddtOP6yLPeyltX23j71a/fUed1uU6but4Zuva8X3b7+rGGdnXp7PEy/fbANmPq+6/Y9fCZ87VnTbe32d7Op+61hG9D7Y9vte5VjbJ92so3q9nm3pufelK8XU3eb9LH4VO6qF9zLQ5I3r3Le1VzmNRuOv+WI8y5dIe+sJH+Z5K4blv3jDm7v1HmXHXH6QVnsFvyW2729R2xgj8x9zV52W1Pex5I8JcmTj3L40Ap5k91/a3gsur2ta+o32XZlDduUqW/r1HndbvP2wTZl6vVi6ufe67PY02CS/HWS4zaet82sqV+7p87rdpsydb81dDvyudvbetHt688a1tmpt8fD9NsH24Cp77tu38NnwteeNd3Wbn83m7rfGrYBvT+23b5XWcM2qtvn3ZGP30TPvcny1tBt0sfi8GFf7o03yaur6neSnJfNe946O8lrVsjbuPeVRx5x3rb3ptJau6CqXpzkF5cfLX5YVttN/FrykpxYVdduyy+UbK09tarek+TFWewpaDtOqarHZDHJPlCf3utlstoutqfsto68y5P8RlvsAnyTWnwZ7nZNef9N/Vj0fFvX0W/K7cqk25RMf1snzet8m9f7NmXq9WLq594jklxcVY9L8jdJ/qSqLkxyxyQv2mbW1K/dU+f1vE2Zut/U3XpfL3p+/en6Pehg/breBqzhvuv5PfyUrz3JxLe189/Npu7X8/uKZPrncc/vVabeRvX8vEumf+5NmTd1t6nfRy2sOiXcy8PyDnxoFhuE1y0PL8piDzKfsULe9yW53lGWf26SR++w622y+Ojqtj9au668JD+e5OuPsvzLk1y0zayzjzicvFx+4yS/spfd1pT3dUlOO8Z5B1fIm+z+W8Nj0e1tXVO/ybYrU29T1nBbJ8074vJdbfP2wTZl6vVi8tezLL44+9eSPCvJc5M8Psm3rJAz9Wv31HndblOm7reGbr2vF92+/qxjnd2QMcV7xmH69b4NWMN91/t7+Elee9ZxW9fwWEx6303Zbx3bgJ4f26nX2ynz1rCN6vZ5t47n3pR5a+i2lsdi3++Ndz+oqkryWa21D/aYBzAl2zyAPvS+/ey9X8/cd/3o/bHovR/z5Hm39/blsK+qTkjy4CRnJrnZcvG7kzwnyRPbpz+6u928eyW5qbyt53ksJss7Mzu8//bRYzH7fvvoeTf7vJ67HZF3ZsZZL3actea87u67qfv13G3N/abOG2kbNfu8ffQ8nn2/kbYBvefto+fd7PN6fp6Mltdzt025+3TY90dJPpDF376/a7n4lCw+unv91tr95e1OXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+dum7Qd/C32Xh2S/P0q58mbPq/nbqPl9dxttLyeu42W13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt42H47I//UtV3beqPtW/qo6rqvsneb+8Xc3rudtoeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+duo+X13O3TVp0S7uUhyelJ/jjJoSR/n+TNSa5aLruFvN3L67nbaHk9dxstr+duo+X13G20vJ67jZbXc7fR8nruNlpez91Gy+u522h5PXcbLa/nbqPl9dxt42FffmffRlX1OUnSWvtneXub13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3UbL67nbfv0z3iRJVV03yUOT/J/l6c+rqm+Tt/t5PXcbLa/nbqPl9dxttLyeu42W13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nrsl+3zYl+TJSf4zyVcvT787yS/J25O8nruNltdzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+duo+X13G20vJ67jZbXc7fR8nruNlpez932/bDvlq21X0/ysSRprX0kScnbk7yeu42W13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3fb9sO8/q+o6SVqSVNUtk3xU3p7k9dxttLyeu42W13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu6W/7LHjv10SHLnJH+ZxV5LnpbkbUm+Qd7u5/XcbbS8nruNltdzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+duo+X13G20vJ67jZbXc7fR8nru1tp89sZ7+yw+3vjy1tr75O1NXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+tu+3HYV1W3ubrzW2uXytudvJ67jZbXc7fR8nruNlpez91Gy+u522h5PXcbLa/nbqPl9dxttLyeu42W13O30fJ67jZaXs/dNuXu02HfxVdzdmut3Une7uT13G20vJ67jZbXc7fR8nruNlpez91Gy+u522h5PXcbLa/nbqPl9dxttLyeu42W13O30fJ67rYpdz8O+wAAAACA/+qEvS6wE1V17SQ/kORrs9hjyV8l+d3W2n/I2928nruNltdzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+duo+X13G20vJ67jZbXc7fR8nruNlpez92Sff7Jvqp6RpIPJXnqctG3JzmptXZfebub13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3UbL67lbkqStuBvfHg5J3rCVZfLWn9dzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+duo+X13G20vJ67jZbXc7fR8nruNlpez91Gy+u5W2stx2V/u7Sqbn/4RFXdLskl8vYkr+duo+X13G20vJ67jZbXc7fR8nruNlpez91Gy+u522h5PXcbLa/nbqPl9dxttLyeu42W13O3ff9nvG9M8gVJ3rFcdFqSNyX5eBZ7LfkyebuT13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3UbL67lbsv+HfTe/uvNba2+Xtzt5PXcbLa/nbqPl9dxttLyeu42W13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nrsl+3zYlyRVdXKSU7Nhz8KttUvl7X5ez91Gy+u522h5PXcbLa/nbqPl9dxttLyeu42W13O30fJ67jZaXs/dRsvrudtoeT13Gy2v524nXPOP9KuqfjHJdyd5a5LDU8uW5E7ydjev526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+duo+X13G20vJ67jZbXc7dkn3+yr6relORLW2v/KW9v83ruNlpez91Gy+u522h5PXcbLa/nbqPl9dxttLyeu42W13O30fJ67jZaXs/dRsvrudtoeT13S7Lv98Z7RZKT5HWR13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3UbL67nbvv9k38Ekz8niTvno4eWttXvI2928nruNltdzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+duo+X13G20vJ67jZbXc7fR8nruNlpez92Sff6dfUnOS/JrSV6X5JPy9jSv526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+duo+X13G20vJ67jZbXc7ektbZvD0leJa+PvJ67jZbXc7fR8nruNlpez91Gy+u522h5PXcbLa/nbqPl9dxttLyeu42W13O30fJ67jZaXs/dWmv7/s94H5nFxxsvzOaPOa6622R5K+b13G20vJ67jZbXc7fR8nruNlpez91Gy+u522h5PXcbLa/nbqPl9dxttLyeu42W13O30fJ67pbs/+/su/goi1trbdVdHctbMa/nbqPl9dxttLyeu42W13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt2SfD/sAAAAAgE/b7zvoSFV9a5IvTnLtw8taa/9b3u7n9dxttLyeu42W13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu523KolelBVv5vk/kl+OEkluW+Sm8vb/byeu42W13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3ZIkbcK9fez2IcnlR/x7vSR/JW/383ruNlpez91Gy+u522h5PXcbLa/nbqPl9dxttLyeu42W13O30fJ67jZaXs/dRsvrudtoeT13a63t70/2Jfn35b8fqaqbJvl4kpvI25O8nruNltdzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+duo+X13G20vJ67jZbXc7fR8nruNlpez932/Xf2Pa+qTkry60levVz2BHl7ktdzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+duo+X13G20vJ67jZbXc7fR8nruNlpez91Gy+u52/78M94kX5nkxhtOf1eSP03ymCTXl7d7eT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+duo+X13G20vJ67bcpd9YJ7eUhy6eEbneQOSf4pyVlJfjHJ+fJ2L6/nbqPl9dxttLyeu42W13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt025q15wLw9JXrvh+OOSPHzD6cvk7V5ez91Gy+u522h5PXcbLa/nbqPl9dxttLyeu42W13O30fJ67jZaXs/dRsvrudtoeT13Gy2v524bD/t1Bx3HV9Xh7xv8xiR/vuG8Vb6HUN7qeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+duo+X13G20vJ677fyCe+yPkvxlVb0viz2W/FWSVNXnJvlXebua13O30fJ67jZaXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3UbL67nbp9Tyo4H7TlXdPovdEP9pa+3Dy2Wfn+R6rbVL5e1eXs/dRsvrudtoeT13Gy2v526j5fXcbbS8nruNltdzt9Hyeu42Wl7P3UbL67nbaHk9dxstr+dun8rcr8M+AAAAAGCz/fqdfQAAAADAEQz7AAAAAGAmDPsAAAZTVSdV1Q/swvWcWVW3Wvf1AADwaYZ9AADjOSnJlod9tbDK+8Yzkxj2AQDsIjvoAAAYTFU9Pck9k7wpycVJvizJyUlOTPLzrbXnVNXpSV6c5BVJviLJ3ZJ8V5IHJTmU5J1JXt1a+42qumWSxyU5kOQjSb43yfWTPC/Jvy4PZ7XW3rpbtxEAYFQn7HUBAAB23c8k+ZLW2hlVdUKS67bWPlhVN0jy8qq6cPlzn5fk7Nbay6vqK5OcleTWWQwFL03y6uXPnZvk+1trb66q2yX5ndbanZY5z2utnb+bNw4AYGSGfQAAY6skv1JVd0jyySQ3S3Kj5Xlvb629fHn8a5I8p7X2H0n+o6qemyRVdb0kX53kT6rqcOZn7FZ5AAA2M+wDABjbd2Tx57df0Vr7WFW9Lcm1l+d9eAuXPy7JB1prZ6ynHgAA22EHHQAA4/lQks9aHv9vSa5aDvrumOTmx7jM3yS5e1Vde/lpvm9LktbaB5P8Y1XdN/nUzjxufZTrAQBgFxj2AQAMprX2z0n+pqquSHJGkoNV9bosdsDxd8e4zKuSXJjk8iQvTPK6LHa8kSw+HfjgqnptktdnsfOPJHl6kp+qqtcsd+IBAMCa2RsvAABbUlXXa639W1VdN8lLk5zTWrt0r3sBAPBpvrMPAICtOreqbpXFd/qdZ9AHANAfn+wDAAAAgJnwnX0AAAAAMBOGfQAAAAAwE4Z9AAAAADAThn0AAAAAMBOGfQAAAAAwE4Z9AAAAADAT/xfv2YtzpzuyPQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1584x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXu2FJYvXwN6"
      },
      "source": [
        "The distibution across all the classes is exactly same. This is a good thing because in this way our model will be less prone to overfitting for a prticular class. We have around 40 Images for each class, which is way less data than I would have preferred. We might overfit the training data due to lack of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6rmhTSKYASE"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mT-BtUiYCqe"
      },
      "source": [
        "Since, in this task in am going to be using `PyTorch`, before we can directly start training the `CNN network`, we need to get our data into `Dataset` and `DataLoader`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXuQWbBuYEXs"
      },
      "source": [
        "import albumentations as A\n",
        "import cv2\n",
        "import torchvision.transforms as T\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.utils import make_grid"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf5eLkaHYF9w"
      },
      "source": [
        "We also need to encode our string labels into interger labels. For this we will use the `LabelEncoder()` from scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVIXgw04YJ5x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "81b8d35f-892f-4c80-c801-0ae5213a48cc"
      },
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(unq_cls)\n",
        "\n",
        "def encode_label(x):\n",
        "    \"Encoder `x`, given x is a scalar value\"\n",
        "    return encoder.transform([x]).item()\n",
        "\n",
        "CLASS_MAP = L(list(encoder.classes_)).map_dict(encode_label)\n",
        "CLASS_MAP = {k:v for v, k in CLASS_MAP.items()}\n",
        "\n",
        "DATASET_01[\"cat_label\"] = DATASET_01[\"target\"].map(encode_label)\n",
        "DATASET_01.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>target</th>\n",
              "      <th>cat_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/train/Sample004/img004-001.png</td>\n",
              "      <td>Sample004</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/train/Sample035/img035-004.png</td>\n",
              "      <td>Sample035</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/train/Sample043/img043-008.png</td>\n",
              "      <td>Sample043</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/train/Sample046/img046-005.png</td>\n",
              "      <td>Sample046</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/train/Sample025/img025-050.png</td>\n",
              "      <td>Sample025</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              image_id     target  cat_label\n",
              "0  data/train/Sample004/img004-001.png  Sample004          3\n",
              "1  data/train/Sample035/img035-004.png  Sample035         34\n",
              "2  data/train/Sample043/img043-008.png  Sample043         42\n",
              "3  data/train/Sample046/img046-005.png  Sample046         45\n",
              "4  data/train/Sample025/img025-050.png  Sample025         24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75E8ot1jjNyw"
      },
      "source": [
        "Save a copy of the `CLASS_MAP` which stores the mapping of `classes` and `class ids`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyUFNlVxjQ1f"
      },
      "source": [
        "import json\n",
        "\n",
        "with open(\"data/class_map_ds01.json\", 'w') as f:\n",
        "    json.dump(CLASS_MAP, f)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRu75JbYS6D"
      },
      "source": [
        "Let's also create a Training and Validation split on the data.\n",
        "\n",
        "Why we need a different training and validation split for the data ?\n",
        "\n",
        "This is so that we don't inadvertently overfit, train a model to work well only on our training data. For this purpose we will compute metric over on the validation data. During the training phase the model will see only the training data. The Validation data will remain separate and will only be used for calculation of the *metric*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9YRWmMAYXPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd2da76a-d1dd-458c-ba64-8a54a1954ca7"
      },
      "source": [
        "TRAIN_DATASET_01, VALID_DATASET_01 = train_test_split(DATASET_01, test_size=0.2, random_state=42, \n",
        "                                                      shuffle=True, stratify=DATASET_01['cat_label'])\n",
        "\n",
        "TRAIN_DATASET_01 = TRAIN_DATASET_01.reset_index(drop=True, inplace=False)\n",
        "VALID_DATASET_01 = VALID_DATASET_01.reset_index(drop=True, inplace=False) \n",
        "\n",
        "print(\"Num Training Examples: \", len(TRAIN_DATASET_01))\n",
        "print(\"Num Validation Examples: \", len(VALID_DATASET_01))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num Training Examples:  1984\n",
            "Num Validation Examples:  496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m0bwAFHYbKL"
      },
      "source": [
        "Now we will create a custom `Dataset` obj that can parse the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tCeY67JYilJ"
      },
      "source": [
        "class PandasDataset(Dataset):\n",
        "    def __init__(self, df:pd.DataFrame, transforms):\n",
        "        self._dataframe  = df\n",
        "        self._transforms = transforms\n",
        "        self._gray_scale = T.Grayscale(num_output_channels=3)\n",
        "    \n",
        "    @property\n",
        "    def transforms(self):\n",
        "        return self._transforms\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._dataframe)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        im = self._dataframe['image_id'][idx]\n",
        "        \n",
        "        # Load and apply transformations to the Image\n",
        "        im = cv2.imread(im)\n",
        "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "        im = self._transforms(image=im)[\"image\"]\n",
        "\n",
        "        # convert the Images to grayscale Images with 3 dimensions\n",
        "        im = self._gray_scale(im)\n",
        "        \n",
        "        lbl = self._dataframe[\"cat_label\"][idx]   \n",
        "        return im, lbl"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NHWPxawYj4I"
      },
      "source": [
        "Lets define what we want our transforms to be -\n",
        "\n",
        "We'll use the presize the images 160 px make things faster still, and will center crop to 128 px.\n",
        "Initially I will not be using any other fancy data augmentations. After the resizing I will normalize the Images by dividing by 255.0 and also converting them to pytorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsS6h-b9Ymb6"
      },
      "source": [
        "PRESIZE  = 80\n",
        "IMG_SIZE = 64\n",
        "\n",
        "base_tfms = A.Compose([\n",
        "    A.Resize(PRESIZE, PRESIZE, p=1.0),\n",
        "    A.CenterCrop(IMG_SIZE, IMG_SIZE, p=1.0),\n",
        "    A.ToFloat(max_value=255, p=1.0),\n",
        "    ToTensorV2(p=1.0),\n",
        "])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZCN6NMcvwhI"
      },
      "source": [
        "Function to get our data -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqV_2ZmOvvJB"
      },
      "source": [
        "def get_data(transforms = base_tfms, valid_transforms=base_tfms, bs: int = 32):\n",
        "    # instantiate the dataset set obj\n",
        "    train_ds = PandasDataset(df=TRAIN_DATASET_01, transforms=transforms)\n",
        "    valid_ds = PandasDataset(df=VALID_DATASET_01, transforms=valid_transforms)\n",
        "\n",
        "    # create the dataloaders obj\n",
        "    train_dl = DataLoader(train_ds, batch_size=bs, num_workers=num_cpus(), shuffle=True) \n",
        "    valid_dl = DataLoader(valid_ds, batch_size=bs, num_workers=num_cpus(), shuffle=False)\n",
        "    return train_dl, valid_dl"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8gP2ClSYq2J"
      },
      "source": [
        "Having a look at the images in our `DataLoader` you can see result of the image transforms:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ceVZHQ6YtWi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "76870837-c603-4f4c-e845-9275403ae5e6"
      },
      "source": [
        "train_dl, valid_dl = get_data(transforms=base_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "ims, lbls = next(iter(train_dl))\n",
        "ims, lbls = ims[:8], lbls[:8]\n",
        "\n",
        "grid = make_grid(ims, normalize=True).permute(1, 2, 0)\n",
        "fig = plt.figure(figsize=(13, 13))\n",
        "plt.imshow(grid) \n",
        "plt.title([CLASS_MAP[o] for o in lbls.data.cpu().numpy()])\n",
        "plt.axis(\"off\");\n",
        "\n",
        "del train_dl, valid_dl"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 936x936 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD2fNoNqbWv1"
      },
      "source": [
        "### PyTorch Lightning Task -\n",
        "For training I will be using PyTorch Lightning ⚡️. PyTorch Lightning expects the whole training pipeline to be defined under a `LightningModule`. Let's convert our model in a `LightningModule`.\n",
        "\n",
        "For evaluating the model initially I will use the `Accuracy` metric and for loss function i will use the standart `nn.CrossEntropyLoss()` from PyTorch.\n",
        "\n",
        "\n",
        "The code block below create a general PyTorch Lightning Classification Task which helps us iterate different models inside a `LightningModule.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HPiGTulY7ma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e2ce199-773e-4ca1-853d-beadb2201fb8"
      },
      "source": [
        "import gc\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from IPython.display import Markdown, display\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.metrics import functional as FM\n",
        "from torch import nn, optim\n",
        "from torch.nn.utils import spectral_norm\n",
        "\n",
        "from progress import NotebookProgressCallback\n",
        "\n",
        "pl.seed_everything(42)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veOLW0L2btN8"
      },
      "source": [
        "class ClassificationTask(pl.LightningModule):\n",
        "    def __init__(self, model: nn.Module, lr: float, wd: float = 0, criterion: nn.Module = nn.CrossEntropyLoss()):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(\"lr\", \"wd\")\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        \"Same as nn.Module forward\"\n",
        "        return self.model(xb)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "         x, y = batch\n",
        "         y_hat = self.model(x)\n",
        "         loss = self.criterion(y_hat, y)\n",
        "         self.log(\"train_loss\", loss)\n",
        "         return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx, *args, **kwargs):\n",
        "        x, y = batch\n",
        "        y_hat = self.model(x)\n",
        "        loss = self.criterion(y_hat, y)\n",
        "        acc = FM.accuracy(F.softmax(y_hat), y)\n",
        "        metrics = {'val_acc': acc, 'val_loss': loss}\n",
        "        self.log_dict(metrics)\n",
        "        return metrics\n",
        "\n",
        "    def test_step(self, batch, batch_idx, *args, **kwargs):\n",
        "        metrics = self.validation_step(batch, batch_idx, *args, **kwargs)\n",
        "        metrics = {'test_acc': metrics['val_acc'], 'test_loss': metrics['val_loss']}\n",
        "        self.log_dict(metrics)\n",
        "        \n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        define optimizers and LR schedulers for use in training.\n",
        "        \"\"\"\n",
        "        # default Adam parameters from fast.ai\n",
        "        opt   = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.wd, betas=(0.9, 0.99), eps=1e-05)\n",
        "        steps = len(self.train_dataloader())\n",
        "        epochs= self.trainer.max_epochs\n",
        "        \n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(opt, max_lr=self.hparams.lr, epochs=epochs, steps_per_epoch=steps)\n",
        "        return [opt], [dict(scheduler=scheduler, interval='step')]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG688lVucgbE"
      },
      "source": [
        "For training I will be using the popular `AdamW + 1cycle` training approach. Here we use the `AdamW` optimizer and `OneCycleLR` scheduler from PyTorch.\n",
        "\n",
        "`AdamW` was inroduced in this [paper](https://arxiv.org/abs/1711.05101). Experiments by the fast.ai group has shown that `AdamW` along with the `1cycle policy` of learning rate scheduling gives very good results. `AdamW` differs from `Adam` is the sense that AdamW uses true weight decay (decay the weights directly) while Adam uses L2 regularization (add the decay to the gradients).\n",
        "\n",
        "The `1cycle policy` was introduced by Leslie N. Smith et al. in [Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120).\n",
        "\n",
        "\n",
        "`AdamW + 1cycle` was incremental in fast.ai winning the `DAWNBench` competition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka0ItH0LYuu1"
      },
      "source": [
        "### Benchmark Model\n",
        "First I will let's start by making a good baseline Model and subsequently we will experiment by improving upon this model.\n",
        "\n",
        "Let's first create the building blocks that we will use building our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pQi76LYY2jp"
      },
      "source": [
        "class ConvBnDropBlock(nn.Sequential):\n",
        "    \"Create sequence of convolutional, Activation, `BatchNorm` & Drouput layers.\"\n",
        "    def __init__(self, in_chans, out_chans, kernel_size, stride=1, dilation=1, \n",
        "                 padding = None, bias=True, act_cls: Callable = nn.ReLU, \n",
        "                 p_drop=0.0, use_bn=True,):\n",
        "\n",
        "        if padding is None: \n",
        "            padding = (kernel_size-1)//2\n",
        "        \n",
        "        layers = []\n",
        "        \n",
        "        conv_layer = nn.Conv2d(in_chans, out_chans, kernel_size, stride, \n",
        "                               dilation=dilation, padding=padding, bias=bias)\n",
        "        \n",
        "        if act_cls is not None:\n",
        "            act_layer = act_cls(inplace=True)\n",
        "        else:\n",
        "            act_layer = nn.Identity()\n",
        "        if use_bn:\n",
        "            norm_layer = nn.BatchNorm2d(out_chans)\n",
        "        else:\n",
        "            norm_layer = nn.Identity()\n",
        "           \n",
        "        layers += [conv_layer, act_layer, norm_layer]\n",
        "        \n",
        "        if p_drop > 0.0:\n",
        "            layers.append(nn.Dropout2d(p=p_drop))\n",
        "            \n",
        "        super(ConvBnDropBlock, self).__init__(*layers)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0wlcZ57kLIX"
      },
      "source": [
        "For this whole experiment my models will be divided into 2 parts a `feature_extractor` and `classifier`. The feature extractor is responsible for generating the feature maps from the Images, while the `classifier` is responsible for final classification.\n",
        "\n",
        "A baseline model will establish a minimum model performance to which all of our other models can be compared, as well as a model architecture that we can use as the basis of study and improvement.\n",
        "\n",
        "A feature extractor of baseline model architecture has been partly inspired from the general architectural principles of the `VGG models` & from this [blog post](https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/).\n",
        "\n",
        "The architecture involves stacking convolutional layers with small `3×3` filters followed by a activation, batchnorm, dropout.Together, these layers form a block, and these blocks can be repeated where the number of filters in each block is increased with the depth of the network such as 32, 64, 128. The first 2 blocks are followed by a max_pool layer while the 3rd layer is follwed by a `pool_flatten` layer i.e., a combination of  AdaptiveAvgPool2d and flatten layer from pytorch.\n",
        "\n",
        "We use a AdaptiveAvgPool layer so that our network is *fully convolutional* and so can work for any image size that is to say that a model trainined on 64x64 images will also work for 120x120 images.\n",
        "\n",
        "This completes the feature extractor part of the model. For the classifier of the model: first, the feature maps output from the feature extraction part of the model must be flattened.\n",
        "We than use a Dropuout layer followed by a Linear layer to generate the predictions.\n",
        "\n",
        "Let's create the network architecture -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwB7Gpy5ZAeX"
      },
      "source": [
        "class BenchmarkModel(nn.Sequential):\n",
        "    def __init__(self, num_outputs: int):\n",
        "        conv_block_01 = ConvBnDropBlock(in_chans=3, out_chans=32, bias=False, \n",
        "                                        use_bn=True, act_cls=nn.ReLU, p_drop=0.25, \n",
        "                                        kernel_size=3)\n",
        "        conv_block_02 = ConvBnDropBlock(in_chans=32, out_chans=32, bias=False, \n",
        "                                        use_bn=True, act_cls=None, p_drop=0.25, \n",
        "                                        kernel_size=3)\n",
        "        pool_block_0  = nn.Sequential(nn.MaxPool2d(2), nn.ReLU())\n",
        "\n",
        "        conv_block_11 = ConvBnDropBlock(in_chans=32, out_chans=64, bias=False, \n",
        "                                        use_bn=True, act_cls=nn.ReLU, p_drop=0.25, \n",
        "                                        kernel_size=3)\n",
        "        conv_block_12 = ConvBnDropBlock(in_chans=64, out_chans=64, bias=False, \n",
        "                                        use_bn=True, act_cls=None, p_drop=0.25, \n",
        "                                        kernel_size=3)\n",
        "        pool_block_1  = nn.Sequential(nn.MaxPool2d(2), nn.ReLU())\n",
        "\n",
        "        conv_block_21 = ConvBnDropBlock(in_chans=64, out_chans=128, bias=False, \n",
        "                                        use_bn=True, act_cls=nn.ReLU, p_drop=0.25, \n",
        "                                        kernel_size=3)\n",
        "        conv_block_22 = ConvBnDropBlock(in_chans=128, out_chans=128, bias=False, stride=2,\n",
        "                                        use_bn=True, act_cls=nn.ReLU, p_drop=0.25, \n",
        "                                        kernel_size=3, padding=0)\n",
        "        \n",
        "        # ensemble the model building blocks\n",
        "        block1 = nn.Sequential(conv_block_01, conv_block_02, pool_block_0)\n",
        "        block2 = nn.Sequential(conv_block_11, conv_block_12, pool_block_1)\n",
        "        block3 = nn.Sequential(conv_block_21, conv_block_22)\n",
        "        pool_flatten = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
        "        fc = nn.Sequential(nn.Dropout(0.3), nn.Linear(128, num_outputs, bias=False))\n",
        "\n",
        "        layers = [block1, block2, block3, pool_flatten, fc]\n",
        "        super(BenchmarkModel, self).__init__(*layers)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdNxn-amZEhK"
      },
      "source": [
        "*Lets Get Training*\n",
        "\n",
        "----\n",
        "\n",
        "Train the model initially for 20 epochs and view preformance. I found that 20 epochs was a good place to stop as the model started to overfit while the accuracy didn't improve further and any lower than 20 epochs the model doesn't convergerce.\n",
        "\n",
        "\n",
        "I will also be using `ModelCheckpoint` callback from pytorch lightning in all of my experiments to that i will be easily able to load my best checkpoints and compare my performaces.\n",
        "\n",
        "\n",
        "`3e-3` is often a good learning rate for CNNs, so let's try that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbttRUWmScwB"
      },
      "source": [
        "# BenchmarkModel(len(CLASS_MAP))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2OfLQXxZHcQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ff54bd94-e866-4a7f-b237-313e8071b11e"
      },
      "source": [
        "exp_name = \"stage-01-cnn-bench\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms, bs=32)\n",
        "\n",
        "# instantiate the model\n",
        "model = BenchmarkModel(num_outputs=len(CLASS_MAP))\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=20, callbacks=cbs, gpus=1, precision=16)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | BenchmarkModel   | 295 K \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "295 K     Trainable params\n",
            "0         Non-trainable params\n",
            "295 K     Total params\n",
            "1.182     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='1240' max='1240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1240/1240 06:49, Epoch 19 {'loss': '1.21', 'v_num': 0}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.018145</td>\n",
              "      <td>4.164146</td>\n",
              "      <td>4.083337</td>\n",
              "      <td>20.514600</td>\n",
              "      <td>3.802200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.024194</td>\n",
              "      <td>4.051664</td>\n",
              "      <td>4.101730</td>\n",
              "      <td>20.556700</td>\n",
              "      <td>3.794400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.042339</td>\n",
              "      <td>3.959492</td>\n",
              "      <td>3.920065</td>\n",
              "      <td>20.361300</td>\n",
              "      <td>3.830800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>3.648189</td>\n",
              "      <td>3.725063</td>\n",
              "      <td>20.791200</td>\n",
              "      <td>3.751600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.127016</td>\n",
              "      <td>3.462666</td>\n",
              "      <td>3.521999</td>\n",
              "      <td>20.672600</td>\n",
              "      <td>3.773100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.203629</td>\n",
              "      <td>3.164064</td>\n",
              "      <td>3.171767</td>\n",
              "      <td>20.578500</td>\n",
              "      <td>3.790400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.292339</td>\n",
              "      <td>2.891078</td>\n",
              "      <td>3.190326</td>\n",
              "      <td>20.361300</td>\n",
              "      <td>3.830800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.370968</td>\n",
              "      <td>2.543974</td>\n",
              "      <td>2.688081</td>\n",
              "      <td>20.266600</td>\n",
              "      <td>3.848700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.518145</td>\n",
              "      <td>2.194857</td>\n",
              "      <td>2.447881</td>\n",
              "      <td>20.384900</td>\n",
              "      <td>3.826400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.570565</td>\n",
              "      <td>1.900911</td>\n",
              "      <td>2.363772</td>\n",
              "      <td>20.456300</td>\n",
              "      <td>3.813000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.631048</td>\n",
              "      <td>1.679571</td>\n",
              "      <td>2.157038</td>\n",
              "      <td>20.241100</td>\n",
              "      <td>3.853600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.679435</td>\n",
              "      <td>1.459724</td>\n",
              "      <td>1.713408</td>\n",
              "      <td>20.392200</td>\n",
              "      <td>3.825000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.701613</td>\n",
              "      <td>1.362067</td>\n",
              "      <td>1.861366</td>\n",
              "      <td>20.354800</td>\n",
              "      <td>3.832000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.727823</td>\n",
              "      <td>1.238234</td>\n",
              "      <td>1.431057</td>\n",
              "      <td>20.295000</td>\n",
              "      <td>3.843300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.756048</td>\n",
              "      <td>1.136020</td>\n",
              "      <td>1.213323</td>\n",
              "      <td>20.121400</td>\n",
              "      <td>3.876500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.760081</td>\n",
              "      <td>1.119743</td>\n",
              "      <td>1.493723</td>\n",
              "      <td>20.455300</td>\n",
              "      <td>3.813200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.752016</td>\n",
              "      <td>1.067767</td>\n",
              "      <td>1.310942</td>\n",
              "      <td>20.782200</td>\n",
              "      <td>3.753200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.770161</td>\n",
              "      <td>1.040585</td>\n",
              "      <td>1.491052</td>\n",
              "      <td>20.987700</td>\n",
              "      <td>3.716500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.772177</td>\n",
              "      <td>1.036213</td>\n",
              "      <td>1.386177</td>\n",
              "      <td>20.847900</td>\n",
              "      <td>3.741400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.770161</td>\n",
              "      <td>1.039185</td>\n",
              "      <td>1.231519</td>\n",
              "      <td>20.350200</td>\n",
              "      <td>3.832900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.815**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.772**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRAlFbD1ZeYh"
      },
      "source": [
        "That's a pretty good start, considering we have to pick the correct one of 62 categories, and we're training from scratch. So in the subsequent sections we will explore tricks that can probably help us to improve our results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqlNrojraX5-"
      },
      "source": [
        "try:\n",
        "    del model, trainer, task\n",
        "except: \n",
        "    pass\n",
        "try:\n",
        "    del train_dl, valid_dl\n",
        "except: \n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bms8tB0La3Z4"
      },
      "source": [
        "### Improving the Benchmark \n",
        "> This section will explore various techniques which will hopefully help us in improving our baseline model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH0VC7qNbD1S"
      },
      "source": [
        "#### Residual Blocks -\n",
        "\n",
        "In our above trained `CNN` model We can do way better than the current results using a deeper mode, but just stacking new layers won't really improve our results.\n",
        "In this experiment we will explore the preformance of our model by introducing `residual connection`s. It was introduced in 2015 by Kaiming He et al. in the article \"[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\". Residual Connections are the main building blocks of the ResNet family of networks, which are one of the most popular CNN Architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq6vibo_vvi1"
      },
      "source": [
        "Here's the definition of a simple ResNet block -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGhgvOpXdjjG"
      },
      "source": [
        "class ResBlock(nn.Module):\n",
        "    \"Creates a simple Residual Block\"\n",
        "    def __init__(self, in_chans: int, out_chans: int, \n",
        "                 kernel_size: int, stride: int = 1, act_cls: Callable = nn.ReLU):\n",
        "        \n",
        "        super(ResBlock, self).__init__()\n",
        "        self.idconv = None\n",
        "        \n",
        "        self.block1 = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=kernel_size, \n",
        "                                      stride=stride, padding=None, use_bn=True, act_cls=act_cls, \n",
        "                                      p_drop=0.0, bias=False)\n",
        "        \n",
        "        self.block2 = ConvBnDropBlock(in_chans=out_chans, out_chans=out_chans, kernel_size=kernel_size, \n",
        "                                      stride=1, padding=None, use_bn=True, act_cls=None, p_drop=0.0, bias=False)\n",
        "        \n",
        "        self.act_cls = act_cls(inplace=True)\n",
        "        \n",
        "        if in_chans != out_chans or stride != 1:\n",
        "            self.idconv = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=1, \n",
        "                                          stride=stride, padding=0, use_bn=True, act_cls=None, \n",
        "                                          p_drop=0.0, bias=False)\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "         \n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        for m in self.block2.modules():\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 0)\n",
        "                    \n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.block1(x)\n",
        "        out = self.block2(out)\n",
        "        \n",
        "        if self.idconv is not None:\n",
        "            identity = self.idconv(x)\n",
        "        \n",
        "        out += identity\n",
        "        return self.act_cls(out)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YmH2dP_jcg9"
      },
      "source": [
        "Why use `idconv` block ?\n",
        ">The issue is that with a stride of, say, 2 on one of the convolutions, the grid size of the output activations will be half the size on each axis of the input. So then we can't add that back to x in forward because x and the output activations have different dimensions. The same basic issue occurs if `in_chans`!=`out_chans`: the shapes of the input and output connections won't allow us to add them together. So `idconv` works as a identity map that matches the dimenstions of x to the ouput of `self.block2(self.block1(x))`. `idconv` increases the `channels` out the x and downsamples it with stride if required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6V7-ww3wA8b"
      },
      "source": [
        "We can now proceed towards modifying our BenchMark model. Let's add some skip connections into this network. We remove modify the 1st conv block of the model to resemble the layer in a typical `ResNet` Model, i.e, a conv layer (output_channels=64, kernel_size=7, stride=2) followed by a batch_normalization, activation and a MaxPooling layer. We then add the residual blocks of the model. The classifier of the model remains same.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N0geEarjcry"
      },
      "source": [
        "class ResModel(nn.Sequential):\n",
        "    def __init__(self, num_outputs: int):\n",
        "        conv = ConvBnDropBlock(in_chans=3, out_chans=64, kernel_size=7, p_drop=0, stride=2, \n",
        "                               act_cls=nn.ReLU, bias=False, use_bn=True)\n",
        "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        conv_stem  = nn.Sequential(conv, pool)\n",
        "        block1 = ResBlock(in_chans=64, out_chans=64,  stride=1, kernel_size=3, act_cls=nn.ReLU)\n",
        "        block2 = ResBlock(in_chans=64, out_chans=128, stride=2, kernel_size=3, act_cls=nn.ReLU)\n",
        "        block3 = ResBlock(in_chans=128,out_chans=256, stride=2, kernel_size=3, act_cls=nn.ReLU)\n",
        "        pool_flatten = nn.Sequential(OrderedDict(pool_layer=nn.AdaptiveAvgPool2d(1), flatten=nn.Flatten()))\n",
        "        fc = nn.Sequential(nn.Dropout(0.25), nn.Linear(256, num_outputs))\n",
        "        layers = OrderedDict(stem=conv_stem, block1=block1, block2=block2, block3=block3, pool_flatten=pool_flatten, fc=fc)\n",
        "        super(ResModel, self).__init__(layers)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9O0g6ihyEpp"
      },
      "source": [
        "Since this is a larger model than the previous one I will reduce the number of training epochs so that our model does not overfit the training data and quick experimentation purposes.\n",
        "\n",
        "Let's train it for a little bit and see how it fares compared to the previous model - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuvF6ax3jc1h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "outputId": "9306f8d3-5c2d-44ea-990b-3d5ff2dbc30e"
      },
      "source": [
        "exp_name = \"stage-01-resblock\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms, bs=32)\n",
        "\n",
        "# instantiate the model\n",
        "model = ResModel(num_outputs=len(CLASS_MAP))\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=8, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | ResModel         | 1.2 M \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "1.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.2 M     Total params\n",
            "4.995     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='496' max='496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [496/496 02:48, Epoch 7 {'loss': '0.318', 'v_num': 1}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.030242</td>\n",
              "      <td>4.094044</td>\n",
              "      <td>4.094388</td>\n",
              "      <td>20.643200</td>\n",
              "      <td>3.778500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.100806</td>\n",
              "      <td>3.638201</td>\n",
              "      <td>3.616361</td>\n",
              "      <td>21.197900</td>\n",
              "      <td>3.679600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.264113</td>\n",
              "      <td>2.565619</td>\n",
              "      <td>2.462466</td>\n",
              "      <td>21.011700</td>\n",
              "      <td>3.712200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.528226</td>\n",
              "      <td>1.672508</td>\n",
              "      <td>1.578267</td>\n",
              "      <td>21.185900</td>\n",
              "      <td>3.681700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.671371</td>\n",
              "      <td>1.171452</td>\n",
              "      <td>1.107051</td>\n",
              "      <td>21.586400</td>\n",
              "      <td>3.613400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.776210</td>\n",
              "      <td>0.813072</td>\n",
              "      <td>0.511971</td>\n",
              "      <td>21.030100</td>\n",
              "      <td>3.709000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.814516</td>\n",
              "      <td>0.697436</td>\n",
              "      <td>0.344715</td>\n",
              "      <td>20.859500</td>\n",
              "      <td>3.739300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.830645</td>\n",
              "      <td>0.663033</td>\n",
              "      <td>0.275049</td>\n",
              "      <td>21.036900</td>\n",
              "      <td>3.707800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.975**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.831**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l43AbZbd0tIu"
      },
      "source": [
        "**The accuracy of our model has increased by adding the skip connections. We were able to improve our accuracy in a fewer epochs compared to the previous model.**\n",
        "\n",
        "If we had trained more I am not sure sure if the model performance would have increased on the validation data. This is because the model is overfitting on the training data and we do not want that. What we want is, that the model performs well in unseen data (or the validation dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQBcyfSoMNpR"
      },
      "source": [
        "**Note: At this point it is crearly evident that currently our model is overfitting on the training data. But we will work on improving overfitting. First I want to try different model architectures and find the most optimal after which I will use *regularization* techniques to further improve the model and make it more robust.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcw2YCZullQu"
      },
      "source": [
        "try:\n",
        "    del model, trainer, task\n",
        "except: \n",
        "    pass\n",
        "try:\n",
        "    del train_dl, valid_dl\n",
        "except: \n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFNrOLQ2nRgF"
      },
      "source": [
        "#### A State-of-the-Art ResNets -\n",
        "> In this part I will mainly try to explore different variant of `ResNet`s called `ResNet-D` proposed by Tong He in 2014 in the article \"[Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/abs/1812.01187)\n",
        "\". By using a tweaked ResNet-50 architecture and Mixup they achieved 94.6% top-5 accuracy on ImageNet, in comparison to 92.2% with a regular ResNet-50 without Mixup. This result is better than that achieved by regular ResNet models that are twice as deep (and twice as slow, and much more likely to overfit). These tweaked `Resnet` variants was ultimately popularized Jeremy Howard of Fast.ai and are called as `xResNet`'s.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BPqJNZRnT9K"
      },
      "source": [
        "This experiment will mainly explore the model preformace we replace the Residual blocks with xResidual Blocks.\n",
        "\n",
        "First, let us explore the architecture of `ResNet-D` proposed in the above named article. To obtain the `xResNet` architecture we have to apply three different tweaks in the `ResNet` architecture namely `ResNet-B`, `ResNet-C` and `ResNet-D`. \n",
        "\n",
        "The notable changes in the model architecture in our case would be :\n",
        "\n",
        "* In `ResidualCnn`, `resblock2` we are downsampling the input by applying a convulation `stride=2` in the 1st layer of the residual block. `ResNet-B` simply moves the stride `2` to the second convolution and keeps a stride of `1` for the first layer .\n",
        "* The `ResNet-C`, proposed in Inception-v2, removes the `7x7` convolution in the input stem of the network and replaces it with three consecutive 3x3 convolutions. \n",
        "* In `ResNet-D`, the authors replaced the convolution in the downsampling block with a `2x2` average-pooling layer of stride `2` followed by a `1x1` convolution layer. In our case we would have to replace the `idconv` present in `ResidualBlock` module.\n",
        "\n",
        "Let's apply the above changes to our model test the preformance -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjX2mDHonZwy"
      },
      "source": [
        "class xResBlock(nn.Module):\n",
        "    \"Creates a simple Residual Block for xResNet architecture\"\n",
        "    def __init__(self, in_chans, out_chans, kernel_size, stride=1, act_cls=nn.ReLU):\n",
        "        super(xResBlock, self).__init__()\n",
        "        self.idconv = None\n",
        "        \n",
        "        self.block1 = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=kernel_size,\n",
        "                                      stride=1, use_bn=True, act_cls=act_cls, bias=False)\n",
        "        \n",
        "        # we apply the 1st change here, \n",
        "        # moving the stride 2 to the second convolution and keeps a stride of 1 for the first layer . \n",
        "        self.block2 = ConvBnDropBlock(in_chans=out_chans, out_chans=out_chans, kernel_size=kernel_size, \n",
        "                                      stride=stride, use_bn=True, act_cls=None, bias=False)\n",
        "        \n",
        "        self.act_cls = act_cls(inplace=True)\n",
        "        \n",
        "        if in_chans != out_chans or stride != 1:\n",
        "            # the 3rd change proped above is applied here,\n",
        "            # we replace with a 2x2 average-pooling layer of stride 2 followed by a 1x1 convolution layer\n",
        "            pool_layer = nn.AvgPool2d(stride, ceil_mode=True)\n",
        "            conv_layer = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=1, padding=0, \n",
        "                                         use_bn=True, act_cls=None, p_drop=0.0, bias=False, stride=1)\n",
        "            \n",
        "            self.idconv = nn.Sequential(pool_layer, conv_layer)\n",
        "            \n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "         \n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        for m in self.block2.modules():\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 0)\n",
        "                    \n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.block1(x)\n",
        "        out = self.block2(out)\n",
        "        \n",
        "        if self.idconv is not None:\n",
        "            identity = self.idconv(x)\n",
        "        \n",
        "        out += identity\n",
        "        return self.act_cls(out)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xahTgOcNne14"
      },
      "source": [
        "The code is, for the most part, taken & modified from the [fast.ai course](https://www.fast.ai/), [fast.ai]() library and this [blog post](https://towardsdatascience.com/xresnet-from-scratch-in-pytorch-e64e309af722). \n",
        "\n",
        "Let's move incorporate this into the model that we have been working with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQVrmb4Dnj4v"
      },
      "source": [
        "class xResModel(nn.Sequential):\n",
        "    def __init__(self, num_outputs: int, act_cls=nn.ReLU):\n",
        "        conv = nn.Sequential(\n",
        "            ConvBnDropBlock(in_chans=3,  out_chans=32, kernel_size=3, stride=2, act_cls=act_cls, bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=32, kernel_size=3, stride=1, act_cls=act_cls, bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=64, kernel_size=3, stride=1, act_cls=act_cls, bias=False, use_bn=False),\n",
        "        )\n",
        "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        conv_stem  = nn.Sequential(conv, pool)\n",
        "        block1 = xResBlock(in_chans=64, out_chans=64,  stride=1, kernel_size=3, act_cls=act_cls)\n",
        "        block2 = xResBlock(in_chans=64, out_chans=128, stride=2, kernel_size=3, act_cls=act_cls)\n",
        "        block3 = xResBlock(in_chans=128,out_chans=256, stride=2, kernel_size=3, act_cls=act_cls)\n",
        "        pool_flatten = nn.Sequential(OrderedDict(pool=nn.AdaptiveAvgPool2d(1), flatten=nn.Flatten()))\n",
        "        fc = nn.Sequential(nn.Dropout(0.25), nn.Linear(256,num_outputs))\n",
        "        layers = OrderedDict(stem=conv_stem, block1=block1, block2=block2, block3=block3, pool_flatten=pool_flatten, fc=fc)\n",
        "        super(xResModel, self).__init__(layers)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t99NKFfsL6D"
      },
      "source": [
        "Let's train it and see how it fares compared to the previous model -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJYIBu7Bo95m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "outputId": "db41190b-81ba-4ac8-f389-2bc55fad0d88"
      },
      "source": [
        "exp_name = \"stage-01-xresblock\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms, bs=32)\n",
        "\n",
        "# instantiate the model\n",
        "model = xResModel(num_outputs=len(CLASS_MAP))\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=8, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | xResModel        | 1.3 M \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "1.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.3 M     Total params\n",
            "5.070     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='496' max='496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [496/496 02:44, Epoch 7 {'loss': '0.34', 'v_num': 2}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.042339</td>\n",
              "      <td>4.090287</td>\n",
              "      <td>3.950568</td>\n",
              "      <td>20.761700</td>\n",
              "      <td>3.756900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.028226</td>\n",
              "      <td>6.351081</td>\n",
              "      <td>2.974320</td>\n",
              "      <td>20.798400</td>\n",
              "      <td>3.750300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.116935</td>\n",
              "      <td>3.684258</td>\n",
              "      <td>1.924184</td>\n",
              "      <td>20.709600</td>\n",
              "      <td>3.766400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>2.786000</td>\n",
              "      <td>1.115625</td>\n",
              "      <td>20.619700</td>\n",
              "      <td>3.782800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.667339</td>\n",
              "      <td>1.059044</td>\n",
              "      <td>0.748843</td>\n",
              "      <td>20.555100</td>\n",
              "      <td>3.794700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.832661</td>\n",
              "      <td>0.638534</td>\n",
              "      <td>0.568440</td>\n",
              "      <td>20.488200</td>\n",
              "      <td>3.807100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.848790</td>\n",
              "      <td>0.528997</td>\n",
              "      <td>0.415080</td>\n",
              "      <td>20.725400</td>\n",
              "      <td>3.763500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.848790</td>\n",
              "      <td>0.516791</td>\n",
              "      <td>0.291542</td>\n",
              "      <td>20.405700</td>\n",
              "      <td>3.822500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.941**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.849**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFfabjLSUrQy"
      },
      "source": [
        "Validation accuracy has increased a lot but we are still overfitting on the training data , let's explore more ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAOfe5FRtVtf"
      },
      "source": [
        "try:\n",
        "    del model, trainer\n",
        "except: \n",
        "    pass\n",
        "try:\n",
        "    del train_dl, valid_dl\n",
        "except: \n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bosj5r96zCeG"
      },
      "source": [
        "#### Exploring State-of-the-Art from ImageWoof competition - \n",
        "> In this experiment I plan to use some of [State of the Art Training techniques](https://forums.fast.ai/t/how-we-beat-the-5-epoch-imagewoof-leaderboard-score-some-new-techniques-to-consider/53453) that were used to top the leaderboard score for 5 epoch Imagewoof`.\n",
        "\n",
        "\n",
        "\n",
        "The techniques I will be employing are as follows - \n",
        "* `Mish activation function instead of ReLU` : Mish is a new activation function that was released in a [paper](https://arxiv.org/abs/1908.08681). It has a much smoother curve vs relu, and in theory, that drives information more deeply through the network.\n",
        "* `Self attention layer` : Bringing in Ideas from GAN's into CNN's.The self attention layer is designed to help leverage long range dependencies within an image vs the more local feature focus of convolutions. Paper Link : [https://arxiv.org/abs/1805.08318]\n",
        "\n",
        "\n",
        "\n",
        "The folks from fast.ai who employed the above techniques noticed that `SimpleSelfAttention` layer, when placed within a Resnet block, the network converges if `SimpleSelfAttention` is placed right after a convolution layer that uses batch norm, and initializes the batchnorm weights to 0.\n",
        "\n",
        "[Source](https://github.com/sdoria/SimpleSelfAttention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ElXz7VPzCg2"
      },
      "source": [
        "The code for creating the `SimpleSelfAttention` module has been borrowed from [here](https://github.com/sdoria/SimpleSelfAttention/blob/master/xresnet.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLHtIG9RzCmC"
      },
      "source": [
        "def conv1d(ni:int, no:int, ks:int=1, stride:int=1, padding:int=0, bias:bool=False):\n",
        "    \"Create and initialize a `nn.Conv1d` layer with spectral normalization.\"\n",
        "    conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n",
        "    nn.init.kaiming_normal_(conv.weight)\n",
        "    if bias: conv.bias.data.zero_()\n",
        "    return spectral_norm(conv)\n",
        "\n",
        "\n",
        "class SimpleSelfAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_in:int, ks=1, sym=False):\n",
        "        super().__init__()\n",
        "           \n",
        "        self.conv = conv1d(n_in, n_in, ks, padding=ks//2, bias=False)      \n",
        "       \n",
        "        self.gamma = nn.Parameter(torch.tensor([0.]))\n",
        "        \n",
        "        self.sym = sym\n",
        "        self.n_in = n_in\n",
        "        \n",
        "    def forward(self,x):\n",
        "          \n",
        "        if self.sym:\n",
        "            # symmetry hack by https://github.com/mgrankin\n",
        "            c = self.conv.weight.view(self.n_in,self.n_in)\n",
        "            c = (c + c.t())/2\n",
        "            self.conv.weight = c.view(self.n_in,self.n_in,1)\n",
        "                \n",
        "        size = x.size()  \n",
        "        x = x.view(*size[:2],-1)\n",
        "        \n",
        "        convx = self.conv(x)\n",
        "        xxT = torch.bmm(x,x.permute(0,2,1).contiguous())\n",
        "        \n",
        "        o = torch.bmm(xxT, convx)\n",
        "          \n",
        "        o = self.gamma * o + x\n",
        "        return o.view(*size).contiguous()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rCMGxmSzCoi"
      },
      "source": [
        "The code block implements the `Mish` Activation function, the code for which has been borrowed from the [repository](https://github.com/digantamisra98/Mish/blob/master/Mish/Torch/functional.py) of the original author of `Mish`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv_KzkeLzYVI"
      },
      "source": [
        "@torch.jit.script\n",
        "def mish(input):\n",
        "    '''\n",
        "    Applies the mish function element-wise:\n",
        "    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n",
        "    '''\n",
        "    return input * torch.tanh(F.softplus(input))\n",
        "\n",
        "\n",
        "class Mish(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(Mish, self).__init__()\n",
        "    \n",
        "    def forward(self, xb):\n",
        "        return mish(xb)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx6N4e0WzYYS"
      },
      "source": [
        "The code below adds SelfAttention and Mish activation to our *x*ResNet based model architecture -\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9sspbJozYbG"
      },
      "source": [
        "class sa_m_xResBlock(nn.Module):\n",
        "    \"Creates a xRes Block with SelfAttention and Mish Activation\"\n",
        "    def __init__(self, in_chans: int, out_chans: int, \n",
        "                 kernel_size: int, stride: int = 1, act_cls: Callable = Mish):\n",
        "        \n",
        "        super(sa_m_xResBlock, self).__init__()\n",
        "        self.idconv = None\n",
        "        \n",
        "        self.block1 = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=kernel_size,\n",
        "                                      stride=1, use_bn=True, act_cls=act_cls, bias=False)\n",
        "         \n",
        "        self.block2 = ConvBnDropBlock(in_chans=out_chans, out_chans=out_chans, kernel_size=kernel_size, \n",
        "                                      stride=stride, use_bn=True, act_cls=None, bias=False)\n",
        "        \n",
        "        # initialize the SimpleSelfAttention Layer\n",
        "        self.sa = SimpleSelfAttention(out_chans,ks=1,sym=False)    \n",
        "        \n",
        "        self.act_cls = act_cls(inplace=True)\n",
        "        \n",
        "        if in_chans != out_chans or stride != 1:\n",
        "            pool_layer = nn.AvgPool2d(stride, ceil_mode=True)\n",
        "            conv_layer = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=1, padding=0, \n",
        "                                         use_bn=True, act_cls=None, p_drop=0.0, bias=False, stride=1)\n",
        "            \n",
        "            self.idconv = nn.Sequential(pool_layer, conv_layer)       \n",
        "            \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                \n",
        "        for m in self.block2.modules():\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 0)\n",
        "                    \n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.block1(x)\n",
        "        out = self.block2(out)\n",
        "        out = self.sa(out)\n",
        "        \n",
        "        if self.idconv is not None:\n",
        "            identity = self.idconv(x)\n",
        "        \n",
        "        out += identity\n",
        "        return self.act_cls(out)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TizKT22OHRbU"
      },
      "source": [
        "Let's build up the model : The code below incorporates SelfAttention and Mish into our xResnet based model ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGoJKgBZzmVi"
      },
      "source": [
        "class sa_m_xResModel(nn.Sequential):\n",
        "    def __init__(self, num_outputs: int, act_cls=Mish):\n",
        "        conv = nn.Sequential(\n",
        "            ConvBnDropBlock(in_chans=3,  out_chans=32, kernel_size=3, stride=2, act_cls=act_cls, bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=32, kernel_size=3, stride=1, act_cls=act_cls, bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=64, kernel_size=3, stride=1, act_cls=act_cls, bias=False, use_bn=True),\n",
        "            )\n",
        "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        conv_stem  = nn.Sequential(conv, pool)    \n",
        "        block1 = sa_m_xResBlock(in_chans=64, out_chans=64,  stride=1, kernel_size=3, act_cls=act_cls)\n",
        "        block2 = sa_m_xResBlock(in_chans=64, out_chans=128, stride=2, kernel_size=3, act_cls=act_cls)\n",
        "        block3 = sa_m_xResBlock(in_chans=128,out_chans=256, stride=2, kernel_size=3, act_cls=act_cls)\n",
        "        pool_flatten = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
        "        fc = nn.Sequential(nn.Dropout(0.25), nn.Linear(256,num_outputs))\n",
        "        layers = OrderedDict(stem=conv_stem, block1=block1, block2=block2, block3=block3, pool_flatten=pool_flatten, fc=fc)\n",
        "        super(sa_m_xResModel, self).__init__(layers)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teBv5VmoZW2p"
      },
      "source": [
        "Train the model -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR6qi9tuzmY1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "outputId": "7f4de353-b4e3-4cc2-8c8a-da1575d18633"
      },
      "source": [
        "exp_name = \"stage-01-xresblock-mish-sa\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "# instantiate the model\n",
        "model = sa_m_xResModel(num_outputs=len(CLASS_MAP))\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=8, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | sa_m_xResModel   | 1.4 M \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.415     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='496' max='496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [496/496 02:52, Epoch 7 {'loss': '0.25', 'v_num': 3}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.026210</td>\n",
              "      <td>4.090721</td>\n",
              "      <td>3.947571</td>\n",
              "      <td>24.338400</td>\n",
              "      <td>3.204800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.018145</td>\n",
              "      <td>32.452587</td>\n",
              "      <td>2.550184</td>\n",
              "      <td>21.767300</td>\n",
              "      <td>3.583400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.358871</td>\n",
              "      <td>2.223726</td>\n",
              "      <td>2.090670</td>\n",
              "      <td>21.627700</td>\n",
              "      <td>3.606500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>6.525522</td>\n",
              "      <td>1.625340</td>\n",
              "      <td>21.899100</td>\n",
              "      <td>3.561800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.655242</td>\n",
              "      <td>1.129691</td>\n",
              "      <td>0.804478</td>\n",
              "      <td>21.574500</td>\n",
              "      <td>3.615400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.665323</td>\n",
              "      <td>1.234517</td>\n",
              "      <td>0.386030</td>\n",
              "      <td>21.464200</td>\n",
              "      <td>3.634000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.858871</td>\n",
              "      <td>0.479888</td>\n",
              "      <td>0.214887</td>\n",
              "      <td>21.504400</td>\n",
              "      <td>3.627200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.856855</td>\n",
              "      <td>0.456381</td>\n",
              "      <td>0.190485</td>\n",
              "      <td>21.359200</td>\n",
              "      <td>3.651800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.943**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.859**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 22.1 s, sys: 3.21 s, total: 25.3 s\n",
            "Wall time: 3min 19s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP7Nflj-Y5mu"
      },
      "source": [
        "With the above mentioned tweaks our model is performing even better now ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIibTevdzmfR"
      },
      "source": [
        "try:\n",
        "    del model, trainer\n",
        "except:\n",
        "    pass\n",
        "try:\n",
        "    del train_dl, valid_dl\n",
        "except: \n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIaNPUnsPwNZ"
      },
      "source": [
        "#### Exploring Different Classifiers - \n",
        "\n",
        "In this section, I will take the above model and update the classifiers of the model and check their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhHFKDSKQOED"
      },
      "source": [
        "Let's add another block of layers (Dropout -> Linear -> Activation -> BatchNorm) after the `pooling layer` and the `final fc layer`  to the classifier of the model - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxj7575oQOMU"
      },
      "source": [
        "class xResModelv2(nn.Sequential):\n",
        "    def __init__(self, num_outputs: int, act_cls=Mish):\n",
        "        conv = nn.Sequential(\n",
        "            ConvBnDropBlock(in_chans=3,  out_chans=32, kernel_size=3, stride=2, act_cls=act_cls, bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=32, kernel_size=3, stride=1, act_cls=act_cls, bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=64, kernel_size=3, stride=1, act_cls=act_cls, bias=False, use_bn=True),\n",
        "            )\n",
        "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        conv_stem  = nn.Sequential(conv, pool)     \n",
        "        block1 = sa_m_xResBlock(in_chans=64, out_chans=64,  stride=1, kernel_size=3, act_cls=act_cls)\n",
        "        block2 = sa_m_xResBlock(in_chans=64, out_chans=128, stride=2, kernel_size=3, act_cls=act_cls)\n",
        "        block3 = sa_m_xResBlock(in_chans=128,out_chans=256, stride=2, kernel_size=3, act_cls=act_cls)\n",
        "        pool_flatten = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
        "        fc1 = nn.Sequential(nn.Dropout(0.25), nn.Linear(256, 512, bias=False), act_cls(inplace=True), nn.BatchNorm1d(512))\n",
        "        fc2 = nn.Sequential(nn.Dropout(0.25), nn.Linear(512, num_outputs, bias=False)) \n",
        "        layers = OrderedDict(stem=conv_stem, block1=block1, block2=block2, block3=block3, pool_flatten=pool_flatten, fc1=fc1, fc2=fc2)\n",
        "        super(xResModelv2, self).__init__(layers)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNDZOkCsQOVN"
      },
      "source": [
        "Training the model to evaluate performance -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d9q87zmQOcz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "eac5c0b8-6c02-4747-9c67-24b5fca38d30"
      },
      "source": [
        "exp_name = \"stage-02-xres-mish-sa\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "# instantiate the model\n",
        "model = xResModelv2(num_outputs=len(CLASS_MAP), act_cls=Mish)\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=8, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | xResModelv2      | 1.5 M \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "1.5 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.5 M     Total params\n",
            "6.007     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='329' max='496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [329/496 01:51 < 00:56, 2.95 it/s, Epoch 5 {'loss': '0.436', 'v_num': 4}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>7.699153</td>\n",
              "      <td>3.287882</td>\n",
              "      <td>21.629000</td>\n",
              "      <td>3.606300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>48.742329</td>\n",
              "      <td>2.166679</td>\n",
              "      <td>21.671700</td>\n",
              "      <td>3.599200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.040323</td>\n",
              "      <td>21.336399</td>\n",
              "      <td>1.530663</td>\n",
              "      <td>20.959200</td>\n",
              "      <td>3.721500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.100806</td>\n",
              "      <td>12.682330</td>\n",
              "      <td>0.874202</td>\n",
              "      <td>20.761500</td>\n",
              "      <td>3.757000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.274194</td>\n",
              "      <td>3.872468</td>\n",
              "      <td>0.623521</td>\n",
              "      <td>20.928700</td>\n",
              "      <td>3.726900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlvijK6rQOpQ"
      },
      "source": [
        "This time I will replace the classifier and pool layer of the Model with the model head that is created from by `fast.ai` s cnn_learner. The head begins with fastai's AdaptiveConcatPool2d. Then it uses a Flatten layer before going on blocks of BatchNorm, Dropout and Linear layers.\n",
        "\n",
        "[Source](https://docs.fast.ai/vision.learner.html#create_body)\n",
        "\n",
        "Let's build the model :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHPhIMGQauEY"
      },
      "source": [
        "class AdaptiveConcatPool2d(Module):\n",
        "    \"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`\"\n",
        "    def __init__(self, size=None):\n",
        "        self.size = size or 1\n",
        "        self.ap = nn.AdaptiveAvgPool2d(self.size)\n",
        "        self.mp = nn.AdaptiveMaxPool2d(self.size)\n",
        "    def forward(self, x): \n",
        "        return torch.cat([self.mp(x), self.ap(x)], 1)\n",
        "\n",
        "\n",
        "class FastaiHead(nn.Sequential):\n",
        "    def __init__(self, nf: int, num_outputs: int, act_cls=Mish):\n",
        "        l1 = nn.BatchNorm1d(nf)\n",
        "        l2 = nn.Dropout(p=0.25)\n",
        "        l3 = nn.Linear(in_features=nf, out_features=512, bias=False)\n",
        "        l4 = act_cls(inplace=True)\n",
        "        l5 = nn.BatchNorm1d(512)\n",
        "        l6 = nn.Dropout(p=0.5)\n",
        "        fc = nn.Linear(in_features=512, out_features=num_outputs, bias=False)\n",
        "\n",
        "        layers = OrderedDict(fc1=nn.Sequential(l1, l2, l3, l4, l5, l6), fc2=fc)\n",
        "        super(FastaiHead, self).__init__(layers)\n",
        "\n",
        "\n",
        "# Incorporate these into our xResNet based model :\n",
        "class xResModelv3(nn.Sequential):\n",
        "    def __init__(self, num_outputs: int, act_cls=Mish):\n",
        "        conv = nn.Sequential(\n",
        "            ConvBnDropBlock(in_chans=3,  out_chans=32, kernel_size=3, stride=2, act_cls=act_cls, bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=32, kernel_size=3, stride=1, act_cls=act_cls, bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=64, kernel_size=3, stride=1, act_cls=act_cls, bias=False, use_bn=True),\n",
        "            )\n",
        "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        conv_stem  = nn.Sequential(conv, pool)     \n",
        "        block1 = sa_m_xResBlock(in_chans=64, out_chans=64,  stride=1, kernel_size=3, act_cls=act_cls)\n",
        "        block2 = sa_m_xResBlock(in_chans=64, out_chans=128, stride=2, kernel_size=3, act_cls=act_cls)\n",
        "        block3 = sa_m_xResBlock(in_chans=128,out_chans=256, stride=2, kernel_size=3, act_cls=act_cls)\n",
        "        pool = nn.Sequential(nn.AdaptiveConcatPool2d(1), nn.Flatten())\n",
        "        fc = FastaiHead(nf=256, num_outputs=num_outputs, act_cls=act_cls)\n",
        "        layers = OrderedDict(stem=conv_stem, block1=block1, block2=block2, block3=block3, pool_flatten=pool, fc=fc)\n",
        "        super(xResModelv3, self).__init__(layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZAOl3dqauLe"
      },
      "source": [
        "exp_name = \"stage-03-xres-mish-sa\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "# instantiate the model\n",
        "model = xResModelv2(num_outputs=len(CLASS_MAP), act_cls=Mish)\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=8, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym_7SKWjauSW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-HVnMWlauY5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Bn92_iWaugA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE9qnitPausK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDs5CVKi19Om"
      },
      "source": [
        "#### Exploring Regularization Techniques - \n",
        "> Reducing the overfitting\n",
        "\n",
        "In this section I will attempt to reduce the overfitting of our model using a common regularizing technique *data augmentation*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD66Vvyw2hiX"
      },
      "source": [
        "What is *data augmentation*? \n",
        "\n",
        "Data augmentation artificially increases the size of the training set by generating many realistic variants of each training instance. Data augmentation is a technique used for introducing variety in training data thereby helping to mitigate overfitting.\n",
        "\n",
        "\n",
        "Let's apply some *data augmentation* data input pipeline. We will only apply data augmentation to the training data. The validation data is not augmented. \n",
        "\n",
        "\n",
        "> Note: For the current task given we have to be very carefull while applying *data augmentation*. Since our Images our images can change depending on the rotation, I won't be applying any `rotation` or `flip` transforms. The only transforms i will be using are intorducing noise and changing the lighting of the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvgnvah64F35"
      },
      "source": [
        "base_tfms = A.Compose([\n",
        "    A.Resize(PRESIZE, PRESIZE, p=1.0),\n",
        "    A.CenterCrop(IMG_SIZE, IMG_SIZE, p=1.0),\n",
        "    A.ToFloat(max_value=255, p=1.0),\n",
        "    ToTensorV2(p=1.0),\n",
        "])\n",
        "\n",
        "aug_tfms = A.Compose([\n",
        "    A.Resize(PRESIZE, PRESIZE, p=1.0),\n",
        "    # data augmentation transforms for the training dataset #\n",
        "    A.Blur(blur_limit=10, always_apply=False, p=0.6),\n",
        "    A.GaussNoise(always_apply=False, p=0.5),\n",
        "    A.JpegCompression(quality_lower=0, quality_upper=1, always_apply=False, p=0.5),\n",
        "    A.ColorJitter(always_apply=False, p=0.5),\n",
        "    A.MultiplicativeNoise(multiplier=[0.5, 1.5], per_channel=True, p=0.5),\n",
        "    ##########################################################\n",
        "    A.CenterCrop(IMG_SIZE, IMG_SIZE, p=1.0),\n",
        "    A.ToFloat(max_value=255, p=1.0, ),\n",
        "    A.Cutout(num_holes=5, always_apply=False, p=0.5),\n",
        "    ToTensorV2(p=1.0),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DMqPaqM5IhU"
      },
      "source": [
        "Let's view the augmentations on the Images -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tprO_F9F475O"
      },
      "source": [
        "train_dl, valid_dl = get_data(transforms=aug_tfms, valid_transforms=base_tfms, bs=64)\n",
        "\n",
        "ims, lbls = next(iter(train_dl))\n",
        "ims, lbls = ims[:8], lbls[:8]\n",
        "\n",
        "grid = make_grid(ims, normalize=True).permute(1, 2, 0)\n",
        "fig = plt.figure(figsize=(13, 13))\n",
        "plt.imshow(grid) \n",
        "plt.title([CLASS_MAP[o] for o in lbls.data.cpu().numpy()])\n",
        "plt.axis(\"off\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqstDxh55MJ0"
      },
      "source": [
        "Another common technique to reduce overfitting is to increase the *weight_decay*, so let's try it also.\n",
        "\n",
        "Let's train on the dataset and explore the performance -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujmZorMj5MOd"
      },
      "source": [
        "exp_name = \"stage-01-xresblock-mish-sa-regularized\"\n",
        "train_dl, valid_dl = get_data(transforms=aug_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "# instantiate the model\n",
        "model = sa_m_xResModel(num_outputs=len(CLASS_MAP))\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03, wd=1e-01)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=20, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH_pziTJcO22"
      },
      "source": [
        "We have incresed the validation_accuracy of the model and have also reduced \n",
        "overfitting compared to the previous experiment. So, let's explore more and see if we can make more improvements. \n",
        "\n",
        "In this next part, instead of using I will be using label smoothing. Label smoothing is known to known to increase the generalization of the models. It reduces the ability of the model of the model to adapt to the training data by adding noise the original labels of the model on the training data.\n",
        "\n",
        "\n",
        "The effectiveness of label smoothing was explored in this [paper](https://arxiv.org/abs/1906.02629#:~:text=Smoothing%20the%20labels%20in%20this,language%20translation%20and%20speech%20recognition.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5d3yMIxdQla"
      },
      "source": [
        "Here's how we can intorduce Label smoothing with CrossEntropy loss in pytorch -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHCIm40TdNJ1"
      },
      "source": [
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"Cross Entropy Loss with Label Smoothing\"\n",
        "    def __init__(self, eps=0.1, reduction: str = \"mean\", weight=None):\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        store_attr(\"eps, reduction, weight\")\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        c = input.size()[1]\n",
        "        log_preds = F.log_softmax(input, dim=1)\n",
        "        if self.reduction == \"sum\":\n",
        "            loss = -log_preds.sum()\n",
        "        else:\n",
        "            loss = -log_preds.sum(dim=1)\n",
        "            if self.reduction == \"mean\":\n",
        "                loss = loss.mean()\n",
        "        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(log_preds, target.long(), weight=self.weight, reduction=self.reduction)\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzeyNzqJnuQ2"
      },
      "source": [
        "I will also add more heavy data augmentation to the Images -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js2ttdxYnx4K"
      },
      "source": [
        "base_tfms = A.Compose([\n",
        "    A.Resize(PRESIZE, PRESIZE, p=1.0),\n",
        "    A.CenterCrop(IMG_SIZE, IMG_SIZE, p=1.0),\n",
        "    A.ToFloat(max_value=255, p=1.0),\n",
        "    ToTensorV2(p=1.0),\n",
        "])\n",
        "\n",
        "aug_tfms = A.Compose([\n",
        "    A.Resize(PRESIZE, PRESIZE, p=1.0),\n",
        "    # data augmentation transforms for the training dataset #\n",
        "    A.Blur(blur_limit=10, always_apply=False, p=0.6),\n",
        "    A.GaussNoise(always_apply=False, p=0.5),\n",
        "    A.JpegCompression(quality_lower=0, quality_upper=1, always_apply=False, p=0.5),\n",
        "    A.ColorJitter(always_apply=False, p=0.5),\n",
        "    A.MultiplicativeNoise(multiplier=[0.5, 1.5], per_channel=True, p=0.5),\n",
        "    A.CLAHE(p=0.5),\n",
        "    ##########################################################\n",
        "    A.RandomResizedCrop(IMG_SIZE, IMG_SIZE, p=1.0),\n",
        "    A.ToFloat(max_value=255, p=1.0, ),\n",
        "    A.Cutout(always_apply=False, p=0.5),\n",
        "    ToTensorV2(p=1.0),\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYbwR9a7iM-Y"
      },
      "source": [
        "*Training the model*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Lo6XVn1KgaNg"
      },
      "source": [
        "exp_name = \"stage-02-xresblock-mish-sa-regularized\"\n",
        "train_dl, valid_dl = get_data(aug_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "# instantiate the model\n",
        "model = sa_m_xResModel(num_outputs=len(CLASS_MAP))\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask_v2(model, lr=8e-03, wd=0.3, eps=0.1, criterion=LabelSmoothingCrossEntropy(eps=0.1))\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=30, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bm0HrZBck5H"
      },
      "source": [
        "try:\n",
        "    del model, trainer, task\n",
        "except: \n",
        "    pass\n",
        "try:\n",
        "    del train_dl, valid_dl\n",
        "except: \n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73zbEmpv5vd6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}