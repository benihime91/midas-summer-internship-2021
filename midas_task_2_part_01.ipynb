{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/benihime91/midas-summer-internship-2021/blob/main/midas_task_2_part_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02qYzvKbV8fx"
   },
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjEbaEVaWQj9"
   },
   "outputs": [],
   "source": [
    "# > Uncomment and run this cell if running on Google Colab\n",
    "# install required dependencies for google colab\n",
    "!git clone https://github.com/benihime91/midas-summer-internship-2021.git\n",
    "!pip install --upgrade -r \"/content/midas-summer-internship-2021/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gxmvU0RruGhb"
   },
   "outputs": [],
   "source": [
    "# > for Goggle Colab\n",
    "import sys\n",
    "sys.path.append(\"midas-summer-internship-2021/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CIttbuJAV3vv"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCkK2vzKWGkW"
   },
   "source": [
    "## Part - 1\n",
    "> Use this dataset (https://www.dropbox.com/s/pan6mutc5xj5kj0/trainPart1.zip) to train a CNN. Use no other data source or pretrained networks, and explain your design choices during preprocessing, model building and training. Also, cite the sources you used to borrow techniques. A test set will be provided later to judge the performance of your classifier. Please save your model checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asp5xGW_WMhz"
   },
   "source": [
    "### Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vtkjtbGpWrQb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Download the 1st Dataset using :\n",
    "!wget -P \"data/\" https://www.dropbox.com/s/pan6mutc5xj5kj0/trainPart1.zip\n",
    "!unzip --qq \"data/trainPart1.zip\" -d \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KSkpCrcLXaRp",
    "outputId": "8ff5e297-0e45-4480-c3a3-79925f107e80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import gc\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from typing import *\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from fastcore.all import *\n",
    "from IPython.display import Markdown, display\n",
    "from PIL import Image\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.metrics import functional as FM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn, optim\n",
    "from torch.nn.utils import spectral_norm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets.folder import IMG_EXTENSIONS\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from progress import NotebookProgressCallback\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMBr1LfVXSEW"
   },
   "source": [
    "### Analyzing the Dataset\n",
    "> In this part we will analyze the get familiar with the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hqblcJPnXd7c",
    "outputId": "15670a57-926b-4113-a83f-5f1a94b9fee5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#62) [Path('data/train/Sample012'),Path('data/train/Sample014'),Path('data/train/Sample053'),Path('data/train/Sample015'),Path('data/train/Sample047'),Path('data/train/Sample017'),Path('data/train/Sample011'),Path('data/train/Sample030'),Path('data/train/Sample018'),Path('data/train/Sample050')...]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_01_PATH = Path(\"data/train/\")\n",
    "DATASET_01_PATH.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7g4-FfRrXf_i"
   },
   "source": [
    "Perform some one-off data manipulations to get all the files and filenames in an easy to use format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xcAjvjblXn9Q"
   },
   "outputs": [],
   "source": [
    "def folder2df(directory: Union[str, Path], extensions: list = IMG_EXTENSIONS,\n",
    "              shuffle: bool = False, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Parses all the Images in `directory` and puts them in a `DataFrame` object.\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    image_list = L()\n",
    "    target_list = L()\n",
    "\n",
    "    if not isinstance(directory, Path):\n",
    "        directory = Path(directory)\n",
    "\n",
    "    for label in directory.ls():\n",
    "        label = Path(label)\n",
    "        if os.path.isdir(label):\n",
    "            for img in label.ls():\n",
    "                if str(img).lower().endswith(extensions):\n",
    "                    image_list.append(img)\n",
    "                    target_list.append(str(label).split(os.path.sep)[-1])\n",
    "\n",
    "    print(f\"Found {len(image_list)} files belonging to {len(set(target_list))} classes.\")\n",
    "\n",
    "    dataframe: pd.DataFrame = pd.DataFrame()\n",
    "    dataframe[\"image_id\"] = image_list.map(str)\n",
    "    dataframe[\"target\"] = target_list\n",
    "    if shuffle:\n",
    "        dataframe = (dataframe.sample(frac=1, random_state=seed)\n",
    "                     .reset_index(inplace=False, drop=True))\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDgGDQ_fivrx"
   },
   "source": [
    "Create the a `pandas Dataframe` which contains all the file info we need to get started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "fI-aXyU-XpbQ",
    "outputId": "5d74a415-e132-4656-a649-be86fb93d34c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2480 files belonging to 62 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/train/Sample005/img005-044.png</td>\n",
       "      <td>Sample005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/train/Sample011/img011-053.png</td>\n",
       "      <td>Sample011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/train/Sample058/img058-008.png</td>\n",
       "      <td>Sample058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/train/Sample027/img027-053.png</td>\n",
       "      <td>Sample027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/train/Sample014/img014-017.png</td>\n",
       "      <td>Sample014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              image_id     target\n",
       "0  data/train/Sample005/img005-044.png  Sample005\n",
       "1  data/train/Sample011/img011-053.png  Sample011\n",
       "2  data/train/Sample058/img058-008.png  Sample058\n",
       "3  data/train/Sample027/img027-053.png  Sample027\n",
       "4  data/train/Sample014/img014-017.png  Sample014"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_01 = folder2df(directory=DATASET_01_PATH, shuffle=True)\n",
    "DATASET_01.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXDt8fOwiyoA"
   },
   "source": [
    "Lets look closer at the data, how many `class_ids` do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rmIk-P2AXrbF",
    "outputId": "06989fb7-7a8d-4320-d7db-e256b6fbc0aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Images in the Dataset:  2480\n",
      "Number of unique classes in the Dataset:  62\n"
     ]
    }
   ],
   "source": [
    "unq_cls = DATASET_01.target.unique()\n",
    "tot_itm = len(DATASET_01)\n",
    "print(\"Total number of Images in the Dataset: \", tot_itm)\n",
    "print(\"Number of unique classes in the Dataset: \",len(unq_cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "kD9BffaOXtT2",
    "outputId": "c7cb9ac4-9ce0-4673-abe4-d6bf5b605d41"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1584x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "_, ax = plt.subplots(1, 1, figsize=(22, 8))\n",
    "sns.countplot(data=DATASET_01, x=\"target\", ax=ax);\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXu2FJYvXwN6"
   },
   "source": [
    "The distibution across all the classes is exactly same. This is a good thing because in this way our model will be less prone to overfitting for a prticular class. We have around 40 Images for each class, which is way less data than I would have preferred. We might overfit the training data due to lack of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6rmhTSKYASE"
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mT-BtUiYCqe"
   },
   "source": [
    "Since, in this task in am going to be using `PyTorch`, before we can directly start training the `CNN network`, we need to get our data into `Dataset` and `DataLoader`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uf5eLkaHYF9w"
   },
   "source": [
    "We also need to encode our string labels into interger labels. For this we will use the `LabelEncoder()` from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "eVIXgw04YJ5x",
    "outputId": "ff5a04c2-b39c-492a-bfef-ff11f3e0db5e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>target</th>\n",
       "      <th>cat_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/train/Sample005/img005-044.png</td>\n",
       "      <td>Sample005</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/train/Sample011/img011-053.png</td>\n",
       "      <td>Sample011</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/train/Sample058/img058-008.png</td>\n",
       "      <td>Sample058</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/train/Sample027/img027-053.png</td>\n",
       "      <td>Sample027</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/train/Sample014/img014-017.png</td>\n",
       "      <td>Sample014</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              image_id     target  cat_label\n",
       "0  data/train/Sample005/img005-044.png  Sample005          4\n",
       "1  data/train/Sample011/img011-053.png  Sample011         10\n",
       "2  data/train/Sample058/img058-008.png  Sample058         57\n",
       "3  data/train/Sample027/img027-053.png  Sample027         26\n",
       "4  data/train/Sample014/img014-017.png  Sample014         13"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(unq_cls)\n",
    "\n",
    "def encode_label(x):\n",
    "    \"Encoder `x`, given x is a scalar value\"\n",
    "    return encoder.transform([x]).item()\n",
    "\n",
    "CLASS_MAP = L(list(encoder.classes_)).map_dict(encode_label)\n",
    "CLASS_MAP = {k:v for v, k in CLASS_MAP.items()}\n",
    "\n",
    "DATASET_01[\"cat_label\"] = DATASET_01[\"target\"].map(encode_label)\n",
    "DATASET_01.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75E8ot1jjNyw"
   },
   "source": [
    "Save a copy of the `CLASS_MAP` which stores the mapping of `classes` and `class ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cyUFNlVxjQ1f"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/class_map_ds01.json\", 'w') as f:\n",
    "    json.dump(CLASS_MAP, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InRu75JbYS6D"
   },
   "source": [
    "Let's also create a Training and Validation split on the data by holding out 20% of the training data for validation.\n",
    "\n",
    "Why we need a different training and validation split for the data ?\n",
    "\n",
    "This is so that we don't inadvertently overfit, train a model to work well only on our training data. For this purpose we will compute metric over on the validation data. During the training phase the model will see only the training data. The Validation data will remain separate and will only be used for calculation of the *metric*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9YRWmMAYXPu",
    "outputId": "43019cfa-afa2-41a8-d2b3-0a1b86dd0794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Training Examples:  1984\n",
      "Num Validation Examples:  496\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATASET_01, VALID_DATASET_01 = train_test_split(DATASET_01, test_size=0.2, random_state=42, \n",
    "                                                      shuffle=True, stratify=DATASET_01['cat_label'])\n",
    "\n",
    "TRAIN_DATASET_01 = TRAIN_DATASET_01.reset_index(drop=True, inplace=False)\n",
    "VALID_DATASET_01 = VALID_DATASET_01.reset_index(drop=True, inplace=False) \n",
    "\n",
    "print(\"Num Training Examples: \", len(TRAIN_DATASET_01))\n",
    "print(\"Num Validation Examples: \", len(VALID_DATASET_01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-m0bwAFHYbKL"
   },
   "source": [
    "Now we will create a custom `Dataset` obj that can parse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4tCeY67JYilJ"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, df:pd.DataFrame, transforms):\n",
    "        self._dataframe  = df\n",
    "        self._transforms = transforms\n",
    "        self._gray_scale = T.Grayscale(num_output_channels=3)\n",
    "    \n",
    "    @property\n",
    "    def transforms(self):\n",
    "        return self._transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        im = self._dataframe['image_id'][idx]\n",
    "        \n",
    "        # Load and apply transformations to the Image\n",
    "        im = cv2.imread(im)\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "        im = self._transforms(image=im)[\"image\"]\n",
    "\n",
    "        # convert the Images to grayscale Images with 3 dimensions\n",
    "        im = self._gray_scale(im)\n",
    "        \n",
    "        lbl = self._dataframe[\"cat_label\"][idx]   \n",
    "        return im, lbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NHWPxawYj4I"
   },
   "source": [
    "Lets define what we want our transforms to be -\n",
    "\n",
    "We'll use the presize the images 160 px make things faster still, and will center crop to 128 px.\n",
    "Initially I will not be using any other fancy data augmentations. After the resizing I will normalize the Images by dividing by 255.0 and also converting them to pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BsS6h-b9Ymb6"
   },
   "outputs": [],
   "source": [
    "PRESIZE  = 80\n",
    "IMG_SIZE = 64\n",
    "\n",
    "base_tfms = A.Compose([\n",
    "    A.Resize(PRESIZE, PRESIZE, p=1.0),\n",
    "    A.CenterCrop(IMG_SIZE, IMG_SIZE, p=1.0),\n",
    "    A.ToFloat(max_value=255, p=1.0),\n",
    "    ToTensorV2(p=1.0),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZCN6NMcvwhI"
   },
   "source": [
    "Function to get our data -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KqV_2ZmOvvJB"
   },
   "outputs": [],
   "source": [
    "def get_data(transforms = base_tfms, valid_transforms=base_tfms, bs: int = 64):\n",
    "    # instantiate the dataset set obj\n",
    "    train_ds = MyDataset(df=TRAIN_DATASET_01, transforms=transforms)\n",
    "    valid_ds = MyDataset(df=VALID_DATASET_01, transforms=valid_transforms)\n",
    "\n",
    "    # create the dataloaders obj\n",
    "    train_dl = DataLoader(train_ds, batch_size=bs, num_workers=num_cpus(), shuffle=True) \n",
    "    valid_dl = DataLoader(valid_ds, batch_size=bs, num_workers=num_cpus(), shuffle=False)\n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8gP2ClSYq2J"
   },
   "source": [
    "Having a look at the images in our `DataLoader` you can see result of the image transforms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "7ceVZHQ6YtWi",
    "outputId": "d89e77da-6e6c-49a9-ad89-29cea0be9a9e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x936 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(transforms=base_tfms, valid_transforms=base_tfms)\n",
    "\n",
    "ims, lbls = next(iter(train_dl))\n",
    "ims, lbls = ims[:8], lbls[:8]\n",
    "\n",
    "grid = make_grid(ims, normalize=True).permute(1, 2, 0)\n",
    "fig = plt.figure(figsize=(13, 13))\n",
    "plt.imshow(grid) \n",
    "plt.title([CLASS_MAP[o] for o in lbls.data.cpu().numpy()])\n",
    "plt.axis(\"off\");\n",
    "\n",
    "del train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KD2fNoNqbWv1"
   },
   "source": [
    "### PyTorch Lightning Task -\n",
    "For training I will be using PyTorch Lightning ⚡️. PyTorch Lightning expects the whole training pipeline to be defined under a `LightningModule`. Let's convert our model in a `LightningModule`.\n",
    "\n",
    "For evaluating the model initially I will use the `Accuracy` metric and for loss function i will use the standart `nn.CrossEntropyLoss()` from PyTorch.\n",
    "\n",
    "\n",
    "The code block below create a general PyTorch Lightning Classification Task which helps us iterate different models inside a `LightningModule.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "veOLW0L2btN8"
   },
   "outputs": [],
   "source": [
    "class ClassificationTask(pl.LightningModule):\n",
    "    def __init__(self, model: nn.Module, lr: float, wd: float = 0, criterion: nn.Module = nn.CrossEntropyLoss()):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(\"lr\", \"wd\")\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.apply_init(self)\n",
    "\n",
    "    def apply_init(self, m):\n",
    "        \"apply kaiming_normal initialization to the weights of a cnn\"\n",
    "        if getattr(m, 'bias', None) is not None: \n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d,nn.Linear)): \n",
    "            nn.init.kaiming_normal_(m.weight)\n",
    "        for l in m.children(): \n",
    "            self.apply_init(l)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        \"Same as nn.Module forward\"\n",
    "        return self.model(xb)\n",
    "\n",
    "    def shared_step(self, batch, batch_idx, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        The common step shared between the training, validation & test steps\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc  = FM.accuracy(F.softmax(y_hat), y)\n",
    "        metrics = {'accuracy': acc, 'loss': loss}\n",
    "        return metrics\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, *args, **kwargs):\n",
    "        metrics = self.shared_step(batch, batch_idx, *args, **kwargs)\n",
    "        metrics = {'train_acc': metrics['accuracy'], 'train_loss': metrics['loss']}\n",
    "        self.log_dict(metrics)\n",
    "        return metrics[\"train_loss\"]\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, *args, **kwargs):\n",
    "        metrics = self.shared_step(batch, batch_idx, *args, **kwargs)\n",
    "        metrics = {'val_acc': metrics['accuracy'], 'val_loss': metrics['loss']}\n",
    "        self.log_dict(metrics)\n",
    "\n",
    "    def test_step(self, batch, batch_idx, *args, **kwargs):\n",
    "        metrics = self.shared_step(batch, batch_idx, *args, **kwargs)\n",
    "        metrics = {'test_acc': metrics['accuracy'], 'test_loss': metrics['loss']}\n",
    "        self.log_dict(metrics)\n",
    "           \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        define optimizers and LR schedulers for use in training.\n",
    "        \"\"\"\n",
    "        # default Adam parameters from fast.ai\n",
    "        opt   = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.wd, \n",
    "                            betas=(0.9, 0.99), eps=1e-05)\n",
    "        steps = len(self.train_dataloader())\n",
    "        epochs= self.trainer.max_epochs\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(opt, max_lr=self.hparams.lr, epochs=epochs, steps_per_epoch=steps)\n",
    "        return [opt], [dict(scheduler=scheduler, interval='step')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oG688lVucgbE"
   },
   "source": [
    "For training I will be using the popular `AdamW + 1cycle` training approach. Here we use the `AdamW` optimizer and `OneCycleLR` scheduler from PyTorch.\n",
    "\n",
    "`AdamW` was inroduced in this [paper](https://arxiv.org/abs/1711.05101). Experiments by the fast.ai group has shown that `AdamW` along with the `1cycle policy` of learning rate scheduling gives very good results. `AdamW` differs from `Adam` is the sense that AdamW uses true weight decay (decay the weights directly) while Adam uses L2 regularization (add the decay to the gradients).\n",
    "\n",
    "The `1cycle policy` was introduced by Leslie N. Smith et al. in [Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120).\n",
    "\n",
    "\n",
    "`AdamW + 1cycle` was incremental in fast.ai winning the `DAWNBench` competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka0ItH0LYuu1"
   },
   "source": [
    "### Benchmark Model\n",
    "First I will let's start by making a good baseline Model and subsequently we will experiment by improving upon this model.\n",
    "\n",
    "Let's first create the building blocks that we will use building our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9pQi76LYY2jp"
   },
   "outputs": [],
   "source": [
    "class ConvBnDropBlock(nn.Sequential):\n",
    "    \"Create sequence of convolutional, Activation, `BatchNorm` & Drouput layers.\"\n",
    "    def __init__(self, in_chans, out_chans, kernel_size, stride=1, dilation=1, \n",
    "                 padding = None, bias=True, act_cls: Callable = nn.ReLU, \n",
    "                 p_drop=0.0, use_bn=True,):\n",
    "\n",
    "        if padding is None: \n",
    "            # for same padding\n",
    "            padding = (kernel_size-1)//2\n",
    "        \n",
    "        layers = []\n",
    "        # Initialize the convolutional layer\n",
    "        conv_layer = nn.Conv2d(in_chans, out_chans, kernel_size, stride, \n",
    "                               dilation=dilation, padding=padding, bias=bias)\n",
    "        \n",
    "        if act_cls is not None:\n",
    "            act_layer = act_cls(inplace=True)\n",
    "        else:\n",
    "            act_layer = nn.Identity()\n",
    "        if use_bn:\n",
    "            norm_layer = nn.BatchNorm2d(out_chans)\n",
    "        else:\n",
    "            norm_layer = nn.Identity()\n",
    "           \n",
    "        layers += [conv_layer, act_layer, norm_layer]\n",
    "        \n",
    "        if p_drop > 0.0:\n",
    "            layers.append(nn.Dropout2d(p=p_drop))\n",
    "            \n",
    "        super(ConvBnDropBlock, self).__init__(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0wlcZ57kLIX"
   },
   "source": [
    "For this whole experiment my models will be divided into 2 parts a `feature_extractor` and `classifier`. The feature extractor is responsible for generating the feature maps from the Images, while the `classifier` is responsible for final classification.\n",
    "\n",
    "A baseline model will establish a minimum model performance to which all of our other models can be compared, as well as a model architecture that we can use as the basis of study and improvement.\n",
    "\n",
    "A feature extractor of baseline model architecture has been partly inspired from the general architectural principles of the `VGG models` & from this [blog post](https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/).\n",
    "\n",
    "The architecture involves stacking convolutional layers with small `3×3` filters followed by a activation, batchnorm, dropout.Together, these layers form a block, and these blocks can be repeated where the number of filters in each block is increased with the depth of the network such as 32, 64, 128. The first 2 blocks are followed by a max_pool layer while the 3rd layer is follwed by a `pool_flatten` layer i.e., a combination of  AdaptiveAvgPool2d and flatten layer from pytorch.\n",
    "\n",
    "We use a AdaptiveAvgPool layer so that our network is *fully convolutional* and so can work for any image size that is to say that a model trained on 64x64 images will also work for 120x120 images.\n",
    "\n",
    "This completes the feature extractor part of the model. For the classifier of the model: first, the feature maps output from the feature extraction part of the model must be flattened.\n",
    "We than use a Dropuout layer followed by a Linear layer to generate the predictions.\n",
    "\n",
    "Let's create the network architecture -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "cwB7Gpy5ZAeX"
   },
   "outputs": [],
   "source": [
    "class BenchmarkModel(nn.Sequential):\n",
    "    def __init__(self, num_outputs: int):\n",
    "        conv_block_01 = ConvBnDropBlock(in_chans=3, out_chans=32, bias=False, \n",
    "                                        use_bn=True, act_cls=nn.ReLU, p_drop=0.25, \n",
    "                                        kernel_size=3)\n",
    "        conv_block_02 = ConvBnDropBlock(in_chans=32, out_chans=32, bias=False, \n",
    "                                        use_bn=True, act_cls=None, p_drop=0.25, \n",
    "                                        kernel_size=3)\n",
    "        pool_block_0  = nn.Sequential(nn.MaxPool2d(2), nn.ReLU())\n",
    "\n",
    "        conv_block_11 = ConvBnDropBlock(in_chans=32, out_chans=64, bias=False, \n",
    "                                        use_bn=True, act_cls=nn.ReLU, p_drop=0.25, \n",
    "                                        kernel_size=3)\n",
    "        conv_block_12 = ConvBnDropBlock(in_chans=64, out_chans=64, bias=False, \n",
    "                                        use_bn=True, act_cls=None, p_drop=0.25, \n",
    "                                        kernel_size=3)\n",
    "        pool_block_1  = nn.Sequential(nn.MaxPool2d(2), nn.ReLU())\n",
    "\n",
    "        conv_block_21 = ConvBnDropBlock(in_chans=64, out_chans=128, bias=False, \n",
    "                                        use_bn=True, act_cls=nn.ReLU, p_drop=0.25, \n",
    "                                        kernel_size=3)\n",
    "        conv_block_22 = ConvBnDropBlock(in_chans=128, out_chans=128, bias=False, stride=2,\n",
    "                                        use_bn=True, act_cls=nn.ReLU, p_drop=0.25, \n",
    "                                        kernel_size=3, padding=0)\n",
    "        \n",
    "        # ensemble the model building blocks\n",
    "        block1 = nn.Sequential(conv_block_01, conv_block_02, pool_block_0)\n",
    "        block2 = nn.Sequential(conv_block_11, conv_block_12, pool_block_1)\n",
    "        block3 = nn.Sequential(conv_block_21, conv_block_22)\n",
    "        pool_flatten = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
    "        fc = nn.Sequential(nn.Dropout(0.3), nn.Linear(128, num_outputs, bias=False))\n",
    "\n",
    "        layers = [block1, block2, block3, pool_flatten, fc]\n",
    "        super(BenchmarkModel, self).__init__(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdNxn-amZEhK"
   },
   "source": [
    "*Training*\n",
    "\n",
    "Train the model initially for 20 epochs and view preformance. I found that 15 epochs was a good place to stop as the model started to overfit while the accuracy didn't improve further and any lower than 20 epochs the model doesn't convergerce.\n",
    "\n",
    "\n",
    "I will also be using `ModelCheckpoint` callback from pytorch lightning in all of my experiments to that i will be easily able to load my best checkpoints and compare my performaces.\n",
    "\n",
    "\n",
    "`3e-3` is often a good learning rate for CNNs, so let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 995
    },
    "id": "_2OfLQXxZHcQ",
    "outputId": "3f450e54-42ac-43fd-bc39-aabcb2f33e5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | BenchmarkModel   | 295 K \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "295 K     Trainable params\n",
      "0         Non-trainable params\n",
      "295 K     Total params\n",
      "1.182     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            progress {\n",
       "                border: none;\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      Training\n",
       "      <progress value='1240' max='1240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1240/1240 06:44, Epoch 19 {'loss': '1.2', 'v_num': 0}]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>time</th>\n",
       "      <th>samples/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>4.232053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.189208</td>\n",
       "      <td>20.339800</td>\n",
       "      <td>3.834800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>4.159909</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>4.138566</td>\n",
       "      <td>20.473500</td>\n",
       "      <td>3.809800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.030242</td>\n",
       "      <td>4.074357</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.959195</td>\n",
       "      <td>20.277200</td>\n",
       "      <td>3.846700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.050403</td>\n",
       "      <td>3.833298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.851967</td>\n",
       "      <td>20.138400</td>\n",
       "      <td>3.873200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.106855</td>\n",
       "      <td>3.563886</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>3.728870</td>\n",
       "      <td>20.253800</td>\n",
       "      <td>3.851100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.205645</td>\n",
       "      <td>3.189341</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>3.404187</td>\n",
       "      <td>20.138800</td>\n",
       "      <td>3.873100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.300403</td>\n",
       "      <td>2.847658</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>2.970315</td>\n",
       "      <td>20.463200</td>\n",
       "      <td>3.811700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.475806</td>\n",
       "      <td>2.425936</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>2.655712</td>\n",
       "      <td>20.403500</td>\n",
       "      <td>3.822900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.556452</td>\n",
       "      <td>2.026138</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>2.592381</td>\n",
       "      <td>20.007500</td>\n",
       "      <td>3.898500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.635081</td>\n",
       "      <td>1.691863</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.133938</td>\n",
       "      <td>20.118500</td>\n",
       "      <td>3.877000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.711694</td>\n",
       "      <td>1.461400</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>2.257470</td>\n",
       "      <td>20.312200</td>\n",
       "      <td>3.840100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.723790</td>\n",
       "      <td>1.394088</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>1.677782</td>\n",
       "      <td>20.128400</td>\n",
       "      <td>3.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.743952</td>\n",
       "      <td>1.212499</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>1.411806</td>\n",
       "      <td>20.368400</td>\n",
       "      <td>3.829500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.756048</td>\n",
       "      <td>1.101572</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>1.749272</td>\n",
       "      <td>20.211800</td>\n",
       "      <td>3.859100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.764113</td>\n",
       "      <td>1.070746</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.272563</td>\n",
       "      <td>20.348000</td>\n",
       "      <td>3.833300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.780242</td>\n",
       "      <td>1.010018</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.367799</td>\n",
       "      <td>20.164100</td>\n",
       "      <td>3.868300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.788306</td>\n",
       "      <td>0.950518</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.299212</td>\n",
       "      <td>20.241900</td>\n",
       "      <td>3.853400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.788306</td>\n",
       "      <td>0.954568</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.329283</td>\n",
       "      <td>20.051400</td>\n",
       "      <td>3.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.796371</td>\n",
       "      <td>0.928489</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1.385210</td>\n",
       "      <td>20.350800</td>\n",
       "      <td>3.832800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.786290</td>\n",
       "      <td>0.925548</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.956120</td>\n",
       "      <td>20.281000</td>\n",
       "      <td>3.846000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 🎯 Final Training Accuracy: 0.836, Final Validation Accuracy: 0.796"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = \"benchmark-cnn\"\n",
    "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms, bs=32)\n",
    "\n",
    "# instantiate the model\n",
    "model = BenchmarkModel(num_outputs=len(CLASS_MAP))\n",
    "\n",
    "# Put the model into Lightning-Task\n",
    "task = ClassificationTask(model, lr=3e-03)\n",
    "\n",
    "cbs = [ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
    "       NotebookProgressCallback(), \n",
    "       pl.callbacks.LearningRateMonitor(logging_interval=\"step\", log_momentum=True)]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=20, callbacks=cbs, gpus=1, precision=16, log_every_n_steps=1)\n",
    "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
    "\n",
    "# Evalute the final performance of the Model\n",
    "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
    "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
    "\n",
    "display(Markdown(f\"> 🎯 Final Training Accuracy: {round(trn_acc, 3)}, Final Validation Accuracy: {round(val_acc, 3)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRAlFbD1ZeYh"
   },
   "source": [
    "That's a pretty good start, considering we have to pick the correct one of 62 categories, and we're training from scratch. So in the subsequent sections we will explore tricks that can probably help us to improve our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wqlNrojraX5-"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del model, trainer, task\n",
    "except: \n",
    "    pass\n",
    "try:\n",
    "    del train_dl, valid_dl\n",
    "except: \n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bms8tB0La3Z4"
   },
   "source": [
    "## Improving the Benchmark \n",
    "> This section will explore various techniques which will hopefully help us in improving our baseline model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vH0VC7qNbD1S"
   },
   "source": [
    "### Residual Blocks -\n",
    "\n",
    "In our above trained `CNN` model We can do way better than the current results using a deeper mode, but just stacking new layers won't really improve our results.\n",
    "In this experiment we will explore the preformance of our model by introducing `residual connection`s. It was introduced in 2015 by Kaiming He et al. in the article \"[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\". Residual Connections are the main building blocks of the ResNet family of networks, which are one of the most popular CNN Architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq6vibo_vvi1"
   },
   "source": [
    "Here's the definition of a simple ResNet block -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "IGhgvOpXdjjG"
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    \"Creates a simple Residual Block\"\n",
    "    def __init__(self, in_chans: int, out_chans: int, \n",
    "                 kernel_size: int, stride: int = 1, act_cls: Callable = nn.ReLU):\n",
    "        \n",
    "        super(ResBlock, self).__init__()\n",
    "        self.idconv = None\n",
    "        \n",
    "        self.block1 = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=kernel_size, \n",
    "                                      stride=stride, padding=None, use_bn=True, act_cls=act_cls, \n",
    "                                      p_drop=0.0, bias=False)\n",
    "        \n",
    "        self.block2 = ConvBnDropBlock(in_chans=out_chans, out_chans=out_chans, kernel_size=kernel_size, \n",
    "                                      stride=1, padding=None, use_bn=True, act_cls=None, p_drop=0.0, bias=False)\n",
    "        \n",
    "        self.act_cls = act_cls(inplace=True)\n",
    "        \n",
    "        if in_chans != out_chans or stride != 1:\n",
    "            self.idconv = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=1, \n",
    "                                          stride=stride, padding=0, use_bn=True, act_cls=None, \n",
    "                                          p_drop=0.0, bias=False)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "         \n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        for m in self.block2.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 0)\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        \n",
    "        if self.idconv is not None:\n",
    "            identity = self.idconv(x)\n",
    "        \n",
    "        out += identity\n",
    "        return self.act_cls(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YmH2dP_jcg9"
   },
   "source": [
    "Why use `idconv` block ?\n",
    ">The issue is that with a stride of, say, 2 on one of the convolutions, the grid size of the output activations will be half the size on each axis of the input. So then we can't add that back to x in forward because x and the output activations have different dimensions. The same basic issue occurs if `in_chans`!=`out_chans`: the shapes of the input and output connections won't allow us to add them together. So `idconv` works as a identity map that matches the dimenstions of x to the ouput of `self.block2(self.block1(x))`. `idconv` increases the `channels` out the x and downsamples it with stride if required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6V7-ww3wA8b"
   },
   "source": [
    "We can now proceed towards modifying our BenchMark model. Let's add some skip connections into this network. We remove modify the 1st conv block of the model to resemble the layer in a typical `ResNet` Model, i.e, a conv layer (output_channels=64, kernel_size=7, stride=2) followed by a batch_normalization, activation and a MaxPooling layer. We then add the residual blocks of the model. The classifier of the model remains same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-N0geEarjcry"
   },
   "outputs": [],
   "source": [
    "class ResModel(nn.Sequential):\n",
    "    def __init__(self, num_outputs: int):\n",
    "        conv = ConvBnDropBlock(in_chans=3, out_chans=64, kernel_size=7, p_drop=0, stride=2, \n",
    "                               act_cls=nn.ReLU, bias=False, use_bn=True)\n",
    "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        conv_stem  = nn.Sequential(conv, pool)\n",
    "        block1 = ResBlock(in_chans=64, out_chans=64,  stride=1, kernel_size=3, act_cls=nn.ReLU)\n",
    "        block2 = ResBlock(in_chans=64, out_chans=128, stride=2, kernel_size=3, act_cls=nn.ReLU)\n",
    "        block3 = ResBlock(in_chans=128,out_chans=256, stride=2, kernel_size=3, act_cls=nn.ReLU)\n",
    "        pool_flatten = nn.Sequential(OrderedDict(pool_layer=nn.AdaptiveAvgPool2d(1), flatten=nn.Flatten()))\n",
    "        fc = nn.Sequential(nn.Dropout(0.25), nn.Linear(256, num_outputs))\n",
    "        layers = OrderedDict(stem=conv_stem, block1=block1, block2=block2, block3=block3, pool_flatten=pool_flatten, fc=fc)\n",
    "        super(ResModel, self).__init__(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9O0g6ihyEpp"
   },
   "source": [
    "Since this is a larger model than the previous one I will reduce the number of training epochs so that our model does not overfit the training data and quick experimentation purposes.\n",
    "\n",
    "Let's train it for a little bit and see how it fares compared to the previous model - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "id": "LuvF6ax3jc1h",
    "outputId": "a5dfd739-222c-4ed8-e1da-cbde52ad176e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | ResModel         | 1.2 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.995     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            progress {\n",
       "                border: none;\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      Training\n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 03:26, Epoch 9 {'loss': '0.101', 'v_num': 1}]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>time</th>\n",
       "      <th>samples/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.026210</td>\n",
       "      <td>4.093469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.164094</td>\n",
       "      <td>20.203500</td>\n",
       "      <td>3.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>3.830323</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>3.776309</td>\n",
       "      <td>20.636400</td>\n",
       "      <td>3.779700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.256048</td>\n",
       "      <td>2.840598</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>2.786022</td>\n",
       "      <td>20.785600</td>\n",
       "      <td>3.752600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.475806</td>\n",
       "      <td>1.949436</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>1.587280</td>\n",
       "      <td>20.723900</td>\n",
       "      <td>3.763800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.604839</td>\n",
       "      <td>1.386291</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.997597</td>\n",
       "      <td>20.751600</td>\n",
       "      <td>3.758700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.445565</td>\n",
       "      <td>2.046606</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.391236</td>\n",
       "      <td>20.820300</td>\n",
       "      <td>3.746300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.788306</td>\n",
       "      <td>0.765603</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.277591</td>\n",
       "      <td>20.771000</td>\n",
       "      <td>3.755200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.802419</td>\n",
       "      <td>0.736617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.151766</td>\n",
       "      <td>20.638400</td>\n",
       "      <td>3.779400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.697750</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.158890</td>\n",
       "      <td>20.880100</td>\n",
       "      <td>3.735600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.816532</td>\n",
       "      <td>0.681674</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068983</td>\n",
       "      <td>20.779200</td>\n",
       "      <td>3.753800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 🎯 Final Training Accuracy: 0.998, Final Validation Accuracy: 0.817"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = \"stage-01-resblock\"\n",
    "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms, bs=32)\n",
    "\n",
    "# instantiate the model\n",
    "model = ResModel(num_outputs=len(CLASS_MAP))\n",
    "\n",
    "# Put the model into Lightning-Task\n",
    "task = ClassificationTask(model, lr=3e-03)\n",
    "\n",
    "cbs = [ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
    "       NotebookProgressCallback(), \n",
    "       pl.callbacks.LearningRateMonitor(logging_interval=\"step\", log_momentum=True)]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, callbacks=cbs, gpus=1, precision=16, log_every_n_steps=1)\n",
    "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
    "\n",
    "# Evalute the final performance of the Model\n",
    "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
    "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
    "\n",
    "display(Markdown(f\"> 🎯 Final Training Accuracy: {round(trn_acc, 3)}, Final Validation Accuracy: {round(val_acc, 3)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l43AbZbd0tIu"
   },
   "source": [
    "The accuracy of our model has increased by adding the skip connections. We were able to improve our accuracy in a fewer epochs compared to the previous model.\n",
    "\n",
    "If we had trained more I am not sure sure if the model performance would have increased on the validation data. This is because the model is overfitting on the training data and we do not want that. What we want is, that the model performs well in unseen data (or the validation dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQBcyfSoMNpR"
   },
   "source": [
    "> Note: At this point it is crearly evident that currently our model is overfitting on the training data. But we will work on improving overfitting. First I want to try different model architectures and find the most optimal after which I will use *regularization* techniques to further improve the model and make it more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Dcw2YCZullQu"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del model, trainer, task\n",
    "except: \n",
    "    pass\n",
    "try:\n",
    "    del train_dl, valid_dl\n",
    "except: \n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFNrOLQ2nRgF"
   },
   "source": [
    "### A State-of-the-Art ResNets -\n",
    "> In this part I will mainly try to explore different variant of `ResNet`s called `ResNet-D` proposed by Tong He in 2014 in the article \"[Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/abs/1812.01187)\n",
    "\". By using a tweaked ResNet-50 architecture and Mixup they achieved 94.6% top-5 accuracy on ImageNet, in comparison to 92.2% with a regular ResNet-50 without Mixup. This result is better than that achieved by regular ResNet models that are twice as deep (and twice as slow, and much more likely to overfit). These tweaked `Resnet` variants was ultimately popularized Jeremy Howard of Fast.ai and are called as `xResNet`'s.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BPqJNZRnT9K"
   },
   "source": [
    "This experiment will mainly explore the model preformace we replace the Residual blocks with xResidual Blocks.\n",
    "\n",
    "First, let us explore the architecture of `ResNet-D` proposed in the above named article. To obtain the `xResNet` architecture we have to apply three different tweaks in the `ResNet` architecture namely `ResNet-B`, `ResNet-C` and `ResNet-D`. \n",
    "\n",
    "The notable changes in the model architecture in our case would be :\n",
    "\n",
    "* In `ResidualCnn`, `resblock2` we are downsampling the input by applying a convulation `stride=2` in the 1st layer of the residual block. `ResNet-B` simply moves the stride `2` to the second convolution and keeps a stride of `1` for the first layer .\n",
    "* The `ResNet-C`, proposed in Inception-v2, removes the `7x7` convolution in the input stem of the network and replaces it with three consecutive 3x3 convolutions. \n",
    "* In `ResNet-D`, the authors replaced the convolution in the downsampling block with a `2x2` average-pooling layer of stride `2` followed by a `1x1` convolution layer. In our case we would have to replace the `idconv` present in `ResidualBlock` module.\n",
    "\n",
    "Let's apply the above changes to our model test the preformance -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "qjX2mDHonZwy"
   },
   "outputs": [],
   "source": [
    "class xResBlock(nn.Module):\n",
    "    \"Creates a simple Residual Block for xResNet architecture\"\n",
    "    def __init__(self, in_chans, out_chans, kernel_size, stride=1, act_cls=nn.ReLU):\n",
    "        super(xResBlock, self).__init__()\n",
    "        self.idconv = None\n",
    "        \n",
    "        self.block1 = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=kernel_size,\n",
    "                                      stride=1, use_bn=True, act_cls=act_cls, bias=False)\n",
    "        \n",
    "        # we apply the 1st change here, \n",
    "        # moving the stride 2 to the second convolution and keeps a stride of 1 for the first layer . \n",
    "        self.block2 = ConvBnDropBlock(in_chans=out_chans, out_chans=out_chans, kernel_size=kernel_size, \n",
    "                                      stride=stride, use_bn=True, act_cls=None, bias=False)\n",
    "        \n",
    "        self.act_cls = act_cls(inplace=True)\n",
    "        \n",
    "        if in_chans != out_chans or stride != 1:\n",
    "            # the 3rd change proped above is applied here,\n",
    "            # we replace with a 2x2 average-pooling layer of stride 2 followed by a 1x1 convolution layer\n",
    "            pool_layer = nn.AvgPool2d(stride, ceil_mode=True)\n",
    "            conv_layer = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=1, padding=0, \n",
    "                                         use_bn=True, act_cls=None, p_drop=0.0, bias=False, stride=1)\n",
    "            \n",
    "            self.idconv = nn.Sequential(pool_layer, conv_layer)\n",
    "            \n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "         \n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        for m in self.block2.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 0)\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        \n",
    "        if self.idconv is not None:\n",
    "            identity = self.idconv(x)\n",
    "        \n",
    "        out += identity\n",
    "        return self.act_cls(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xahTgOcNne14"
   },
   "source": [
    "The code is, for the most part, taken & modified from the [fast.ai course](https://www.fast.ai/), [fast.ai]() library and this [blog post](https://towardsdatascience.com/xresnet-from-scratch-in-pytorch-e64e309af722). \n",
    "\n",
    "Let's move incorporate this into the model that we have been working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "zQVrmb4Dnj4v"
   },
   "outputs": [],
   "source": [
    "class xResModel(nn.Sequential):\n",
    "    def __init__(self, num_outputs: int, act_cls=nn.ReLU):\n",
    "        conv = nn.Sequential(\n",
    "            ConvBnDropBlock(in_chans=3,  out_chans=32, kernel_size=3, stride=2, act_cls=act_cls, \n",
    "                            bias=False, use_bn=False),\n",
    "            ConvBnDropBlock(in_chans=32, out_chans=32, kernel_size=3, stride=1, act_cls=act_cls, \n",
    "                            bias=False, use_bn=False),\n",
    "            ConvBnDropBlock(in_chans=32, out_chans=64, kernel_size=3, stride=1, act_cls=act_cls,\n",
    "                            bias=False, use_bn=False),\n",
    "        )\n",
    "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        conv_stem  = nn.Sequential(conv, pool)\n",
    "        block1 = xResBlock(in_chans=64, out_chans=64,  stride=1, kernel_size=3, act_cls=act_cls)\n",
    "        block2 = xResBlock(in_chans=64, out_chans=128, stride=2, kernel_size=3, act_cls=act_cls)\n",
    "        block3 = xResBlock(in_chans=128,out_chans=256, stride=2, kernel_size=3, act_cls=act_cls)\n",
    "        pool_flatten = nn.Sequential(OrderedDict(pool=nn.AdaptiveAvgPool2d(1), flatten=nn.Flatten()))\n",
    "        fc = nn.Sequential(nn.Dropout(0.25), nn.Linear(256,num_outputs))\n",
    "        layers = OrderedDict(stem=conv_stem, block1=block1, block2=block2, block3=block3, pool_flatten=pool_flatten, fc=fc)\n",
    "        super(xResModel, self).__init__(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7t99NKFfsL6D"
   },
   "source": [
    "Let's train it and see how it fares compared to the previous model -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "id": "JJYIBu7Bo95m",
    "outputId": "06b12439-7a82-4eb2-d500-bf504de7f850"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | xResModel        | 1.3 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.070     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            progress {\n",
       "                border: none;\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      Training\n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 03:27, Epoch 9 {'loss': '0.145', 'v_num': 2}]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>time</th>\n",
       "      <th>samples/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.036290</td>\n",
       "      <td>4.075097</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>4.055149</td>\n",
       "      <td>20.408300</td>\n",
       "      <td>3.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.072581</td>\n",
       "      <td>3.936617</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>3.410817</td>\n",
       "      <td>20.820400</td>\n",
       "      <td>3.746300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.282258</td>\n",
       "      <td>2.692599</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>2.334053</td>\n",
       "      <td>21.078000</td>\n",
       "      <td>3.700500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.203629</td>\n",
       "      <td>3.004666</td>\n",
       "      <td>0.593750</td>\n",
       "      <td>1.412039</td>\n",
       "      <td>20.762800</td>\n",
       "      <td>3.756700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.151210</td>\n",
       "      <td>4.252478</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>20.367100</td>\n",
       "      <td>3.829700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.651210</td>\n",
       "      <td>1.169238</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.353632</td>\n",
       "      <td>20.867600</td>\n",
       "      <td>3.737800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.695565</td>\n",
       "      <td>1.088703</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.719027</td>\n",
       "      <td>20.961200</td>\n",
       "      <td>3.721200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.840726</td>\n",
       "      <td>0.528903</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.212674</td>\n",
       "      <td>20.960300</td>\n",
       "      <td>3.721300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.864919</td>\n",
       "      <td>0.476138</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.164252</td>\n",
       "      <td>20.920500</td>\n",
       "      <td>3.728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.862903</td>\n",
       "      <td>0.472210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.094409</td>\n",
       "      <td>20.859400</td>\n",
       "      <td>3.739300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 🎯 Final Training Accuracy: 0.987, Final Validation Accuracy: 0.865"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = \"stage-01-xresblock\"\n",
    "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms, bs=32)\n",
    "\n",
    "# instantiate the model\n",
    "model = xResModel(num_outputs=len(CLASS_MAP))\n",
    "\n",
    "# Put the model into Lightning-Task\n",
    "task = ClassificationTask(model, lr=3e-03)\n",
    "\n",
    "cbs = [ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
    "       NotebookProgressCallback(), \n",
    "       pl.callbacks.LearningRateMonitor(logging_interval=\"step\", log_momentum=True)]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, callbacks=cbs, gpus=1, precision=16, log_every_n_steps=1)\n",
    "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
    "\n",
    "# Evalute the final performance of the Model\n",
    "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
    "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
    "\n",
    "display(Markdown(f\"> 🎯 Final Training Accuracy: {round(trn_acc, 3)}, Final Validation Accuracy: {round(val_acc, 3)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFfabjLSUrQy"
   },
   "source": [
    "Validation accuracy has increased a lot but we are still overfitting on the training data , let's explore more ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "XAOfe5FRtVtf"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del model, trainer, task\n",
    "except: \n",
    "    pass\n",
    "try:\n",
    "    del train_dl, valid_dl\n",
    "except: \n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bosj5r96zCeG"
   },
   "source": [
    "### Mish and SelfAttention - \n",
    "> In this experiment I plan to use some of [State of the Art Training techniques](https://forums.fast.ai/t/how-we-beat-the-5-epoch-imagewoof-leaderboard-score-some-new-techniques-to-consider/53453) that were used to top the leaderboard score for 5 epoch Imagewoof`.\n",
    "\n",
    "\n",
    "\n",
    "The techniques I will be employing are as follows - \n",
    "* `Mish activation function instead of ReLU` : Mish is a new activation function that was released in a [paper](https://arxiv.org/abs/1908.08681). It has a much smoother curve vs relu, and in theory, that drives information more deeply through the network.\n",
    "* `Self attention layer` : Bringing in Ideas from GAN's into CNN's.The self attention layer is designed to help leverage long range dependencies within an image vs the more local feature focus of convolutions. Paper Link : [https://arxiv.org/abs/1805.08318]\n",
    "\n",
    "\n",
    "\n",
    "The folks from fast.ai who employed the above techniques noticed that `SimpleSelfAttention` layer, when placed within a Resnet block, the network converges if `SimpleSelfAttention` is placed right after a convolution layer that uses batch norm, and initializes the batchnorm weights to 0.\n",
    "\n",
    "[Source](https://github.com/sdoria/SimpleSelfAttention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ElXz7VPzCg2"
   },
   "source": [
    "The code for creating the `SimpleSelfAttention` module has been borrowed from [here](https://github.com/sdoria/SimpleSelfAttention/blob/master/xresnet.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "eLHtIG9RzCmC"
   },
   "outputs": [],
   "source": [
    "def conv1d(ni:int, no:int, ks:int=1, stride:int=1, padding:int=0, bias:bool=False):\n",
    "    \"Create and initialize a `nn.Conv1d` layer with spectral normalization.\"\n",
    "    conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n",
    "    nn.init.kaiming_normal_(conv.weight)\n",
    "    if bias: conv.bias.data.zero_()\n",
    "    return spectral_norm(conv)\n",
    "\n",
    "\n",
    "class SimpleSelfAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_in:int, ks=1, sym=False):\n",
    "        super().__init__()\n",
    "           \n",
    "        self.conv = conv1d(n_in, n_in, ks, padding=ks//2, bias=False)      \n",
    "       \n",
    "        self.gamma = nn.Parameter(torch.tensor([0.]))\n",
    "        \n",
    "        self.sym = sym\n",
    "        self.n_in = n_in\n",
    "        \n",
    "    def forward(self,x):\n",
    "          \n",
    "        if self.sym:\n",
    "            # symmetry hack by https://github.com/mgrankin\n",
    "            c = self.conv.weight.view(self.n_in,self.n_in)\n",
    "            c = (c + c.t())/2\n",
    "            self.conv.weight = c.view(self.n_in,self.n_in,1)\n",
    "                \n",
    "        size = x.size()  \n",
    "        x = x.view(*size[:2],-1)\n",
    "        convx = self.conv(x)\n",
    "        xxT = torch.bmm(x,x.permute(0,2,1).contiguous())\n",
    "        \n",
    "        o = torch.bmm(xxT, convx)\n",
    "          \n",
    "        o = self.gamma * o + x\n",
    "        return o.view(*size).contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rCMGxmSzCoi"
   },
   "source": [
    "The code block implements the `Mish` Activation function, the code for which has been borrowed from the [repository](https://github.com/digantamisra98/Mish/blob/master/Mish/Torch/functional.py) of the original author of `Mish`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Rv_KzkeLzYVI"
   },
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def mish(input):\n",
    "    '''\n",
    "    Applies the mish function element-wise:\n",
    "    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n",
    "    '''\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Mish, self).__init__()\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return mish(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx6N4e0WzYYS"
   },
   "source": [
    "The code below adds `SelfAttention` to our *x*Resnet block -\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "z9sspbJozYbG"
   },
   "outputs": [],
   "source": [
    "class xResBlock(nn.Module):\n",
    "    \"Creates a xRes Block with SelfAttention and Mish Activation\"\n",
    "    def __init__(self, in_chans: int, out_chans: int, kernel_size: int, stride: int = 1, \n",
    "                 act_cls: Callable = Mish, sa: bool = False):\n",
    "        \n",
    "        super(xResBlock, self).__init__()\n",
    "        self.idconv = None\n",
    "        self.sa = None\n",
    "        \n",
    "        self.block1 = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=kernel_size,\n",
    "                                      stride=1, use_bn=True, act_cls=act_cls, bias=False)\n",
    "         \n",
    "        self.block2 = ConvBnDropBlock(in_chans=out_chans, out_chans=out_chans, kernel_size=kernel_size, \n",
    "                                      stride=stride, use_bn=True, act_cls=None, bias=False)\n",
    "        \n",
    "        # initialize the SimpleSelfAttention Layer\n",
    "        if sa :\n",
    "            self.sa = SimpleSelfAttention(out_chans,ks=1,sym=False)    \n",
    "        \n",
    "        self.act_cls = act_cls(inplace=True)\n",
    "        \n",
    "        if in_chans != out_chans or stride != 1:\n",
    "            pool_layer = nn.AvgPool2d(stride, ceil_mode=True)\n",
    "            conv_layer = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=1, padding=0, \n",
    "                                         use_bn=True, act_cls=None, p_drop=0.0, bias=False, stride=1)\n",
    "            \n",
    "            self.idconv = nn.Sequential(pool_layer, conv_layer)       \n",
    "            \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "        for m in self.block2.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 0)\n",
    "                    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        if self.sa is not None:\n",
    "            out = self.sa(out) # <- self attention is inserted here\n",
    "        \n",
    "        if self.idconv is not None:\n",
    "            identity = self.idconv(x)\n",
    "        \n",
    "        out += identity\n",
    "        return self.act_cls(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TizKT22OHRbU"
   },
   "source": [
    "Let's build up the model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ZGoJKgBZzmVi"
   },
   "outputs": [],
   "source": [
    "class xResModelSA(nn.Sequential):\n",
    "    def __init__(self, num_outputs: int, act_cls=Mish):\n",
    "        conv = nn.Sequential(\n",
    "            ConvBnDropBlock(in_chans=3,  out_chans=32, kernel_size=3, stride=2, \n",
    "                            act_cls=act_cls, bias=False, use_bn=False),\n",
    "            ConvBnDropBlock(in_chans=32, out_chans=32, kernel_size=3, stride=1, \n",
    "                            act_cls=act_cls, bias=False, use_bn=False),\n",
    "            ConvBnDropBlock(in_chans=32, out_chans=64, kernel_size=3, stride=1, \n",
    "                            act_cls=act_cls, bias=False, use_bn=False),\n",
    "            )\n",
    "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        conv_stem  = nn.Sequential(conv, pool)    \n",
    "        block1 = xResBlock(in_chans=64, out_chans=64,  stride=1, kernel_size=3, act_cls=act_cls, sa=True)\n",
    "        block2 = xResBlock(in_chans=64, out_chans=128, stride=2, kernel_size=3, act_cls=act_cls, sa=True)\n",
    "        block3 = xResBlock(in_chans=128,out_chans=256, stride=2, kernel_size=3, act_cls=act_cls, sa=True)\n",
    "        pool_flatten = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
    "        fc = nn.Sequential(nn.Dropout(0.25), nn.Linear(256, num_outputs))\n",
    "        layers = OrderedDict(stem=conv_stem, block1=block1, block2=block2, block3=block3, pool_flatten=pool_flatten, fc=fc)\n",
    "        super(xResModelSA, self).__init__(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teBv5VmoZW2p"
   },
   "source": [
    "*`SelfAttention` + `ReLU` activation* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "id": "MR6qi9tuzmY1",
    "outputId": "c6bbda6e-4438-4e96-a1f0-4a7494923d18"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | xResModelSA      | 1.4 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.415     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            progress {\n",
       "                border: none;\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      Training\n",
       "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [310/310 03:21, Epoch 9 {'loss': '0.14', 'v_num': 3}]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>time</th>\n",
       "      <th>samples/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.022177</td>\n",
       "      <td>4.170353</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>4.148663</td>\n",
       "      <td>20.513500</td>\n",
       "      <td>1.901200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.042339</td>\n",
       "      <td>4.104131</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>3.800660</td>\n",
       "      <td>20.263000</td>\n",
       "      <td>1.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>1172.781860</td>\n",
       "      <td>0.328125</td>\n",
       "      <td>2.461102</td>\n",
       "      <td>20.163200</td>\n",
       "      <td>1.934200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.100806</td>\n",
       "      <td>9.106421</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.621551</td>\n",
       "      <td>20.242500</td>\n",
       "      <td>1.926600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.122984</td>\n",
       "      <td>5.691170</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>1.079999</td>\n",
       "      <td>20.230400</td>\n",
       "      <td>1.927800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.344758</td>\n",
       "      <td>3.325113</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.618536</td>\n",
       "      <td>20.235600</td>\n",
       "      <td>1.927300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.604839</td>\n",
       "      <td>1.476612</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.400208</td>\n",
       "      <td>20.072900</td>\n",
       "      <td>1.942900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.784274</td>\n",
       "      <td>0.772749</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.231457</td>\n",
       "      <td>20.355500</td>\n",
       "      <td>1.915900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.818548</td>\n",
       "      <td>0.632207</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.146994</td>\n",
       "      <td>20.089900</td>\n",
       "      <td>1.941300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.828629</td>\n",
       "      <td>0.724895</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.140880</td>\n",
       "      <td>20.310300</td>\n",
       "      <td>1.920200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 🎯 Final Training Accuracy: 0.982, Final Validation Accuracy: 0.829"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = \"xresblock-sa\"\n",
    "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms)\n",
    "\n",
    "# instantiate the model\n",
    "model = xResModelSA(num_outputs=len(CLASS_MAP), act_cls=nn.ReLU)\n",
    "\n",
    "# Put the model into Lightning-Task\n",
    "task = ClassificationTask(model, lr=3e-03)\n",
    "\n",
    "cbs = [ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
    "       NotebookProgressCallback(), \n",
    "       pl.callbacks.LearningRateMonitor(logging_interval=\"step\", log_momentum=True)]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, callbacks=cbs, gpus=1, precision=16, log_every_n_steps=1)\n",
    "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
    "\n",
    "# Evalute the final performance of the Model\n",
    "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
    "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
    "\n",
    "display(Markdown(f\"> 🎯 Final Training Accuracy: {round(trn_acc, 3)}, Final Validation Accuracy: {round(val_acc, 3)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rs_KKiewstlB"
   },
   "source": [
    "*`SelfAttention` + `Mish`* ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "id": "U5s_XsTHs1Xl",
    "outputId": "8cbab5c5-c831-4414-d458-7cb9017dfd45"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | xResModelSA      | 1.4 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.415     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            progress {\n",
       "                border: none;\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      Training\n",
       "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [310/310 03:22, Epoch 9 {'loss': '0.119', 'v_num': 4}]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>time</th>\n",
       "      <th>samples/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.040323</td>\n",
       "      <td>4.403789</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>4.004300</td>\n",
       "      <td>23.469900</td>\n",
       "      <td>1.661700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.056452</td>\n",
       "      <td>4.125689</td>\n",
       "      <td>0.140625</td>\n",
       "      <td>3.557842</td>\n",
       "      <td>21.174300</td>\n",
       "      <td>1.841900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.068548</td>\n",
       "      <td>11.582187</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>2.368009</td>\n",
       "      <td>20.004600</td>\n",
       "      <td>1.949500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.108871</td>\n",
       "      <td>6.606654</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>1.344991</td>\n",
       "      <td>20.090000</td>\n",
       "      <td>1.941300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.153226</td>\n",
       "      <td>5.799909</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.738546</td>\n",
       "      <td>20.300000</td>\n",
       "      <td>1.921200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>4.110724</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.688133</td>\n",
       "      <td>20.406800</td>\n",
       "      <td>1.911100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.328629</td>\n",
       "      <td>3.438424</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.388012</td>\n",
       "      <td>20.091000</td>\n",
       "      <td>1.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.824597</td>\n",
       "      <td>0.614956</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.556442</td>\n",
       "      <td>20.325700</td>\n",
       "      <td>1.918800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.836694</td>\n",
       "      <td>0.556627</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.180228</td>\n",
       "      <td>20.188900</td>\n",
       "      <td>1.931800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.846774</td>\n",
       "      <td>0.537813</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.115660</td>\n",
       "      <td>20.249200</td>\n",
       "      <td>1.926000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 🎯 Final Training Accuracy: 0.982, Final Validation Accuracy: 0.847"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = \"mxresblock-sa\"\n",
    "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms)\n",
    "\n",
    "# instantiate the model\n",
    "model = xResModelSA(num_outputs=len(CLASS_MAP), act_cls=Mish)\n",
    "\n",
    "# Put the model into Lightning-Task\n",
    "task = ClassificationTask(model, lr=3e-03)\n",
    "\n",
    "cbs = [ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
    "       NotebookProgressCallback(), \n",
    "       pl.callbacks.LearningRateMonitor(logging_interval=\"step\", log_momentum=True)]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, callbacks=cbs, gpus=1, precision=16, log_every_n_steps=1)\n",
    "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
    "\n",
    "# Evalute the final performance of the Model\n",
    "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
    "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
    "\n",
    "display(Markdown(f\"> 🎯 Final Training Accuracy: {round(trn_acc, 3)}, Final Validation Accuracy: {round(val_acc, 3)}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "FIibTevdzmfR"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del model, trainer, task\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del train_dl, valid_dl\n",
    "except: \n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXXwGwHHwCHQ"
   },
   "source": [
    "*`Mish` activation* - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "id": "R3-hr5QWwLXS",
    "outputId": "8f1b91b9-06d3-4f63-eff1-1df2d33b0b72"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | xResModel        | 1.3 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.070     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            progress {\n",
       "                border: none;\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      Training\n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 03:26, Epoch 9 {'loss': '0.0584', 'v_num': 5}]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>time</th>\n",
       "      <th>samples/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.030242</td>\n",
       "      <td>4.468395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.056336</td>\n",
       "      <td>21.697600</td>\n",
       "      <td>3.594900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.022177</td>\n",
       "      <td>9.366550</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>3.407614</td>\n",
       "      <td>20.740700</td>\n",
       "      <td>3.760700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.181452</td>\n",
       "      <td>3.536513</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>1.458171</td>\n",
       "      <td>20.658400</td>\n",
       "      <td>3.775700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.673387</td>\n",
       "      <td>1.186092</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.976937</td>\n",
       "      <td>20.622800</td>\n",
       "      <td>3.782200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.173387</td>\n",
       "      <td>5.391195</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.762040</td>\n",
       "      <td>20.894400</td>\n",
       "      <td>3.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.802419</td>\n",
       "      <td>0.663204</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.234503</td>\n",
       "      <td>20.827900</td>\n",
       "      <td>3.745000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.802419</td>\n",
       "      <td>0.629864</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.081613</td>\n",
       "      <td>20.761800</td>\n",
       "      <td>3.756900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.860887</td>\n",
       "      <td>0.455085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.103138</td>\n",
       "      <td>20.954800</td>\n",
       "      <td>3.722300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.879032</td>\n",
       "      <td>0.426057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.061851</td>\n",
       "      <td>20.642000</td>\n",
       "      <td>3.778700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.879032</td>\n",
       "      <td>0.424010</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.039342</td>\n",
       "      <td>20.573100</td>\n",
       "      <td>3.791400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 🎯 Final Training Accuracy: 0.998, Final Validation Accuracy: 0.879"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = \"Mxresblock\"\n",
    "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms, bs=32)\n",
    "\n",
    "# instantiate the model\n",
    "model = xResModel(num_outputs=len(CLASS_MAP), act_cls=Mish)\n",
    "\n",
    "# Put the model into Lightning-Task\n",
    "task = ClassificationTask(model, lr=3e-03)\n",
    "\n",
    "cbs = [ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
    "       NotebookProgressCallback(), \n",
    "       pl.callbacks.LearningRateMonitor(logging_interval=\"step\", log_momentum=True)]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, callbacks=cbs, gpus=1, precision=16, log_every_n_steps=1)\n",
    "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
    "\n",
    "# Evalute the final performance of the Model\n",
    "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
    "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
    "\n",
    "display(Markdown(f\"> 🎯 Final Training Accuracy: {round(trn_acc, 3)}, Final Validation Accuracy: {round(val_acc, 3)}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "sj5UCHYawLbk"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del model, trainer, task\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del train_dl, valid_dl\n",
    "except: \n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP7Nflj-Y5mu"
   },
   "source": [
    "We do see much improved using the above tricks so we will discard them and work on improving our original *x*Resnet based model ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIaNPUnsPwNZ"
   },
   "source": [
    "### Exploring Different Classifiers - \n",
    "\n",
    "In this section, I will take the above model and update the classifiers of the model and check their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhHFKDSKQOED"
   },
   "source": [
    "Let's add another block of layers (Dropout -> Linear -> Activation -> BatchNorm) after the `pooling layer` and the `final fc layer`  to the classifier of the model - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "pxj7575oQOMU"
   },
   "outputs": [],
   "source": [
    "class xResModelv2(nn.Sequential):\n",
    "    def __init__(self, num_outputs: int, act_cls=Mish, sa=False):\n",
    "        conv = nn.Sequential(\n",
    "            ConvBnDropBlock(in_chans=3,  out_chans=32, kernel_size=3, stride=2, act_cls=act_cls, \n",
    "                            bias=False, use_bn=False),\n",
    "            ConvBnDropBlock(in_chans=32, out_chans=32, kernel_size=3, stride=1, act_cls=act_cls, \n",
    "                            bias=False, use_bn=False),\n",
    "            ConvBnDropBlock(in_chans=32, out_chans=64, kernel_size=3, stride=1, act_cls=act_cls, \n",
    "                            bias=False, use_bn=True),\n",
    "            )\n",
    "        \n",
    "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        conv_stem  = nn.Sequential(conv, pool)     \n",
    "        block1 = xResBlock(in_chans=64, out_chans=64,  stride=1, kernel_size=3, act_cls=act_cls, sa=sa)\n",
    "        block2 = xResBlock(in_chans=64, out_chans=128, stride=2, kernel_size=3, act_cls=act_cls, sa=sa)\n",
    "        block3 = xResBlock(in_chans=128,out_chans=256, stride=2, kernel_size=3, act_cls=act_cls, sa=sa)\n",
    "        \n",
    "        pool_flatten = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
    "        \n",
    "        fc1 = nn.Sequential(nn.Dropout(0.25), nn.Linear(256, 512, bias=False), act_cls(inplace=True), nn.BatchNorm1d(512))\n",
    "        fc2 = nn.Sequential(nn.Dropout(0.5), nn.Linear(512, num_outputs, bias=False)) \n",
    "        \n",
    "        layers = OrderedDict(stem=conv_stem, block1=block1, block2=block2, block3=block3, \n",
    "                             pool_flatten=pool_flatten, fc1=fc1, fc2=fc2)\n",
    "        super(xResModelv2, self).__init__(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNDZOkCsQOVN"
   },
   "source": [
    "Training the model to evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "id": "5d9q87zmQOcz",
    "outputId": "18bf36ee-2585-4866-878a-2d4d17e8a6ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | xResModelv2      | 1.4 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.4 M     Total params\n",
      "5.663     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            progress {\n",
       "                border: none;\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      Training\n",
       "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [310/310 03:27, Epoch 9 {'loss': '0.249', 'v_num': 6}]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>time</th>\n",
       "      <th>samples/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.034274</td>\n",
       "      <td>4.608113</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>4.974113</td>\n",
       "      <td>20.679900</td>\n",
       "      <td>1.885900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>15.511646</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>3.743794</td>\n",
       "      <td>20.708200</td>\n",
       "      <td>1.883300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>5.019724</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>2.380446</td>\n",
       "      <td>20.694000</td>\n",
       "      <td>1.884600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.106855</td>\n",
       "      <td>9.635838</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>1.393277</td>\n",
       "      <td>20.929100</td>\n",
       "      <td>1.863400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>17.542812</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.976982</td>\n",
       "      <td>21.018500</td>\n",
       "      <td>1.855500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.715726</td>\n",
       "      <td>0.960965</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.775667</td>\n",
       "      <td>20.962300</td>\n",
       "      <td>1.860500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>2.427079</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>0.662684</td>\n",
       "      <td>20.912800</td>\n",
       "      <td>1.864900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>0.655303</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.477250</td>\n",
       "      <td>21.020300</td>\n",
       "      <td>1.855300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.842742</td>\n",
       "      <td>0.509028</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.351608</td>\n",
       "      <td>20.918400</td>\n",
       "      <td>1.864400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.842742</td>\n",
       "      <td>0.471490</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.199294</td>\n",
       "      <td>20.923500</td>\n",
       "      <td>1.863900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 🎯 Final Training Accuracy: 0.974, Final Validation Accuracy: 0.843"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = \"Mxresblockv2\"\n",
    "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms)\n",
    "\n",
    "# instantiate the model\n",
    "model = xResModelv2(num_outputs=len(CLASS_MAP), act_cls=Mish, sa=False)\n",
    "\n",
    "# Put the model into Lightning-Task\n",
    "task = ClassificationTask(model, lr=3e-03)\n",
    "\n",
    "cbs = [ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
    "       NotebookProgressCallback(), \n",
    "       pl.callbacks.LearningRateMonitor(logging_interval=\"step\", log_momentum=True)]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, callbacks=cbs, gpus=1, precision=16, log_every_n_steps=1)\n",
    "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
    "\n",
    "# Evalute the final performance of the Model\n",
    "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
    "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
    "\n",
    "display(Markdown(f\"> 🎯 Final Training Accuracy: {round(trn_acc, 3)}, Final Validation Accuracy: {round(val_acc, 3)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlvijK6rQOpQ"
   },
   "source": [
    "This time I will replace the classifier and pool layer of the Model with the model head that is created from by `fast.ai` s cnn_learner. The head begins with fastai's AdaptiveConcatPool2d. Then it uses a Flatten layer before going on blocks of BatchNorm, Dropout and Linear layers.\n",
    "\n",
    "[Source](https://docs.fast.ai/vision.learner.html#create_body)\n",
    "\n",
    "Let's build & train the model after adding these tweaks -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "lHPhIMGQauEY"
   },
   "outputs": [],
   "source": [
    "class AdaptiveConcatPool2d(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`\"\n",
    "    def __init__(self, size=None):\n",
    "        super(AdaptiveConcatPool2d, self).__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool2d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool2d(self.size)\n",
    "    def forward(self, x): \n",
    "        return torch.cat([self.mp(x), self.ap(x)], 1)\n",
    "\n",
    "\n",
    "class FastaiHead(nn.Sequential):\n",
    "    def __init__(self, nf: int, num_outputs: int, hiddens: int = 1024, act_cls=nn.ReLU):\n",
    "        l1 = nn.BatchNorm1d(nf)\n",
    "        l2 = nn.Dropout(p=0.25)\n",
    "        l3 = nn.Linear(in_features=nf, out_features=hiddens, bias=False)\n",
    "        l4 = act_cls(inplace=True)\n",
    "        l5 = nn.BatchNorm1d(hiddens)\n",
    "        l6 = nn.Dropout(p=0.5)\n",
    "        fc = nn.Linear(in_features=hiddens, out_features=num_outputs, bias=False)\n",
    "\n",
    "        layers = OrderedDict(fc1=nn.Sequential(l1, l2, l3, l4, l5, l6), fc2=fc)\n",
    "        super(FastaiHead, self).__init__(layers)\n",
    "\n",
    "\n",
    "# Incorporate these into our xResNet based model :\n",
    "class xResModelv3(nn.Sequential):\n",
    "    def __init__(self, num_outputs: int, act_cls=nn.ReLU, sa=False, hiddens: int = 1024):\n",
    "        conv = nn.Sequential(\n",
    "            ConvBnDropBlock(in_chans=3,  out_chans=32, kernel_size=3, stride=2, \n",
    "                            act_cls=act_cls, bias=False, use_bn=False),\n",
    "            ConvBnDropBlock(in_chans=32, out_chans=32, kernel_size=3, stride=1, \n",
    "                            act_cls=act_cls, bias=False, use_bn=False),\n",
    "            ConvBnDropBlock(in_chans=32, out_chans=64, kernel_size=3, stride=1, \n",
    "                            act_cls=act_cls, bias=False, use_bn=False),\n",
    "            )\n",
    "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        conv_stem  = nn.Sequential(conv, pool)     \n",
    "        block1 = xResBlock(in_chans=64, out_chans=64,  stride=1, kernel_size=3, \n",
    "                           act_cls=act_cls, sa=sa)\n",
    "        block2 = xResBlock(in_chans=64, out_chans=128, stride=2, kernel_size=3, \n",
    "                           act_cls=act_cls, sa=sa)\n",
    "        block3 = xResBlock(in_chans=128,out_chans=256, stride=2, kernel_size=3, \n",
    "                           act_cls=act_cls, sa=sa)\n",
    "        pool = nn.Sequential(AdaptiveConcatPool2d(1), nn.Flatten())\n",
    "        fc = FastaiHead(nf=2*256, num_outputs=num_outputs, act_cls=act_cls, hiddens=hiddens)\n",
    "        layers = OrderedDict(stem=conv_stem, block1=block1, block2=block2, block3=block3, pool_flatten=pool, fc=fc)\n",
    "        super(xResModelv3, self).__init__(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 685
    },
    "id": "elv7lwyL0UfJ",
    "outputId": "e48c01ca-3bc9-495e-faa0-491ae3da238e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | xResModelv3      | 1.5 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.190     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            progress {\n",
       "                border: none;\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      Training\n",
       "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [310/310 03:21, Epoch 9 {'loss': '0.264', 'v_num': 8}]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>time</th>\n",
       "      <th>samples/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.022177</td>\n",
       "      <td>5.847618</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>4.724604</td>\n",
       "      <td>20.353200</td>\n",
       "      <td>1.916200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.020161</td>\n",
       "      <td>20.221750</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>3.182718</td>\n",
       "      <td>20.501800</td>\n",
       "      <td>1.902300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.028226</td>\n",
       "      <td>20.828972</td>\n",
       "      <td>0.359375</td>\n",
       "      <td>2.556564</td>\n",
       "      <td>20.169500</td>\n",
       "      <td>1.933600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.036290</td>\n",
       "      <td>22.155218</td>\n",
       "      <td>0.515625</td>\n",
       "      <td>1.680501</td>\n",
       "      <td>20.401400</td>\n",
       "      <td>1.911600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.185484</td>\n",
       "      <td>5.201253</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>1.565266</td>\n",
       "      <td>19.860400</td>\n",
       "      <td>1.963700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.217742</td>\n",
       "      <td>4.616158</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.694270</td>\n",
       "      <td>19.936500</td>\n",
       "      <td>1.956200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.457661</td>\n",
       "      <td>2.475965</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.543665</td>\n",
       "      <td>20.137200</td>\n",
       "      <td>1.936700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.155242</td>\n",
       "      <td>6.301936</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.525846</td>\n",
       "      <td>20.218000</td>\n",
       "      <td>1.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.867519</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.322677</td>\n",
       "      <td>19.987500</td>\n",
       "      <td>1.951200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.816532</td>\n",
       "      <td>0.640167</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.422371</td>\n",
       "      <td>20.138800</td>\n",
       "      <td>1.936600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 🎯 Final Training Accuracy: 0.981, Final Validation Accuracy: 0.817"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = \"Mxresblockv3\"\n",
    "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms)\n",
    "\n",
    "# instantiate the model\n",
    "model = xResModelv3(num_outputs=len(CLASS_MAP), act_cls=Mish, sa=False, hiddens=512)\n",
    "\n",
    "# Put the model into Lightning-Task\n",
    "task = ClassificationTask(model, lr=3e-03)\n",
    "\n",
    "cbs = [ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
    "       NotebookProgressCallback(), \n",
    "       pl.callbacks.LearningRateMonitor(logging_interval=\"step\", log_momentum=True)]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, callbacks=cbs, gpus=1, precision=16, log_every_n_steps=1)\n",
    "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
    "\n",
    "# Evalute the final performance of the Model\n",
    "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
    "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
    "\n",
    "display(Markdown(f\"> 🎯 Final Training Accuracy: {round(trn_acc, 3)}, Final Validation Accuracy: {round(val_acc, 3)}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "NxZ16TND0PdC"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del model, trainer, task\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    del train_dl, valid_dl\n",
    "except: \n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iQjOe802mUf"
   },
   "source": [
    "We do not see much improvements with the above experiremnts. \n",
    "\n",
    "So for the final classifier architecture we will go with the architecture that we had created at the beginning.To summarize the architecture of our current best model is as follows -\n",
    "* `feature_extractor`: a `conv_stem` follwed by `3` *x*ResNet blocks.\n",
    "*  `pool_flatten`:  `nn.Sequential(OrderedDict(pool=nn.AdaptiveAvgPool2d(1), flatten=nn.Flatten()))`\n",
    "*  `fc`: `nn.Sequential(nn.Dropout(0.25), nn.Linear(256,num_outputs))`\n",
    "\n",
    "and we use `Mish` activation function in all the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tDs5CVKi19Om"
   },
   "source": [
    "### Regularization Techniques - \n",
    "> Reducing overfitting on training data via regularization\n",
    "\n",
    "In this section I will attempt to reduce the overfitting of our model using a common regularizing technique *data augmentation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vD66Vvyw2hiX"
   },
   "source": [
    "What is *data augmentation*? \n",
    "\n",
    "Data augmentation artificially increases the size of the training set by generating many realistic variants of each training instance. Data augmentation is a technique used for introducing variety in training data thereby helping to mitigate overfitting.\n",
    "\n",
    "\n",
    "Let's apply some *data augmentation* data input pipeline. We will only apply data augmentation to the training data. The validation data is not augmented. \n",
    "\n",
    "\n",
    "> Note: For the current task given we have to be very carefull while applying *data augmentation*. Since our Images our images can change depending on the rotation, I won't be applying any `rotation` or `flip` transforms. The only transforms i will be using are intorducing noise and changing the lighting of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: In this and the precedding experiments, I will be training the model for a larger number of epochs as compared to above. This is because with heavy *Regularization Techniques* it is generally a good idea to train for more number of epochs to get good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Qvgnvah64F35"
   },
   "outputs": [],
   "source": [
    "base_tfms = A.Compose([\n",
    "    A.Resize(PRESIZE, PRESIZE, p=1.0),\n",
    "    A.CenterCrop(IMG_SIZE, IMG_SIZE, p=1.0),\n",
    "    A.ToFloat(max_value=255, p=1.0),\n",
    "    ToTensorV2(p=1.0),])\n",
    "\n",
    "aug_tfms = A.Compose([\n",
    "    A.Resize(PRESIZE, PRESIZE, p=1.0),\n",
    "    A.MultiplicativeNoise(multiplier=[0.5, 1.5], per_channel=True, p=0.5),\n",
    "    A.Blur(blur_limit=10, always_apply=False, p=0.5),\n",
    "    A.JpegCompression(quality_lower=0, quality_upper=1, always_apply=False, p=0.5),\n",
    "    A.ShiftScaleRotate(rotate_limit=15, p=0.5),\n",
    "    A.CenterCrop(IMG_SIZE, IMG_SIZE, p=1.0),\n",
    "    A.Cutout(num_holes=3, always_apply=False, p=0.5),\n",
    "    A.ToFloat(max_value=255, p=1.0),\n",
    "    ToTensorV2(p=1.0),])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DMqPaqM5IhU"
   },
   "source": [
    "Let's view the augmentations on the Images -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "tprO_F9F475O",
    "outputId": "7e012a53-dbcd-46da-b9d3-c9f16135cb71"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x936 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(transforms=aug_tfms, valid_transforms=base_tfms, bs=32)\n",
    "\n",
    "ims, lbls = next(iter(train_dl))\n",
    "ims, lbls = ims[:8], lbls[:8]\n",
    "\n",
    "grid = make_grid(ims, normalize=True).permute(1, 2, 0)\n",
    "fig = plt.figure(figsize=(13, 13))\n",
    "plt.imshow(grid) \n",
    "plt.title([CLASS_MAP[o] for o in lbls.data.cpu().numpy()])\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqstDxh55MJ0"
   },
   "source": [
    "Another common technique to reduce overfitting is to increase the *weight_decay*, so let's try it also.\n",
    "\n",
    "Let's train on the dataset and explore the performance. Since this time we are using heavy data augmentations we probably would need to train more to get good results - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 995
    },
    "id": "ujmZorMj5MOd",
    "outputId": "f88007eb-66c4-425d-e225-3ca24ee513f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | xResModel        | 1.3 M \n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.070     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            progress {\n",
       "                border: none;\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      Training\n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 06:58, Epoch 19 {'loss': '0.326', 'v_num': 10}]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>time</th>\n",
       "      <th>samples/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>4.235421</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>4.168990</td>\n",
       "      <td>21.469400</td>\n",
       "      <td>1.816500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.026210</td>\n",
       "      <td>4.273798</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>4.039053</td>\n",
       "      <td>21.502000</td>\n",
       "      <td>1.813800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.056452</td>\n",
       "      <td>4.027565</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>3.959040</td>\n",
       "      <td>20.780700</td>\n",
       "      <td>1.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>4.001908</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>3.569169</td>\n",
       "      <td>20.949900</td>\n",
       "      <td>1.861600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.350806</td>\n",
       "      <td>2.628295</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>2.966611</td>\n",
       "      <td>20.692300</td>\n",
       "      <td>1.884800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.497984</td>\n",
       "      <td>1.809648</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>2.264626</td>\n",
       "      <td>21.167800</td>\n",
       "      <td>1.842400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.637097</td>\n",
       "      <td>1.438899</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>1.478992</td>\n",
       "      <td>20.811300</td>\n",
       "      <td>1.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.679435</td>\n",
       "      <td>1.133458</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>1.108948</td>\n",
       "      <td>20.851900</td>\n",
       "      <td>1.870300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.784274</td>\n",
       "      <td>0.825069</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>0.788343</td>\n",
       "      <td>20.864600</td>\n",
       "      <td>1.869200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.944664</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.673532</td>\n",
       "      <td>20.860100</td>\n",
       "      <td>1.869600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.864919</td>\n",
       "      <td>0.505733</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.747192</td>\n",
       "      <td>20.711700</td>\n",
       "      <td>1.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.796371</td>\n",
       "      <td>0.711716</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.542030</td>\n",
       "      <td>20.821200</td>\n",
       "      <td>1.873100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.842742</td>\n",
       "      <td>0.483177</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.515949</td>\n",
       "      <td>20.887400</td>\n",
       "      <td>1.867200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.862903</td>\n",
       "      <td>0.497445</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.462561</td>\n",
       "      <td>20.824000</td>\n",
       "      <td>1.872800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.885081</td>\n",
       "      <td>0.379424</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.412767</td>\n",
       "      <td>20.852400</td>\n",
       "      <td>1.870300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.380206</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.352156</td>\n",
       "      <td>20.793800</td>\n",
       "      <td>1.875600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.885081</td>\n",
       "      <td>0.359034</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.371926</td>\n",
       "      <td>20.764300</td>\n",
       "      <td>1.878200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.893145</td>\n",
       "      <td>0.355361</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.336139</td>\n",
       "      <td>21.002300</td>\n",
       "      <td>1.856900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.891129</td>\n",
       "      <td>0.350819</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.353714</td>\n",
       "      <td>20.927900</td>\n",
       "      <td>1.863500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.891129</td>\n",
       "      <td>0.351890</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.411255</td>\n",
       "      <td>21.003700</td>\n",
       "      <td>1.856800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 🎯 Final Training Accuracy: 0.916, Final Validation Accuracy: 0.893"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = \"Mxresblock-aug-wd\"\n",
    "train_dl, valid_dl = get_data(transforms=aug_tfms, valid_transforms=base_tfms)\n",
    "\n",
    "# instantiate the model\n",
    "model = xResModel(num_outputs=len(CLASS_MAP), act_cls=Mish)\n",
    "\n",
    "# Put the model into Lightning-Task\n",
    "task = ClassificationTask(model, lr=3e-03, wd=0.1)\n",
    "\n",
    "cbs = [ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
    "       NotebookProgressCallback(), \n",
    "       pl.callbacks.LearningRateMonitor(logging_interval=\"step\", log_momentum=True)]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=20, callbacks=cbs, gpus=1, precision=16, log_every_n_steps=1)\n",
    "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
    "\n",
    "# Evalute the final performance of the Model\n",
    "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
    "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
    "\n",
    "display(Markdown(f\"> 🎯 Final Training Accuracy: {round(trn_acc, 3)}, Final Validation Accuracy: {round(val_acc, 3)}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "N7iY6lNR4LJ_"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del model, trainer, task\n",
    "except: \n",
    "    pass\n",
    "try:\n",
    "    del train_dl, valid_dl\n",
    "except: \n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MldJzAA27iSm"
   },
   "source": [
    "Our model is more robust now it is not overfitting on the training data let's see if we can improve furthur..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EH_pziTJcO22"
   },
   "source": [
    "In this next part, I will be using label smoothing. Label smoothing is known to known to increase the generalization of the models. It reduces the ability of the model of the model to adapt to the training data by adding noise the original labels of the model on the training data.\n",
    "\n",
    "\n",
    "The effectiveness of label smoothing was explored in this [paper](https://arxiv.org/abs/1906.02629#:~:text=Smoothing%20the%20labels%20in%20this,language%20translation%20and%20speech%20recognition.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5d3yMIxdQla"
   },
   "source": [
    "Here's how we can intorduce Label smoothing with CrossEntropy loss in pytorch -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "ZHCIm40TdNJ1"
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"Cross Entropy Loss with Label Smoothing\"\n",
    "    def __init__(self, eps=0.1, reduction: str = \"mean\", weight=None):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        store_attr(\"eps, reduction, weight\")\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        c = input.size()[1]\n",
    "        log_preds = F.log_softmax(input, dim=1)\n",
    "        if self.reduction == \"sum\":\n",
    "            loss = -log_preds.sum()\n",
    "        else:\n",
    "            loss = -log_preds.sum(dim=1)\n",
    "            if self.reduction == \"mean\":\n",
    "                loss = loss.mean()\n",
    "        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(log_preds, target.long(), \n",
    "                                                                 weight=self.weight, reduction=self.reduction)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYbwR9a7iM-Y"
   },
   "source": [
    "With label smoothing we generally need to train more epochs to get decent results, so I will train increasing the number of epochs -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 995
    },
    "id": "Lo6XVn1KgaNg",
    "outputId": "2a61d86d-a795-42e6-e191-8688905b301c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                       | Params\n",
      "---------------------------------------------------------\n",
      "0 | model     | xResModel                  | 1.3 M \n",
      "1 | criterion | LabelSmoothingCrossEntropy | 0     \n",
      "---------------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.070     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            progress {\n",
       "                border: none;\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      Training\n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 06:58, Epoch 19 {'loss': '1.15', 'v_num': 11}]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>time</th>\n",
       "      <th>samples/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>4.240947</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>4.287457</td>\n",
       "      <td>20.930500</td>\n",
       "      <td>1.863300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>4.551510</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>4.120698</td>\n",
       "      <td>20.855700</td>\n",
       "      <td>1.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.022177</td>\n",
       "      <td>4.286321</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>4.000009</td>\n",
       "      <td>20.582400</td>\n",
       "      <td>1.894800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.074597</td>\n",
       "      <td>3.790640</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>3.752719</td>\n",
       "      <td>20.912900</td>\n",
       "      <td>1.864900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.254032</td>\n",
       "      <td>3.192256</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>3.381557</td>\n",
       "      <td>21.166000</td>\n",
       "      <td>1.842600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.514113</td>\n",
       "      <td>2.435053</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>2.596548</td>\n",
       "      <td>20.780200</td>\n",
       "      <td>1.876800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.622984</td>\n",
       "      <td>2.040122</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>2.406656</td>\n",
       "      <td>21.134000</td>\n",
       "      <td>1.845400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.743952</td>\n",
       "      <td>1.680030</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>1.717976</td>\n",
       "      <td>20.806000</td>\n",
       "      <td>1.874500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.669355</td>\n",
       "      <td>1.829832</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>1.462101</td>\n",
       "      <td>21.181300</td>\n",
       "      <td>1.841200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.770161</td>\n",
       "      <td>1.594995</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>1.646976</td>\n",
       "      <td>21.070900</td>\n",
       "      <td>1.850900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.834677</td>\n",
       "      <td>1.487002</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>1.563486</td>\n",
       "      <td>20.822200</td>\n",
       "      <td>1.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.862903</td>\n",
       "      <td>1.320350</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.355926</td>\n",
       "      <td>20.925900</td>\n",
       "      <td>1.863700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.852823</td>\n",
       "      <td>1.314239</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>1.316386</td>\n",
       "      <td>21.464700</td>\n",
       "      <td>1.816900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.249807</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>1.510128</td>\n",
       "      <td>20.944800</td>\n",
       "      <td>1.862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.885081</td>\n",
       "      <td>1.189182</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>1.248467</td>\n",
       "      <td>20.951800</td>\n",
       "      <td>1.861400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.881048</td>\n",
       "      <td>1.172561</td>\n",
       "      <td>0.781250</td>\n",
       "      <td>1.348662</td>\n",
       "      <td>21.138200</td>\n",
       "      <td>1.845000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>1.160721</td>\n",
       "      <td>0.796875</td>\n",
       "      <td>1.280231</td>\n",
       "      <td>20.908300</td>\n",
       "      <td>1.865300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.897177</td>\n",
       "      <td>1.160439</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>1.217729</td>\n",
       "      <td>20.833300</td>\n",
       "      <td>1.872000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.891129</td>\n",
       "      <td>1.153343</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>1.188019</td>\n",
       "      <td>20.866200</td>\n",
       "      <td>1.869100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.891129</td>\n",
       "      <td>1.152011</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>1.151053</td>\n",
       "      <td>20.835900</td>\n",
       "      <td>1.871800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 🎯 Final Training Accuracy: 0.915, Final Validation Accuracy: 0.897"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = \"MxResblock-aug-wd-labelsmooth\"\n",
    "train_dl, valid_dl = get_data(transforms=aug_tfms, valid_transforms=base_tfms)\n",
    "\n",
    "# instantiate the model\n",
    "model = xResModel(num_outputs=len(CLASS_MAP), act_cls=Mish)\n",
    "\n",
    "# Put the model into Lightning-Task\n",
    "task = ClassificationTask(model, lr=3e-03, wd=0.1, criterion=LabelSmoothingCrossEntropy(eps=0.1))\n",
    "\n",
    "cbs = [ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
    "       NotebookProgressCallback(), \n",
    "       pl.callbacks.LearningRateMonitor(logging_interval=\"step\", log_momentum=True)]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=20, callbacks=cbs, gpus=1, precision=16, log_every_n_steps=1)\n",
    "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
    "\n",
    "# Evalute the final performance of the Model\n",
    "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
    "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
    "\n",
    "display(Markdown(f\"> 🎯 Final Training Accuracy: {round(trn_acc, 3)}, Final Validation Accuracy: {round(val_acc, 3)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `LabelSmoothingCrossEntropy` has increased our model preformance lot. We should also note that our model *overfitting* on less on the *training data* and is actually really robust on unseen *validation data*. \n",
    "\n",
    "I think we should explore more ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "_Bm0HrZBck5H"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del model, trainer, task\n",
    "except: \n",
    "    pass\n",
    "try:\n",
    "    del train_dl, valid_dl\n",
    "except: \n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *regularizing* methods are working will for our model. This time I want to furthur test the model by increasing the `weight_decay`, we will ramp up the `weight_decay` to `0.3`. \n",
    "\n",
    "*Training Paramters* -\n",
    "* Model Arch: *xResnet* based with (dropout + Linear) classifier and Mish activation\n",
    "* Loss function : `LabelSmoothingCrossEntropy`\n",
    "* Learning_rate: `1e-03`\n",
    "* Optimizer + Scheduler: AdamW + OneCycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 995
    },
    "id": "Ensl6dWVBeQk",
    "outputId": "e6592fc5-66d9-43cd-851d-0e61f4402ca8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                       | Params\n",
      "---------------------------------------------------------\n",
      "0 | model     | xResModel                  | 1.3 M \n",
      "1 | criterion | LabelSmoothingCrossEntropy | 0     \n",
      "---------------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.070     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            progress {\n",
       "                border: none;\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      Training\n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 06:58, Epoch 19 {'loss': '1.68', 'v_num': 12}]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>time</th>\n",
       "      <th>samples/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.014113</td>\n",
       "      <td>4.236087</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>4.406903</td>\n",
       "      <td>20.775500</td>\n",
       "      <td>1.877200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.022177</td>\n",
       "      <td>4.404287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.297649</td>\n",
       "      <td>20.751300</td>\n",
       "      <td>1.879400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>4.586543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.215205</td>\n",
       "      <td>20.758200</td>\n",
       "      <td>1.878800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>4.213976</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>4.059754</td>\n",
       "      <td>20.959500</td>\n",
       "      <td>1.860700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.026210</td>\n",
       "      <td>4.220945</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>3.953399</td>\n",
       "      <td>21.119700</td>\n",
       "      <td>1.846600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.078629</td>\n",
       "      <td>3.856108</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>3.853620</td>\n",
       "      <td>20.745200</td>\n",
       "      <td>1.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.110887</td>\n",
       "      <td>3.590917</td>\n",
       "      <td>0.203125</td>\n",
       "      <td>3.481786</td>\n",
       "      <td>20.861100</td>\n",
       "      <td>1.869500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.173387</td>\n",
       "      <td>3.329273</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>3.198253</td>\n",
       "      <td>21.100400</td>\n",
       "      <td>1.848300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.260081</td>\n",
       "      <td>3.025266</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>2.935417</td>\n",
       "      <td>20.892800</td>\n",
       "      <td>1.866700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.489919</td>\n",
       "      <td>2.616322</td>\n",
       "      <td>0.421875</td>\n",
       "      <td>2.723997</td>\n",
       "      <td>20.827100</td>\n",
       "      <td>1.872600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.699597</td>\n",
       "      <td>2.119049</td>\n",
       "      <td>0.640625</td>\n",
       "      <td>2.235285</td>\n",
       "      <td>21.109900</td>\n",
       "      <td>1.847500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.522177</td>\n",
       "      <td>2.345495</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>2.133408</td>\n",
       "      <td>20.849400</td>\n",
       "      <td>1.870600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.743952</td>\n",
       "      <td>1.852223</td>\n",
       "      <td>0.578125</td>\n",
       "      <td>2.140400</td>\n",
       "      <td>20.939800</td>\n",
       "      <td>1.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.770161</td>\n",
       "      <td>1.738642</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>2.052946</td>\n",
       "      <td>21.190600</td>\n",
       "      <td>1.840400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.776210</td>\n",
       "      <td>1.729042</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>1.778018</td>\n",
       "      <td>21.016900</td>\n",
       "      <td>1.855600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.842742</td>\n",
       "      <td>1.536543</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>1.822670</td>\n",
       "      <td>20.920400</td>\n",
       "      <td>1.864200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.836694</td>\n",
       "      <td>1.481884</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>1.866893</td>\n",
       "      <td>20.970200</td>\n",
       "      <td>1.859800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.854839</td>\n",
       "      <td>1.474197</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>1.577474</td>\n",
       "      <td>20.978600</td>\n",
       "      <td>1.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.846774</td>\n",
       "      <td>1.457589</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>1.826093</td>\n",
       "      <td>20.912100</td>\n",
       "      <td>1.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.846774</td>\n",
       "      <td>1.460531</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>1.728360</td>\n",
       "      <td>21.179700</td>\n",
       "      <td>1.841400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 🎯 Final Training Accuracy: 0.802, Final Validation Accuracy: 0.855"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = \"MxResblock-aug-wd-labelsmooth-v2\"\n",
    "train_dl, valid_dl = get_data(transforms=aug_tfms, valid_transforms=base_tfms)\n",
    "\n",
    "# instantiate the model\n",
    "model = xResModel(num_outputs=len(CLASS_MAP), act_cls=Mish)\n",
    "\n",
    "# Put the model into Lightning-Task\n",
    "task = ClassificationTask(model, lr=1e-03, wd=0.3, criterion=LabelSmoothingCrossEntropy(eps=0.1))\n",
    "\n",
    "cbs = [ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
    "       NotebookProgressCallback(), \n",
    "       pl.callbacks.LearningRateMonitor(logging_interval=\"step\", log_momentum=True)]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=20, callbacks=cbs, gpus=1, precision=16, log_every_n_steps=1)\n",
    "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
    "\n",
    "# Evalute the final performance of the Model\n",
    "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
    "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
    "\n",
    "display(Markdown(f\"> 🎯 Final Training Accuracy: {round(trn_acc, 3)}, Final Validation Accuracy: {round(val_acc, 3)}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9CTrkYxnaJd"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del model, trainer, task\n",
    "except: \n",
    "    pass\n",
    "try:\n",
    "    del train_dl, valid_dl\n",
    "except: \n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end the loss was still decreasing and accuracy still increasing so, let's train again but ramp up the number of epochs ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Gbk6AbUDkdLN",
    "outputId": "6e139d41-6157-4823-acd0-b9484ffaa520"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                       | Params\n",
      "---------------------------------------------------------\n",
      "0 | model     | xResModel                  | 1.3 M \n",
      "1 | criterion | LabelSmoothingCrossEntropy | 0     \n",
      "---------------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.070     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            progress {\n",
       "                border: none;\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      Training\n",
       "      <progress value='930' max='930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [930/930 10:30, Epoch 29 {'loss': '1.27', 'v_num': 14}]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>time</th>\n",
       "      <th>samples/s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.012097</td>\n",
       "      <td>4.207198</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>4.319057</td>\n",
       "      <td>21.078000</td>\n",
       "      <td>1.850300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>4.228527</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.135925</td>\n",
       "      <td>21.256900</td>\n",
       "      <td>1.834700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>4.269227</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>4.218159</td>\n",
       "      <td>21.235400</td>\n",
       "      <td>1.836600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>4.300421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.139277</td>\n",
       "      <td>21.281900</td>\n",
       "      <td>1.832500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>4.443923</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>4.017538</td>\n",
       "      <td>21.252200</td>\n",
       "      <td>1.835100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.040323</td>\n",
       "      <td>4.201268</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>3.949118</td>\n",
       "      <td>21.149100</td>\n",
       "      <td>1.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.050403</td>\n",
       "      <td>4.028548</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>3.813335</td>\n",
       "      <td>21.430800</td>\n",
       "      <td>1.819800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.058468</td>\n",
       "      <td>3.851801</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>3.634636</td>\n",
       "      <td>20.755200</td>\n",
       "      <td>1.879100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.171371</td>\n",
       "      <td>3.372709</td>\n",
       "      <td>0.234375</td>\n",
       "      <td>3.356740</td>\n",
       "      <td>20.573500</td>\n",
       "      <td>1.895600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.272177</td>\n",
       "      <td>3.175793</td>\n",
       "      <td>0.406250</td>\n",
       "      <td>2.950300</td>\n",
       "      <td>20.779400</td>\n",
       "      <td>1.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.308468</td>\n",
       "      <td>2.813777</td>\n",
       "      <td>0.390625</td>\n",
       "      <td>2.933949</td>\n",
       "      <td>20.773100</td>\n",
       "      <td>1.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>2.731912</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>2.322737</td>\n",
       "      <td>20.746600</td>\n",
       "      <td>1.879800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.683468</td>\n",
       "      <td>2.092513</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>2.204176</td>\n",
       "      <td>20.674300</td>\n",
       "      <td>1.886400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.655242</td>\n",
       "      <td>1.982889</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>1.869247</td>\n",
       "      <td>20.824000</td>\n",
       "      <td>1.872800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.743952</td>\n",
       "      <td>1.820670</td>\n",
       "      <td>0.703125</td>\n",
       "      <td>1.844356</td>\n",
       "      <td>21.209300</td>\n",
       "      <td>1.838800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.766129</td>\n",
       "      <td>1.659970</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.573415</td>\n",
       "      <td>21.403400</td>\n",
       "      <td>1.822100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.745968</td>\n",
       "      <td>1.803182</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>1.717984</td>\n",
       "      <td>21.661200</td>\n",
       "      <td>1.800500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.776210</td>\n",
       "      <td>1.575352</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>1.489521</td>\n",
       "      <td>20.951400</td>\n",
       "      <td>1.861500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.840726</td>\n",
       "      <td>1.406284</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>1.376975</td>\n",
       "      <td>21.311100</td>\n",
       "      <td>1.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.856855</td>\n",
       "      <td>1.369984</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>1.399007</td>\n",
       "      <td>20.883700</td>\n",
       "      <td>1.867500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.296118</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>1.307240</td>\n",
       "      <td>21.090300</td>\n",
       "      <td>1.849200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.862903</td>\n",
       "      <td>1.301802</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>1.366706</td>\n",
       "      <td>21.354800</td>\n",
       "      <td>1.826300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.881048</td>\n",
       "      <td>1.261553</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>1.255512</td>\n",
       "      <td>20.758400</td>\n",
       "      <td>1.878800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>1.232776</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>1.342012</td>\n",
       "      <td>20.972100</td>\n",
       "      <td>1.859600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>1.223211</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>1.259241</td>\n",
       "      <td>20.941300</td>\n",
       "      <td>1.862300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.881048</td>\n",
       "      <td>1.216840</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>1.294663</td>\n",
       "      <td>20.789600</td>\n",
       "      <td>1.875900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.883065</td>\n",
       "      <td>1.216586</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>1.279066</td>\n",
       "      <td>20.821700</td>\n",
       "      <td>1.873000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.881048</td>\n",
       "      <td>1.210406</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.402395</td>\n",
       "      <td>20.752500</td>\n",
       "      <td>1.879300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.881048</td>\n",
       "      <td>1.206436</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>1.402231</td>\n",
       "      <td>20.716600</td>\n",
       "      <td>1.882600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.883065</td>\n",
       "      <td>1.208103</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>1.310112</td>\n",
       "      <td>20.805500</td>\n",
       "      <td>1.874500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "> 🎯 Final Training Accuracy: 0.883, Final Validation Accuracy: 0.887"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp_name = \"MxResblock-aug-wd-labelsmooth-v3\"\n",
    "train_dl, valid_dl = get_data(transforms=aug_tfms, valid_transforms=base_tfms)\n",
    "\n",
    "# instantiate the model\n",
    "model = xResModel(num_outputs=len(CLASS_MAP), act_cls=Mish)\n",
    "\n",
    "# Put the model into Lightning-Task\n",
    "task = ClassificationTask(model, lr=1e-03, wd=0.3, criterion=LabelSmoothingCrossEntropy(eps=0.1))\n",
    "\n",
    "cbs = [ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
    "       NotebookProgressCallback(), \n",
    "       pl.callbacks.LearningRateMonitor(logging_interval=\"step\", log_momentum=True)]\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=30, callbacks=cbs, gpus=1, precision=16, log_every_n_steps=1)\n",
    "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
    "\n",
    "# Evalute the final performance of the Model\n",
    "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
    "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
    "\n",
    "display(Markdown(f\"> 🎯 Final Training Accuracy: {round(trn_acc, 3)}, Final Validation Accuracy: {round(val_acc, 3)}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, finally we have a really good model that is robust. The model is overfitting on the training data and performs quite well on the validation. Accuracy also seem good considering that we have to choose between both handwritten digits and handwritten letters. This should also be noted that many of the letters and aplhabets are quite similar in shape when written, so we can make a quick analysis on the classes that our model might be getting confused in .\n",
    "\n",
    "For example: `0` and `o`; `1` and `l` ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize the training parameters of our best model - \n",
    "\n",
    "* Model Arch: *xResnet* based with (dropout + Linear) classifier and Mish activation\n",
    "* Loss function : `LabelSmoothingCrossEntropy`\n",
    "* Learning rate: `1e-03`\n",
    "* Weight Decay: 0.3\n",
    "* Optimizer + Scheduler: AdamW + OneCycle\n",
    "* Epochs trained for : 30 \n",
    "+ `data_augmentation` used ....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "upCFzgdOmo-3"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del model, trainer, task\n",
    "except: \n",
    "    pass\n",
    "try:\n",
    "    del train_dl, valid_dl\n",
    "except: \n",
    "    pass\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnriO0yRp4_l"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOSY1GRGWegF/6+8jNp6NHa",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1lEHy0eoIwCS_z-iehTA5vlq6MLntmxXm",
   "name": "midas-task-2-part-01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
