{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "midas-task-2-part-01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1lEHy0eoIwCS_z-iehTA5vlq6MLntmxXm",
      "authorship_tag": "ABX9TyNP1FlsM8ZciIcoJVekZxDH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benihime91/midas-summer-internship-2021/blob/main/midas_task_2_part_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjEbaEVaWQj9"
      },
      "source": [
        "# > Uncomment and run this cell if running on Google Colab\n",
        "# install required dependencies for google colab\n",
        "!git clone https://github.com/benihime91/midas-summer-internship-2021.git\n",
        "!pip install --upgrade -r \"/content/midas-summer-internship-2021/requirements.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxmvU0RruGhb"
      },
      "source": [
        "# > for Goggle Colab\n",
        "import sys\n",
        "sys.path.append(\"midas-summer-internship-2021/\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIttbuJAV3vv"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02qYzvKbV8fx"
      },
      "source": [
        "# Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCkK2vzKWGkW"
      },
      "source": [
        "## Part - 1\n",
        "> Use this dataset (https://www.dropbox.com/s/pan6mutc5xj5kj0/trainPart1.zip) to train a CNN. Use no other data source or pretrained networks, and explain your design choices during preprocessing, model building and training. Also, cite the sources you used to borrow techniques. A test set will be provided later to judge the performance of your classifier. Please save your model checkpoints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asp5xGW_WMhz"
      },
      "source": [
        "### Getting the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtkjtbGpWrQb"
      },
      "source": [
        "import os\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Download the 1st Dataset using :\n",
        "!wget -P \"data/\" https://www.dropbox.com/s/pan6mutc5xj5kj0/trainPart1.zip\n",
        "!unzip --qq \"data/trainPart1.zip\" -d \"data/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMBr1LfVXSEW"
      },
      "source": [
        "### Analyzing the Dataset\n",
        "> In this part we will analyze the get familiar with the given dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSkpCrcLXaRp"
      },
      "source": [
        "# imports\n",
        "import os\n",
        "import random\n",
        "from typing import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from fastcore.all import *\n",
        "from torchvision.datasets.folder import IMG_EXTENSIONS\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqblcJPnXd7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0744a19-97c9-463b-8251-624d481dffc0"
      },
      "source": [
        "DATASET_01_PATH = Path(\"data/train/\")\n",
        "DATASET_01_PATH.ls()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(#62) [Path('data/train/Sample033'),Path('data/train/Sample025'),Path('data/train/Sample053'),Path('data/train/Sample026'),Path('data/train/Sample050'),Path('data/train/Sample044'),Path('data/train/Sample035'),Path('data/train/Sample060'),Path('data/train/Sample024'),Path('data/train/Sample020')...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g4-FfRrXf_i"
      },
      "source": [
        "Perform some one-off data manipulations to get all the files and filenames in an easy to use format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcAjvjblXn9Q"
      },
      "source": [
        "def folder2df(directory: Union[str, Path], extensions: list = IMG_EXTENSIONS,\n",
        "              shuffle: bool = False, seed: int = 42):\n",
        "    \"\"\"\n",
        "    Parses all the Images in `directory` and puts them in a `DataFrame` object.\n",
        "    \"\"\"\n",
        "\n",
        "    random.seed(seed)\n",
        "\n",
        "    image_list = L()\n",
        "    target_list = L()\n",
        "\n",
        "    if not isinstance(directory, Path):\n",
        "        directory = Path(directory)\n",
        "\n",
        "    for label in directory.ls():\n",
        "        label = Path(label)\n",
        "        if os.path.isdir(label):\n",
        "            for img in label.ls():\n",
        "                if str(img).lower().endswith(extensions):\n",
        "                    image_list.append(img)\n",
        "                    target_list.append(str(label).split(os.path.sep)[-1])\n",
        "\n",
        "    print(f\"Found {len(image_list)} files belonging to {len(set(target_list))} classes.\")\n",
        "\n",
        "    dataframe: pd.DataFrame = pd.DataFrame()\n",
        "    dataframe[\"image_id\"] = image_list.map(str)\n",
        "    dataframe[\"target\"] = target_list\n",
        "    if shuffle:\n",
        "        dataframe = (dataframe.sample(frac=1, random_state=seed)\n",
        "                     .reset_index(inplace=False, drop=True))\n",
        "    return dataframe"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDgGDQ_fivrx"
      },
      "source": [
        "Create the a `pandas Dataframe` which contains all the file info we need to get started"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI-aXyU-XpbQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "eb448fd8-c559-4482-b6d8-35075a0d4e11"
      },
      "source": [
        "DATASET_01 = folder2df(directory=DATASET_01_PATH, shuffle=True)\n",
        "DATASET_01.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2480 files belonging to 62 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/train/Sample004/img004-001.png</td>\n",
              "      <td>Sample004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/train/Sample035/img035-004.png</td>\n",
              "      <td>Sample035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/train/Sample043/img043-008.png</td>\n",
              "      <td>Sample043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/train/Sample046/img046-005.png</td>\n",
              "      <td>Sample046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/train/Sample025/img025-050.png</td>\n",
              "      <td>Sample025</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              image_id     target\n",
              "0  data/train/Sample004/img004-001.png  Sample004\n",
              "1  data/train/Sample035/img035-004.png  Sample035\n",
              "2  data/train/Sample043/img043-008.png  Sample043\n",
              "3  data/train/Sample046/img046-005.png  Sample046\n",
              "4  data/train/Sample025/img025-050.png  Sample025"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXDt8fOwiyoA"
      },
      "source": [
        "Lets look closer at the data, how many `class_ids` do we have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmIk-P2AXrbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f1cdd6a-78e4-4732-dff5-cc00959e76b1"
      },
      "source": [
        "unq_cls = DATASET_01.target.unique()\n",
        "tot_itm = len(DATASET_01)\n",
        "print(\"Total number of Images in the Dataset: \", tot_itm)\n",
        "print(\"Number of unique classes in the Dataset: \",len(unq_cls))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of Images in the Dataset:  2480\n",
            "Number of unique classes in the Dataset:  62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD9BffaOXtT2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "outputId": "2e295b1d-d025-4ab4-c47d-2cb2722a21a3"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "_, ax = plt.subplots(1, 1, figsize=(22, 8))\n",
        "sns.countplot(data=DATASET_01, x=\"target\", ax=ax);\n",
        "plt.xticks(rotation=90);"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1584x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXu2FJYvXwN6"
      },
      "source": [
        "The distibution across all the classes is exactly same. This is a good thing because in this way our model will be less prone to overfitting for a prticular class. We have around 40 Images for each class, which is way less data than I would have preferred. We might overfit the training data due to lack of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6rmhTSKYASE"
      },
      "source": [
        "### Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mT-BtUiYCqe"
      },
      "source": [
        "Since, in this task in am going to be using `PyTorch`, before we can directly start training the `CNN network`, we need to get our data into `Dataset` and `DataLoader`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXuQWbBuYEXs"
      },
      "source": [
        "import albumentations as A\n",
        "import cv2\n",
        "import torchvision.transforms as T\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.utils import make_grid"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf5eLkaHYF9w"
      },
      "source": [
        "We also need to encode our string labels into interger labels. For this we will use the `LabelEncoder()` from scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVIXgw04YJ5x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f77e58fb-4ac5-4f53-8c4f-1ae610bd4b7a"
      },
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit(unq_cls)\n",
        "\n",
        "def encode_label(x):\n",
        "    \"Encoder `x`, given x is a scalar value\"\n",
        "    return encoder.transform([x]).item()\n",
        "\n",
        "CLASS_MAP = L(list(encoder.classes_)).map_dict(encode_label)\n",
        "CLASS_MAP = {k:v for v, k in CLASS_MAP.items()}\n",
        "\n",
        "DATASET_01[\"cat_label\"] = DATASET_01[\"target\"].map(encode_label)\n",
        "DATASET_01.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>target</th>\n",
              "      <th>cat_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>data/train/Sample004/img004-001.png</td>\n",
              "      <td>Sample004</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>data/train/Sample035/img035-004.png</td>\n",
              "      <td>Sample035</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>data/train/Sample043/img043-008.png</td>\n",
              "      <td>Sample043</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>data/train/Sample046/img046-005.png</td>\n",
              "      <td>Sample046</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>data/train/Sample025/img025-050.png</td>\n",
              "      <td>Sample025</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              image_id     target  cat_label\n",
              "0  data/train/Sample004/img004-001.png  Sample004          3\n",
              "1  data/train/Sample035/img035-004.png  Sample035         34\n",
              "2  data/train/Sample043/img043-008.png  Sample043         42\n",
              "3  data/train/Sample046/img046-005.png  Sample046         45\n",
              "4  data/train/Sample025/img025-050.png  Sample025         24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75E8ot1jjNyw"
      },
      "source": [
        "Save a copy of the `CLASS_MAP` which stores the mapping of `classes` and `class ids`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyUFNlVxjQ1f"
      },
      "source": [
        "import json\n",
        "\n",
        "with open(\"data/class_map_ds01.json\", 'w') as f:\n",
        "    json.dump(CLASS_MAP, f)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InRu75JbYS6D"
      },
      "source": [
        "Let's also create a Training and Validation split on the data.\n",
        "\n",
        "Why we need a different training and validation split for the data ?\n",
        "\n",
        "This is so that we don't inadvertently overfit, train a model to work well only on our training data. For this purpose we will compute metric over on the validation data. During the training phase the model will see only the training data. The Validation data will remain separate and will only be used for calculation of the *metric*. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9YRWmMAYXPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bb0801f-18e6-4db7-cbeb-6e2e58d4d031"
      },
      "source": [
        "TRAIN_DATASET_01, VALID_DATASET_01 = train_test_split(DATASET_01, test_size=0.2, random_state=42, \n",
        "                                                      shuffle=True, stratify=DATASET_01['cat_label'])\n",
        "\n",
        "TRAIN_DATASET_01 = TRAIN_DATASET_01.reset_index(drop=True, inplace=False)\n",
        "VALID_DATASET_01 = VALID_DATASET_01.reset_index(drop=True, inplace=False) \n",
        "\n",
        "print(\"Num Training Examples: \", len(TRAIN_DATASET_01))\n",
        "print(\"Num Validation Examples: \", len(VALID_DATASET_01))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num Training Examples:  1984\n",
            "Num Validation Examples:  496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m0bwAFHYbKL"
      },
      "source": [
        "Now we will create a custom `Dataset` obj that can parse the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tCeY67JYilJ"
      },
      "source": [
        "class PandasDataset(Dataset):\n",
        "    def __init__(self, df:pd.DataFrame, transforms):\n",
        "        self._dataframe  = df\n",
        "        self._transforms = transforms\n",
        "        self._gray_scale = T.Grayscale(num_output_channels=3)\n",
        "    \n",
        "    @property\n",
        "    def transforms(self):\n",
        "        return self._transforms\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._dataframe)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        im = self._dataframe['image_id'][idx]\n",
        "        \n",
        "        # Load and apply transformations to the Image\n",
        "        im = cv2.imread(im)\n",
        "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "        im = self._transforms(image=im)[\"image\"]\n",
        "\n",
        "        # convert the Images to grayscale Images with 3 dimensions\n",
        "        im = self._gray_scale(im)\n",
        "        \n",
        "        lbl = self._dataframe[\"cat_label\"][idx]   \n",
        "        return im, lbl"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NHWPxawYj4I"
      },
      "source": [
        "Lets define what we want our transforms to be -\n",
        "\n",
        "We'll use the presize the images 160 px make things faster still, and will center crop to 128 px.\n",
        "Initially I will not be using any other fancy data augmentations. After the resizing I will normalize the Images by dividing by 255.0 and also converting them to pytorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsS6h-b9Ymb6"
      },
      "source": [
        "PRESIZE  = 80\n",
        "IMG_SIZE = 64\n",
        "\n",
        "base_tfms = A.Compose([\n",
        "    A.Resize(PRESIZE, PRESIZE, p=1.0),\n",
        "    A.CenterCrop(IMG_SIZE, IMG_SIZE, p=1.0),\n",
        "    A.ToFloat(max_value=255, p=1.0),\n",
        "    ToTensorV2(p=1.0),\n",
        "])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZCN6NMcvwhI"
      },
      "source": [
        "Function to get our data -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqV_2ZmOvvJB"
      },
      "source": [
        "def get_data(transforms = base_tfms, valid_transforms=base_tfms, bs: int = 32):\n",
        "    # instantiate the dataset set obj\n",
        "    train_ds = PandasDataset(df=TRAIN_DATASET_01, transforms=transforms)\n",
        "    valid_ds = PandasDataset(df=VALID_DATASET_01, transforms=valid_transforms)\n",
        "\n",
        "    # create the dataloaders obj\n",
        "    train_dl = DataLoader(train_ds, batch_size=bs, num_workers=num_cpus(), shuffle=True) \n",
        "    valid_dl = DataLoader(valid_ds, batch_size=bs, num_workers=num_cpus(), shuffle=False)\n",
        "    return train_dl, valid_dl"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8gP2ClSYq2J"
      },
      "source": [
        "Having a look at the images in our `DataLoader` you can see result of the image transforms:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ceVZHQ6YtWi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "874124c3-125a-44cd-cc92-8cca77bbd40e"
      },
      "source": [
        "train_dl, valid_dl = get_data(transforms=base_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "ims, lbls = next(iter(train_dl))\n",
        "ims, lbls = ims[:8], lbls[:8]\n",
        "\n",
        "grid = make_grid(ims, normalize=True).permute(1, 2, 0)\n",
        "fig = plt.figure(figsize=(13, 13))\n",
        "plt.imshow(grid) \n",
        "plt.title([CLASS_MAP[o] for o in lbls.data.cpu().numpy()])\n",
        "plt.axis(\"off\");\n",
        "\n",
        "del train_dl, valid_dl"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 936x936 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD2fNoNqbWv1"
      },
      "source": [
        "### PyTorch Lightning Task -\n",
        "For training I will be using PyTorch Lightning ⚡️. PyTorch Lightning expects the whole training pipeline to be defined under a `LightningModule`. Let's convert our model in a `LightningModule`.\n",
        "\n",
        "For evaluating the model initially I will use the `Accuracy` metric and for loss function i will use the standart `nn.CrossEntropyLoss()` from PyTorch.\n",
        "\n",
        "\n",
        "The code block below create a general PyTorch Lightning Classification Task which helps us iterate different models inside a `LightningModule.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HPiGTulY7ma",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de3ffd6c-9ea0-401f-e540-e9d156a58ab1"
      },
      "source": [
        "import gc\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from IPython.display import Markdown, display\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.metrics import functional as FM\n",
        "from torch import nn, optim\n",
        "from torch.nn.utils import spectral_norm\n",
        "\n",
        "from progress import NotebookProgressCallback\n",
        "\n",
        "pl.seed_everything(42)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veOLW0L2btN8"
      },
      "source": [
        "class ClassificationTask(pl.LightningModule):\n",
        "    def __init__(self, model: nn.Module, lr: float, wd: float = 0, criterion: nn.Module = nn.CrossEntropyLoss()):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(\"lr\", \"wd\")\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        \"Same as nn.Module forward\"\n",
        "        return self.model(xb)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "         x, y = batch\n",
        "         y_hat = self.model(x)\n",
        "         loss = self.criterion(y_hat, y)\n",
        "         self.log(\"train_loss\", loss)\n",
        "         return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx, *args, **kwargs):\n",
        "        x, y = batch\n",
        "        y_hat = self.model(x)\n",
        "        loss = self.criterion(y_hat, y)\n",
        "        acc = FM.accuracy(F.softmax(y_hat), y)\n",
        "        metrics = {'val_acc': acc, 'val_loss': loss}\n",
        "        self.log_dict(metrics)\n",
        "        return metrics\n",
        "\n",
        "    def test_step(self, batch, batch_idx, *args, **kwargs):\n",
        "        metrics = self.validation_step(batch, batch_idx, *args, **kwargs)\n",
        "        metrics = {'test_acc': metrics['val_acc'], 'test_loss': metrics['val_loss']}\n",
        "        self.log_dict(metrics)\n",
        "        \n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        define optimizers and LR schedulers for use in training.\n",
        "        \"\"\"\n",
        "        # default Adam parameters from fast.ai\n",
        "        opt   = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.wd, betas=(0.9, 0.99), eps=1e-05)\n",
        "        steps = len(self.train_dataloader())\n",
        "        epochs= self.trainer.max_epochs\n",
        "        \n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(opt, max_lr=self.hparams.lr, epochs=epochs, steps_per_epoch=steps)\n",
        "        return [opt], [dict(scheduler=scheduler, interval='step')]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG688lVucgbE"
      },
      "source": [
        "For training I will be using the popular `AdamW + 1cycle` training approach. Here we use the `AdamW` optimizer and `OneCycleLR` scheduler from PyTorch.\n",
        "\n",
        "`AdamW` was inroduced in this [paper](https://arxiv.org/abs/1711.05101). Experiments by the fast.ai group has shown that `AdamW` along with the `1cycle policy` of learning rate scheduling gives very good results. `AdamW` differs from `Adam` is the sense that AdamW uses true weight decay (decay the weights directly) while Adam uses L2 regularization (add the decay to the gradients).\n",
        "\n",
        "The `1cycle policy` was introduced by Leslie N. Smith et al. in [Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates](https://arxiv.org/abs/1708.07120).\n",
        "\n",
        "\n",
        "`AdamW + 1cycle` was incremental in fast.ai winning the `DAWNBench` competition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ka0ItH0LYuu1"
      },
      "source": [
        "### Benchmark Model\n",
        "First I will let's start by making a good baseline Model and subsequently we will experiment by improving upon this model.\n",
        "\n",
        "Let's first create the building blocks that we will use building our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pQi76LYY2jp"
      },
      "source": [
        "class ConvBnDropBlock(nn.Sequential):\n",
        "    \"Create sequence of convolutional, Activation, `BatchNorm` & Drouput layers.\"\n",
        "    def __init__(self, in_chans, out_chans, kernel_size, stride=1, dilation=1, \n",
        "                 padding = None, bias=True, act_cls: Callable = nn.ReLU, \n",
        "                 p_drop=0.0, use_bn=True,):\n",
        "\n",
        "        if padding is None: \n",
        "            padding = (kernel_size-1)//2\n",
        "        \n",
        "        layers = []\n",
        "        \n",
        "        conv_layer = nn.Conv2d(in_chans, out_chans, kernel_size, stride, \n",
        "                               dilation=dilation, padding=padding, bias=bias)\n",
        "        \n",
        "        if act_cls is not None:\n",
        "            act_layer = act_cls(inplace=True)\n",
        "        else:\n",
        "            act_layer = nn.Identity()\n",
        "        if use_bn:\n",
        "            norm_layer = nn.BatchNorm2d(out_chans)\n",
        "        else:\n",
        "            norm_layer = nn.Identity()\n",
        "           \n",
        "        layers += [conv_layer, act_layer, norm_layer]\n",
        "        \n",
        "        if p_drop > 0.0:\n",
        "            layers.append(nn.Dropout2d(p=p_drop))\n",
        "            \n",
        "        super(ConvBnDropBlock, self).__init__(*layers)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0wlcZ57kLIX"
      },
      "source": [
        "For this whole experiment my models will be divided into 2 parts a `feature_extractor` and `classifier`. The feature extractor is responsible for generating the feature maps from the Images, while the `classifier` is responsible for final classification.\n",
        "\n",
        "A baseline model will establish a minimum model performance to which all of our other models can be compared, as well as a model architecture that we can use as the basis of study and improvement.\n",
        "\n",
        "A feature extractor of baseline model architecture has been partly inspired from the general architectural principles of the `VGG models` & from this [blog post](https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/).\n",
        "\n",
        "The architecture involves stacking convolutional layers with small `3×3` filters followed by a activation, batchnorm, dropout.Together, these layers form a block, and these blocks can be repeated where the number of filters in each block is increased with the depth of the network such as 32, 64, 128. The first 2 blocks are followed by a max_pool layer while the 3rd layer is follwed by a `pool_flatten` layer i.e., a combination of  AdaptiveAvgPool2d and flatten layer from pytorch.\n",
        "\n",
        "We use a AdaptiveAvgPool layer so that our network is *fully convolutional* and so can work for any image size that is to say that a model trainined on 64x64 images will also work for 120x120 images.\n",
        "\n",
        "This completes the feature extractor part of the model. For the classifier of the model: first, the feature maps output from the feature extraction part of the model must be flattened.\n",
        "We than use a Dropuout layer followed by a Linear layer to generate the predictions.\n",
        "\n",
        "Let's create the network architecture -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwB7Gpy5ZAeX"
      },
      "source": [
        "class BenchmarkModel(nn.Sequential):\n",
        "    def __init__(self, num_outputs: int):\n",
        "        conv_block_01 = ConvBnDropBlock(in_chans=3, out_chans=32, bias=False, \n",
        "                                        use_bn=True, act_cls=nn.ReLU, p_drop=0.25, \n",
        "                                        kernel_size=3)\n",
        "        conv_block_02 = ConvBnDropBlock(in_chans=32, out_chans=32, bias=False, \n",
        "                                        use_bn=True, act_cls=None, p_drop=0.25, \n",
        "                                        kernel_size=3)\n",
        "        pool_block_0  = nn.Sequential(nn.MaxPool2d(2), nn.ReLU())\n",
        "\n",
        "        conv_block_11 = ConvBnDropBlock(in_chans=32, out_chans=64, bias=False, \n",
        "                                        use_bn=True, act_cls=nn.ReLU, p_drop=0.25, \n",
        "                                        kernel_size=3)\n",
        "        conv_block_12 = ConvBnDropBlock(in_chans=64, out_chans=64, bias=False, \n",
        "                                        use_bn=True, act_cls=None, p_drop=0.25, \n",
        "                                        kernel_size=3)\n",
        "        pool_block_1  = nn.Sequential(nn.MaxPool2d(2), nn.ReLU())\n",
        "\n",
        "        conv_block_21 = ConvBnDropBlock(in_chans=64, out_chans=128, bias=False, \n",
        "                                        use_bn=True, act_cls=nn.ReLU, p_drop=0.25, \n",
        "                                        kernel_size=3)\n",
        "        conv_block_22 = ConvBnDropBlock(in_chans=128, out_chans=128, bias=False, stride=2,\n",
        "                                        use_bn=True, act_cls=nn.ReLU, p_drop=0.25, \n",
        "                                        kernel_size=3, padding=0)\n",
        "        \n",
        "        # ensemble the model building blocks\n",
        "        block1 = nn.Sequential(conv_block_01, conv_block_02, pool_block_0)\n",
        "        block2 = nn.Sequential(conv_block_11, conv_block_12, pool_block_1)\n",
        "        block3 = nn.Sequential(conv_block_21, conv_block_22)\n",
        "        pool_flatten = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
        "        fc = nn.Sequential(nn.Dropout(0.3), nn.Linear(128, num_outputs, bias=False))\n",
        "\n",
        "        layers = [block1, block2, block3, pool_flatten, fc]\n",
        "        super(BenchmarkModel, self).__init__(*layers)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdNxn-amZEhK"
      },
      "source": [
        "*Lets Get Training*\n",
        "\n",
        "----\n",
        "\n",
        "Train the model initially for 20 epochs and view preformance. I found that 20 epochs was a good place to stop as the model started to overfit while the accuracy didn't improve further and any lower than 20 epochs the model doesn't convergerce.\n",
        "\n",
        "\n",
        "I will also be using `ModelCheckpoint` callback from pytorch lightning in all of my experiments to that i will be easily able to load my best checkpoints and compare my performaces.\n",
        "\n",
        "\n",
        "`3e-3` is often a good learning rate for CNNs, so let's try that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbttRUWmScwB"
      },
      "source": [
        "# BenchmarkModel(len(CLASS_MAP))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2OfLQXxZHcQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca54a745-1556-460f-9203-4217852ee0a9"
      },
      "source": [
        "exp_name = \"stage-01-cnn-bench\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms, bs=32)\n",
        "\n",
        "# instantiate the model\n",
        "model = BenchmarkModel(num_outputs=len(CLASS_MAP))\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=20, callbacks=cbs, gpus=1, precision=16)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | BenchmarkModel   | 295 K \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "295 K     Trainable params\n",
            "0         Non-trainable params\n",
            "295 K     Total params\n",
            "1.182     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='1240' max='1240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1240/1240 06:48, Epoch 19 {'loss': '1.09', 'v_num': 0}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>4.165741</td>\n",
              "      <td>4.005031</td>\n",
              "      <td>20.166400</td>\n",
              "      <td>3.867800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.024194</td>\n",
              "      <td>4.084889</td>\n",
              "      <td>4.055239</td>\n",
              "      <td>20.158800</td>\n",
              "      <td>3.869300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.034274</td>\n",
              "      <td>3.957995</td>\n",
              "      <td>3.986959</td>\n",
              "      <td>20.163300</td>\n",
              "      <td>3.868400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.104839</td>\n",
              "      <td>3.679163</td>\n",
              "      <td>3.799083</td>\n",
              "      <td>20.683200</td>\n",
              "      <td>3.771200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.127016</td>\n",
              "      <td>3.430405</td>\n",
              "      <td>3.481371</td>\n",
              "      <td>20.189000</td>\n",
              "      <td>3.863500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.239919</td>\n",
              "      <td>3.078896</td>\n",
              "      <td>3.149173</td>\n",
              "      <td>20.507400</td>\n",
              "      <td>3.803500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>2.740639</td>\n",
              "      <td>2.871879</td>\n",
              "      <td>20.422200</td>\n",
              "      <td>3.819400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>2.207591</td>\n",
              "      <td>2.323129</td>\n",
              "      <td>20.641200</td>\n",
              "      <td>3.778900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.512097</td>\n",
              "      <td>2.067173</td>\n",
              "      <td>2.420171</td>\n",
              "      <td>20.327700</td>\n",
              "      <td>3.837100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>1.713780</td>\n",
              "      <td>1.786276</td>\n",
              "      <td>20.364700</td>\n",
              "      <td>3.830200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.685484</td>\n",
              "      <td>1.413857</td>\n",
              "      <td>1.837183</td>\n",
              "      <td>20.441000</td>\n",
              "      <td>3.815900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.709677</td>\n",
              "      <td>1.295217</td>\n",
              "      <td>1.423525</td>\n",
              "      <td>20.366500</td>\n",
              "      <td>3.829800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.735887</td>\n",
              "      <td>1.221455</td>\n",
              "      <td>1.394401</td>\n",
              "      <td>20.466400</td>\n",
              "      <td>3.811100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.766129</td>\n",
              "      <td>1.092865</td>\n",
              "      <td>1.401184</td>\n",
              "      <td>20.335300</td>\n",
              "      <td>3.835700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.758065</td>\n",
              "      <td>1.067364</td>\n",
              "      <td>1.319750</td>\n",
              "      <td>20.447200</td>\n",
              "      <td>3.814700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.760081</td>\n",
              "      <td>1.001787</td>\n",
              "      <td>1.121193</td>\n",
              "      <td>20.543400</td>\n",
              "      <td>3.796800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.747984</td>\n",
              "      <td>0.989648</td>\n",
              "      <td>1.066340</td>\n",
              "      <td>20.278600</td>\n",
              "      <td>3.846400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.778226</td>\n",
              "      <td>0.960525</td>\n",
              "      <td>0.939700</td>\n",
              "      <td>20.539300</td>\n",
              "      <td>3.797600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.774194</td>\n",
              "      <td>0.952818</td>\n",
              "      <td>1.127597</td>\n",
              "      <td>20.681800</td>\n",
              "      <td>3.771400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.794355</td>\n",
              "      <td>0.955149</td>\n",
              "      <td>1.114097</td>\n",
              "      <td>20.497900</td>\n",
              "      <td>3.805300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.828**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.794**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRAlFbD1ZeYh"
      },
      "source": [
        "That's a pretty good start, considering we have to pick the correct one of 62 categories, and we're training from scratch. So in the subsequent sections we will explore tricks that can probably help us to improve our results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqlNrojraX5-"
      },
      "source": [
        "try:\n",
        "    del model, trainer, task\n",
        "except: \n",
        "    pass\n",
        "try:\n",
        "    del train_dl, valid_dl\n",
        "except: \n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bms8tB0La3Z4"
      },
      "source": [
        "## Improving the Benchmark \n",
        "> This section will explore various techniques which will hopefully help us in improving our baseline model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH0VC7qNbD1S"
      },
      "source": [
        "### Residual Blocks -\n",
        "\n",
        "In our above trained `CNN` model We can do way better than the current results using a deeper mode, but just stacking new layers won't really improve our results.\n",
        "In this experiment we will explore the preformance of our model by introducing `residual connection`s. It was introduced in 2015 by Kaiming He et al. in the article \"[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\". Residual Connections are the main building blocks of the ResNet family of networks, which are one of the most popular CNN Architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq6vibo_vvi1"
      },
      "source": [
        "Here's the definition of a simple ResNet block -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGhgvOpXdjjG"
      },
      "source": [
        "class ResBlock(nn.Module):\n",
        "    \"Creates a simple Residual Block\"\n",
        "    def __init__(self, in_chans: int, out_chans: int, \n",
        "                 kernel_size: int, stride: int = 1, act_cls: Callable = nn.ReLU):\n",
        "        \n",
        "        super(ResBlock, self).__init__()\n",
        "        self.idconv = None\n",
        "        \n",
        "        self.block1 = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=kernel_size, \n",
        "                                      stride=stride, padding=None, use_bn=True, act_cls=act_cls, \n",
        "                                      p_drop=0.0, bias=False)\n",
        "        \n",
        "        self.block2 = ConvBnDropBlock(in_chans=out_chans, out_chans=out_chans, kernel_size=kernel_size, \n",
        "                                      stride=1, padding=None, use_bn=True, act_cls=None, p_drop=0.0, bias=False)\n",
        "        \n",
        "        self.act_cls = act_cls(inplace=True)\n",
        "        \n",
        "        if in_chans != out_chans or stride != 1:\n",
        "            self.idconv = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=1, \n",
        "                                          stride=stride, padding=0, use_bn=True, act_cls=None, \n",
        "                                          p_drop=0.0, bias=False)\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "         \n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        for m in self.block2.modules():\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 0)\n",
        "                    \n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.block1(x)\n",
        "        out = self.block2(out)\n",
        "        \n",
        "        if self.idconv is not None:\n",
        "            identity = self.idconv(x)\n",
        "        \n",
        "        out += identity\n",
        "        return self.act_cls(out)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YmH2dP_jcg9"
      },
      "source": [
        "Why use `idconv` block ?\n",
        ">The issue is that with a stride of, say, 2 on one of the convolutions, the grid size of the output activations will be half the size on each axis of the input. So then we can't add that back to x in forward because x and the output activations have different dimensions. The same basic issue occurs if `in_chans`!=`out_chans`: the shapes of the input and output connections won't allow us to add them together. So `idconv` works as a identity map that matches the dimenstions of x to the ouput of `self.block2(self.block1(x))`. `idconv` increases the `channels` out the x and downsamples it with stride if required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6V7-ww3wA8b"
      },
      "source": [
        "We can now proceed towards modifying our BenchMark model. Let's add some skip connections into this network. We remove modify the 1st conv block of the model to resemble the layer in a typical `ResNet` Model, i.e, a conv layer (output_channels=64, kernel_size=7, stride=2) followed by a batch_normalization, activation and a MaxPooling layer. We then add the residual blocks of the model. The classifier of the model remains same.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N0geEarjcry"
      },
      "source": [
        "class ResModel(nn.Sequential):\n",
        "    def __init__(self, num_outputs: int):\n",
        "        conv = ConvBnDropBlock(in_chans=3, out_chans=64, kernel_size=7, p_drop=0, stride=2, \n",
        "                               act_cls=nn.ReLU, bias=False, use_bn=True)\n",
        "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        conv_stem  = nn.Sequential(conv, pool)\n",
        "        block1 = ResBlock(in_chans=64, out_chans=64,  stride=1, kernel_size=3, act_cls=nn.ReLU)\n",
        "        block2 = ResBlock(in_chans=64, out_chans=128, stride=2, kernel_size=3, act_cls=nn.ReLU)\n",
        "        block3 = ResBlock(in_chans=128,out_chans=256, stride=2, kernel_size=3, act_cls=nn.ReLU)\n",
        "        pool_flatten = nn.Sequential(OrderedDict(pool_layer=nn.AdaptiveAvgPool2d(1), flatten=nn.Flatten()))\n",
        "        fc = nn.Sequential(nn.Dropout(0.25), nn.Linear(256, num_outputs))\n",
        "        layers = OrderedDict(stem=conv_stem, block1=block1, block2=block2, block3=block3, pool_flatten=pool_flatten, fc=fc)\n",
        "        super(ResModel, self).__init__(layers)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9O0g6ihyEpp"
      },
      "source": [
        "Since this is a larger model than the previous one I will reduce the number of training epochs so that our model does not overfit the training data and quick experimentation purposes.\n",
        "\n",
        "Let's train it for a little bit and see how it fares compared to the previous model - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuvF6ax3jc1h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "outputId": "170848ac-c128-455c-e843-3633944cad65"
      },
      "source": [
        "exp_name = \"stage-01-resblock\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms, bs=32)\n",
        "\n",
        "# instantiate the model\n",
        "model = ResModel(num_outputs=len(CLASS_MAP))\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=8, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | ResModel         | 1.2 M \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "1.2 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.2 M     Total params\n",
            "4.995     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='496' max='496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [496/496 02:46, Epoch 7 {'loss': '0.325', 'v_num': 1}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.028226</td>\n",
              "      <td>4.102634</td>\n",
              "      <td>4.063929</td>\n",
              "      <td>20.258000</td>\n",
              "      <td>3.850300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.024194</td>\n",
              "      <td>4.625007</td>\n",
              "      <td>3.525003</td>\n",
              "      <td>20.918500</td>\n",
              "      <td>3.728700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.330645</td>\n",
              "      <td>2.600448</td>\n",
              "      <td>2.461618</td>\n",
              "      <td>20.941700</td>\n",
              "      <td>3.724600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.469758</td>\n",
              "      <td>1.895628</td>\n",
              "      <td>1.286858</td>\n",
              "      <td>20.880200</td>\n",
              "      <td>3.735600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.612903</td>\n",
              "      <td>1.393237</td>\n",
              "      <td>0.908120</td>\n",
              "      <td>20.838800</td>\n",
              "      <td>3.743000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.758065</td>\n",
              "      <td>0.820465</td>\n",
              "      <td>0.558083</td>\n",
              "      <td>20.985900</td>\n",
              "      <td>3.716800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.796371</td>\n",
              "      <td>0.699350</td>\n",
              "      <td>0.285755</td>\n",
              "      <td>20.905300</td>\n",
              "      <td>3.731100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.804435</td>\n",
              "      <td>0.674905</td>\n",
              "      <td>0.214032</td>\n",
              "      <td>20.657800</td>\n",
              "      <td>3.775800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.966**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.804**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l43AbZbd0tIu"
      },
      "source": [
        "The accuracy of our model has increased by adding the skip connections. We were able to improve our accuracy in a fewer epochs compared to the previous model.\n",
        "\n",
        "If we had trained more I am not sure sure if the model performance would have increased on the validation data. This is because the model is overfitting on the training data and we do not want that. What we want is, that the model performs well in unseen data (or the validation dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQBcyfSoMNpR"
      },
      "source": [
        "> Note: At this point it is crearly evident that currently our model is overfitting on the training data. But we will work on improving overfitting. First I want to try different model architectures and find the most optimal after which I will use *regularization* techniques to further improve the model and make it more robust."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcw2YCZullQu"
      },
      "source": [
        "try:\n",
        "    del model, trainer, task\n",
        "except: \n",
        "    pass\n",
        "try:\n",
        "    del train_dl, valid_dl\n",
        "except: \n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFNrOLQ2nRgF"
      },
      "source": [
        "### A State-of-the-Art ResNets -\n",
        "> In this part I will mainly try to explore different variant of `ResNet`s called `ResNet-D` proposed by Tong He in 2014 in the article \"[Bag of Tricks for Image Classification with Convolutional Neural Networks](https://arxiv.org/abs/1812.01187)\n",
        "\". By using a tweaked ResNet-50 architecture and Mixup they achieved 94.6% top-5 accuracy on ImageNet, in comparison to 92.2% with a regular ResNet-50 without Mixup. This result is better than that achieved by regular ResNet models that are twice as deep (and twice as slow, and much more likely to overfit). These tweaked `Resnet` variants was ultimately popularized Jeremy Howard of Fast.ai and are called as `xResNet`'s.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BPqJNZRnT9K"
      },
      "source": [
        "This experiment will mainly explore the model preformace we replace the Residual blocks with xResidual Blocks.\n",
        "\n",
        "First, let us explore the architecture of `ResNet-D` proposed in the above named article. To obtain the `xResNet` architecture we have to apply three different tweaks in the `ResNet` architecture namely `ResNet-B`, `ResNet-C` and `ResNet-D`. \n",
        "\n",
        "The notable changes in the model architecture in our case would be :\n",
        "\n",
        "* In `ResidualCnn`, `resblock2` we are downsampling the input by applying a convulation `stride=2` in the 1st layer of the residual block. `ResNet-B` simply moves the stride `2` to the second convolution and keeps a stride of `1` for the first layer .\n",
        "* The `ResNet-C`, proposed in Inception-v2, removes the `7x7` convolution in the input stem of the network and replaces it with three consecutive 3x3 convolutions. \n",
        "* In `ResNet-D`, the authors replaced the convolution in the downsampling block with a `2x2` average-pooling layer of stride `2` followed by a `1x1` convolution layer. In our case we would have to replace the `idconv` present in `ResidualBlock` module.\n",
        "\n",
        "Let's apply the above changes to our model test the preformance -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjX2mDHonZwy"
      },
      "source": [
        "class xResBlock(nn.Module):\n",
        "    \"Creates a simple Residual Block for xResNet architecture\"\n",
        "    def __init__(self, in_chans, out_chans, kernel_size, stride=1, act_cls=nn.ReLU):\n",
        "        super(xResBlock, self).__init__()\n",
        "        self.idconv = None\n",
        "        \n",
        "        self.block1 = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=kernel_size,\n",
        "                                      stride=1, use_bn=True, act_cls=act_cls, bias=False)\n",
        "        \n",
        "        # we apply the 1st change here, \n",
        "        # moving the stride 2 to the second convolution and keeps a stride of 1 for the first layer . \n",
        "        self.block2 = ConvBnDropBlock(in_chans=out_chans, out_chans=out_chans, kernel_size=kernel_size, \n",
        "                                      stride=stride, use_bn=True, act_cls=None, bias=False)\n",
        "        \n",
        "        self.act_cls = act_cls(inplace=True)\n",
        "        \n",
        "        if in_chans != out_chans or stride != 1:\n",
        "            # the 3rd change proped above is applied here,\n",
        "            # we replace with a 2x2 average-pooling layer of stride 2 followed by a 1x1 convolution layer\n",
        "            pool_layer = nn.AvgPool2d(stride, ceil_mode=True)\n",
        "            conv_layer = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=1, padding=0, \n",
        "                                         use_bn=True, act_cls=None, p_drop=0.0, bias=False, stride=1)\n",
        "            \n",
        "            self.idconv = nn.Sequential(pool_layer, conv_layer)\n",
        "            \n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "         \n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        for m in self.block2.modules():\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 0)\n",
        "                    \n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.block1(x)\n",
        "        out = self.block2(out)\n",
        "        \n",
        "        if self.idconv is not None:\n",
        "            identity = self.idconv(x)\n",
        "        \n",
        "        out += identity\n",
        "        return self.act_cls(out)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xahTgOcNne14"
      },
      "source": [
        "The code is, for the most part, taken & modified from the [fast.ai course](https://www.fast.ai/), [fast.ai]() library and this [blog post](https://towardsdatascience.com/xresnet-from-scratch-in-pytorch-e64e309af722). \n",
        "\n",
        "Let's move incorporate this into the model that we have been working with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQVrmb4Dnj4v"
      },
      "source": [
        "class xResModel(nn.Sequential):\n",
        "    def __init__(self, num_outputs: int, act_cls=nn.ReLU):\n",
        "        conv = nn.Sequential(\n",
        "            ConvBnDropBlock(in_chans=3,  out_chans=32, kernel_size=3, stride=2, act_cls=act_cls, \n",
        "                            bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=32, kernel_size=3, stride=1, act_cls=act_cls, \n",
        "                            bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=64, kernel_size=3, stride=1, act_cls=act_cls,\n",
        "                            bias=False, use_bn=False),\n",
        "        )\n",
        "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        conv_stem  = nn.Sequential(conv, pool)\n",
        "        block1 = xResBlock(in_chans=64, out_chans=64,  stride=1, kernel_size=3, act_cls=act_cls)\n",
        "        block2 = xResBlock(in_chans=64, out_chans=128, stride=2, kernel_size=3, act_cls=act_cls)\n",
        "        block3 = xResBlock(in_chans=128,out_chans=256, stride=2, kernel_size=3, act_cls=act_cls)\n",
        "        pool_flatten = nn.Sequential(OrderedDict(pool=nn.AdaptiveAvgPool2d(1), flatten=nn.Flatten()))\n",
        "        fc = nn.Sequential(nn.Dropout(0.25), nn.Linear(256,num_outputs))\n",
        "        layers = OrderedDict(stem=conv_stem, block1=block1, block2=block2, block3=block3, pool_flatten=pool_flatten, fc=fc)\n",
        "        super(xResModel, self).__init__(layers)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t99NKFfsL6D"
      },
      "source": [
        "Let's train it and see how it fares compared to the previous model -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJYIBu7Bo95m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "outputId": "4df2c12b-cd38-4681-e1c3-5f61af36b753"
      },
      "source": [
        "exp_name = \"stage-01-xresblock\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms, bs=32)\n",
        "\n",
        "# instantiate the model\n",
        "model = xResModel(num_outputs=len(CLASS_MAP))\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=8, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | xResModel        | 1.3 M \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "1.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.3 M     Total params\n",
            "5.070     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='496' max='496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [496/496 02:43, Epoch 7 {'loss': '0.306', 'v_num': 3}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.058468</td>\n",
              "      <td>4.058199</td>\n",
              "      <td>4.023277</td>\n",
              "      <td>20.412100</td>\n",
              "      <td>3.821300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.038306</td>\n",
              "      <td>6.825205</td>\n",
              "      <td>3.064571</td>\n",
              "      <td>20.569600</td>\n",
              "      <td>3.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.088710</td>\n",
              "      <td>5.207982</td>\n",
              "      <td>2.101307</td>\n",
              "      <td>20.418200</td>\n",
              "      <td>3.820100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.651210</td>\n",
              "      <td>1.246989</td>\n",
              "      <td>0.842765</td>\n",
              "      <td>20.318000</td>\n",
              "      <td>3.839000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.762097</td>\n",
              "      <td>0.838247</td>\n",
              "      <td>0.779555</td>\n",
              "      <td>20.463000</td>\n",
              "      <td>3.811800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.810484</td>\n",
              "      <td>0.615017</td>\n",
              "      <td>0.411811</td>\n",
              "      <td>20.377400</td>\n",
              "      <td>3.827800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.860887</td>\n",
              "      <td>0.465593</td>\n",
              "      <td>0.346306</td>\n",
              "      <td>20.546000</td>\n",
              "      <td>3.796400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.870968</td>\n",
              "      <td>0.428967</td>\n",
              "      <td>0.227218</td>\n",
              "      <td>20.365300</td>\n",
              "      <td>3.830000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.951**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.871**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFfabjLSUrQy"
      },
      "source": [
        "Validation accuracy has increased a lot but we are still overfitting on the training data , let's explore more ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAOfe5FRtVtf"
      },
      "source": [
        "try:\n",
        "    del model, trainer, task\n",
        "except: \n",
        "    pass\n",
        "try:\n",
        "    del train_dl, valid_dl\n",
        "except: \n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bosj5r96zCeG"
      },
      "source": [
        "### Mish and SelfAttention - \n",
        "> In this experiment I plan to use some of [State of the Art Training techniques](https://forums.fast.ai/t/how-we-beat-the-5-epoch-imagewoof-leaderboard-score-some-new-techniques-to-consider/53453) that were used to top the leaderboard score for 5 epoch Imagewoof`.\n",
        "\n",
        "\n",
        "\n",
        "The techniques I will be employing are as follows - \n",
        "* `Mish activation function instead of ReLU` : Mish is a new activation function that was released in a [paper](https://arxiv.org/abs/1908.08681). It has a much smoother curve vs relu, and in theory, that drives information more deeply through the network.\n",
        "* `Self attention layer` : Bringing in Ideas from GAN's into CNN's.The self attention layer is designed to help leverage long range dependencies within an image vs the more local feature focus of convolutions. Paper Link : [https://arxiv.org/abs/1805.08318]\n",
        "\n",
        "\n",
        "\n",
        "The folks from fast.ai who employed the above techniques noticed that `SimpleSelfAttention` layer, when placed within a Resnet block, the network converges if `SimpleSelfAttention` is placed right after a convolution layer that uses batch norm, and initializes the batchnorm weights to 0.\n",
        "\n",
        "[Source](https://github.com/sdoria/SimpleSelfAttention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ElXz7VPzCg2"
      },
      "source": [
        "The code for creating the `SimpleSelfAttention` module has been borrowed from [here](https://github.com/sdoria/SimpleSelfAttention/blob/master/xresnet.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLHtIG9RzCmC"
      },
      "source": [
        "def conv1d(ni:int, no:int, ks:int=1, stride:int=1, padding:int=0, bias:bool=False):\n",
        "    \"Create and initialize a `nn.Conv1d` layer with spectral normalization.\"\n",
        "    conv = nn.Conv1d(ni, no, ks, stride=stride, padding=padding, bias=bias)\n",
        "    nn.init.kaiming_normal_(conv.weight)\n",
        "    if bias: conv.bias.data.zero_()\n",
        "    return spectral_norm(conv)\n",
        "\n",
        "\n",
        "class SimpleSelfAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_in:int, ks=1, sym=False):\n",
        "        super().__init__()\n",
        "           \n",
        "        self.conv = conv1d(n_in, n_in, ks, padding=ks//2, bias=False)      \n",
        "       \n",
        "        self.gamma = nn.Parameter(torch.tensor([0.]))\n",
        "        \n",
        "        self.sym = sym\n",
        "        self.n_in = n_in\n",
        "        \n",
        "    def forward(self,x):\n",
        "          \n",
        "        if self.sym:\n",
        "            # symmetry hack by https://github.com/mgrankin\n",
        "            c = self.conv.weight.view(self.n_in,self.n_in)\n",
        "            c = (c + c.t())/2\n",
        "            self.conv.weight = c.view(self.n_in,self.n_in,1)\n",
        "                \n",
        "        size = x.size()  \n",
        "        x = x.view(*size[:2],-1)\n",
        "        \n",
        "        convx = self.conv(x)\n",
        "        xxT = torch.bmm(x,x.permute(0,2,1).contiguous())\n",
        "        \n",
        "        o = torch.bmm(xxT, convx)\n",
        "          \n",
        "        o = self.gamma * o + x\n",
        "        return o.view(*size).contiguous()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rCMGxmSzCoi"
      },
      "source": [
        "The code block implements the `Mish` Activation function, the code for which has been borrowed from the [repository](https://github.com/digantamisra98/Mish/blob/master/Mish/Torch/functional.py) of the original author of `Mish`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv_KzkeLzYVI"
      },
      "source": [
        "@torch.jit.script\n",
        "def mish(input):\n",
        "    '''\n",
        "    Applies the mish function element-wise:\n",
        "    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n",
        "    '''\n",
        "    return input * torch.tanh(F.softplus(input))\n",
        "\n",
        "\n",
        "class Mish(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(Mish, self).__init__()\n",
        "    \n",
        "    def forward(self, xb):\n",
        "        return mish(xb)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx6N4e0WzYYS"
      },
      "source": [
        "The code below adds SelfAttention and Mish activation to our *x*ResNet based model architecture -\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9sspbJozYbG"
      },
      "source": [
        "class xResBlock(nn.Module):\n",
        "    \"Creates a xRes Block with SelfAttention and Mish Activation\"\n",
        "    def __init__(self, in_chans: int, out_chans: int, kernel_size: int, stride: int = 1, \n",
        "                 act_cls: Callable = Mish, sa: bool = False):\n",
        "        \n",
        "        super(xResBlock, self).__init__()\n",
        "        self.idconv = None\n",
        "        self.sa = None\n",
        "        \n",
        "        self.block1 = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=kernel_size,\n",
        "                                      stride=1, use_bn=True, act_cls=act_cls, bias=False)\n",
        "         \n",
        "        self.block2 = ConvBnDropBlock(in_chans=out_chans, out_chans=out_chans, kernel_size=kernel_size, \n",
        "                                      stride=stride, use_bn=True, act_cls=None, bias=False)\n",
        "        \n",
        "        # initialize the SimpleSelfAttention Layer\n",
        "        if sa :\n",
        "            self.sa = SimpleSelfAttention(out_chans,ks=1,sym=False)    \n",
        "        \n",
        "        self.act_cls = act_cls(inplace=True)\n",
        "        \n",
        "        if in_chans != out_chans or stride != 1:\n",
        "            pool_layer = nn.AvgPool2d(stride, ceil_mode=True)\n",
        "            conv_layer = ConvBnDropBlock(in_chans=in_chans, out_chans=out_chans, kernel_size=1, padding=0, \n",
        "                                         use_bn=True, act_cls=None, p_drop=0.0, bias=False, stride=1)\n",
        "            \n",
        "            self.idconv = nn.Sequential(pool_layer, conv_layer)       \n",
        "            \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "                \n",
        "        for m in self.block2.modules():\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 0)\n",
        "                    \n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.block1(x)\n",
        "        out = self.block2(out)\n",
        "        if self.sa is not None:\n",
        "            out = self.sa(out) # <- self attention is inserted here\n",
        "        \n",
        "        if self.idconv is not None:\n",
        "            identity = self.idconv(x)\n",
        "        \n",
        "        out += identity\n",
        "        return self.act_cls(out)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TizKT22OHRbU"
      },
      "source": [
        "Let's build up the model : The code below incorporates SelfAttention and Mish into our xResnet based model ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGoJKgBZzmVi"
      },
      "source": [
        "class sa_m_xResModel(nn.Sequential):\n",
        "    def __init__(self, num_outputs: int, act_cls=Mish):\n",
        "        conv = nn.Sequential(\n",
        "            ConvBnDropBlock(in_chans=3,  out_chans=32, kernel_size=3, stride=2, \n",
        "                            act_cls=act_cls, bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=32, kernel_size=3, stride=1, \n",
        "                            act_cls=act_cls, bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=64, kernel_size=3, stride=1, \n",
        "                            act_cls=act_cls, bias=False, use_bn=False),\n",
        "            )\n",
        "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        conv_stem  = nn.Sequential(conv, pool)    \n",
        "        block1 = xResBlock(in_chans=64, out_chans=64,  stride=1, kernel_size=3, act_cls=act_cls, sa=True)\n",
        "        block2 = xResBlock(in_chans=64, out_chans=128, stride=2, kernel_size=3, act_cls=act_cls, sa=True)\n",
        "        block3 = xResBlock(in_chans=128,out_chans=256, stride=2, kernel_size=3, act_cls=act_cls, sa=True)\n",
        "        pool_flatten = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
        "        fc = nn.Sequential(nn.Dropout(0.25), nn.Linear(256, num_outputs))\n",
        "        layers = OrderedDict(stem=conv_stem, block1=block1, block2=block2, block3=block3, pool_flatten=pool_flatten, fc=fc)\n",
        "        super(sa_m_xResModel, self).__init__(layers)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teBv5VmoZW2p"
      },
      "source": [
        "Train the model -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR6qi9tuzmY1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        },
        "outputId": "e2b199be-f1ab-46d9-de13-f9a12494da1f"
      },
      "source": [
        "exp_name = \"stage-01-xresblock-mish-sa\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "# instantiate the model\n",
        "model = sa_m_xResModel(num_outputs=len(CLASS_MAP))\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=8, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | sa_m_xResModel   | 1.4 M \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.415     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='481' max='496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [481/496 02:45 < 00:05, 2.90 it/s, Epoch 7 {'loss': '0.597', 'v_num': 4}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.022177</td>\n",
              "      <td>4.061556</td>\n",
              "      <td>3.827118</td>\n",
              "      <td>25.001000</td>\n",
              "      <td>3.119900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>20.492125</td>\n",
              "      <td>2.932078</td>\n",
              "      <td>21.945600</td>\n",
              "      <td>3.554200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.040323</td>\n",
              "      <td>15.276151</td>\n",
              "      <td>2.335787</td>\n",
              "      <td>21.686400</td>\n",
              "      <td>3.596700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.048387</td>\n",
              "      <td>13.974190</td>\n",
              "      <td>1.613616</td>\n",
              "      <td>21.836300</td>\n",
              "      <td>3.572000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.159274</td>\n",
              "      <td>3.904216</td>\n",
              "      <td>1.179556</td>\n",
              "      <td>21.728900</td>\n",
              "      <td>3.589700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.703629</td>\n",
              "      <td>0.985508</td>\n",
              "      <td>0.821851</td>\n",
              "      <td>21.676700</td>\n",
              "      <td>3.598300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.788306</td>\n",
              "      <td>0.726567</td>\n",
              "      <td>0.685594</td>\n",
              "      <td>21.504900</td>\n",
              "      <td>3.627100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='496' max='496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [496/496 02:53, Epoch 7 {'loss': '0.566', 'v_num': 4}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.022177</td>\n",
              "      <td>4.061556</td>\n",
              "      <td>3.827118</td>\n",
              "      <td>25.001000</td>\n",
              "      <td>3.119900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>20.492125</td>\n",
              "      <td>2.932078</td>\n",
              "      <td>21.945600</td>\n",
              "      <td>3.554200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.040323</td>\n",
              "      <td>15.276151</td>\n",
              "      <td>2.335787</td>\n",
              "      <td>21.686400</td>\n",
              "      <td>3.596700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.048387</td>\n",
              "      <td>13.974190</td>\n",
              "      <td>1.613616</td>\n",
              "      <td>21.836300</td>\n",
              "      <td>3.572000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.159274</td>\n",
              "      <td>3.904216</td>\n",
              "      <td>1.179556</td>\n",
              "      <td>21.728900</td>\n",
              "      <td>3.589700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.703629</td>\n",
              "      <td>0.985508</td>\n",
              "      <td>0.821851</td>\n",
              "      <td>21.676700</td>\n",
              "      <td>3.598300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.788306</td>\n",
              "      <td>0.726567</td>\n",
              "      <td>0.685594</td>\n",
              "      <td>21.504900</td>\n",
              "      <td>3.627100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.798387</td>\n",
              "      <td>0.678912</td>\n",
              "      <td>0.705633</td>\n",
              "      <td>21.668100</td>\n",
              "      <td>3.599800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.881**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.798**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs_KKiewstlB"
      },
      "source": [
        "As loss still decreasing , let's train for more epochs ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "U5s_XsTHs1Xl",
        "outputId": "d91a03a5-d922-4b9d-b433-31f2746cd5ba"
      },
      "source": [
        "exp_name = \"stage-02-xresblock-mish-sa\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "# instantiate the model\n",
        "model = sa_m_xResModel(num_outputs=len(CLASS_MAP))\n",
        "\n",
        "# Load the model from checkpoint\n",
        "PATH = \"stage-01-xresblock-mish-sa.ckpt\"\n",
        "task = ClassificationTask.load_from_checkpoint(PATH, model=model, lr=3e-05)\n",
        "task.unfreeze()\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=5, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | sa_m_xResModel   | 1.4 M \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.415     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [310/310 01:43, Epoch 4 {'loss': '0.549', 'v_num': 6}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.802419</td>\n",
              "      <td>0.671502</td>\n",
              "      <td>0.389975</td>\n",
              "      <td>20.914700</td>\n",
              "      <td>3.729400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.804435</td>\n",
              "      <td>0.668781</td>\n",
              "      <td>0.567810</td>\n",
              "      <td>20.834500</td>\n",
              "      <td>3.743800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.804435</td>\n",
              "      <td>0.660199</td>\n",
              "      <td>0.611202</td>\n",
              "      <td>20.819300</td>\n",
              "      <td>3.746500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.810484</td>\n",
              "      <td>0.656655</td>\n",
              "      <td>0.697178</td>\n",
              "      <td>20.805100</td>\n",
              "      <td>3.749100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.804435</td>\n",
              "      <td>0.660329</td>\n",
              "      <td>0.688981</td>\n",
              "      <td>20.710400</td>\n",
              "      <td>3.766200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.889**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.81**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIibTevdzmfR"
      },
      "source": [
        "try:\n",
        "    del model, trainer, task\n",
        "except:\n",
        "    pass\n",
        "try:\n",
        "    del train_dl, valid_dl\n",
        "except: \n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXXwGwHHwCHQ"
      },
      "source": [
        "Let's check with only Mish activation and no SelfAttention - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "R3-hr5QWwLXS",
        "outputId": "71d580be-d610-4ab7-fab0-ddb047bdce15"
      },
      "source": [
        "exp_name = \"stage-01-Mxresblock\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms, bs=32)\n",
        "\n",
        "# instantiate the model\n",
        "model = xResModel(num_outputs=len(CLASS_MAP), act_cls=Mish)\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=8, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | xResModel        | 1.3 M \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "1.3 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.3 M     Total params\n",
            "5.070     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='496' max='496' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [496/496 02:45, Epoch 7 {'loss': '0.264', 'v_num': 7}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.018145</td>\n",
              "      <td>4.095335</td>\n",
              "      <td>3.954535</td>\n",
              "      <td>21.150800</td>\n",
              "      <td>3.687800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.020161</td>\n",
              "      <td>18.538307</td>\n",
              "      <td>2.794209</td>\n",
              "      <td>21.041200</td>\n",
              "      <td>3.707000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>6.657030</td>\n",
              "      <td>1.548329</td>\n",
              "      <td>20.763000</td>\n",
              "      <td>3.756700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.387097</td>\n",
              "      <td>2.125322</td>\n",
              "      <td>1.062794</td>\n",
              "      <td>20.803000</td>\n",
              "      <td>3.749500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.096774</td>\n",
              "      <td>6.921845</td>\n",
              "      <td>0.591794</td>\n",
              "      <td>20.831400</td>\n",
              "      <td>3.744300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.538306</td>\n",
              "      <td>1.564277</td>\n",
              "      <td>0.421629</td>\n",
              "      <td>20.679400</td>\n",
              "      <td>3.771900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.870968</td>\n",
              "      <td>0.487614</td>\n",
              "      <td>0.335759</td>\n",
              "      <td>20.812000</td>\n",
              "      <td>3.747800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.870968</td>\n",
              "      <td>0.457603</td>\n",
              "      <td>0.217565</td>\n",
              "      <td>20.816200</td>\n",
              "      <td>3.747100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.953**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.871**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj5UCHYawLbk"
      },
      "source": [
        "try:\n",
        "    del model, trainer, task\n",
        "except:\n",
        "    pass\n",
        "try:\n",
        "    del train_dl, valid_dl\n",
        "except: \n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP7Nflj-Y5mu"
      },
      "source": [
        "We do see much improved using the above tricks so we will discard them and work on improving our original *x*Resnet based model ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIaNPUnsPwNZ"
      },
      "source": [
        "### Exploring Different Classifiers - \n",
        "\n",
        "In this section, I will take the above model and update the classifiers of the model and check their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhHFKDSKQOED"
      },
      "source": [
        "Let's add another block of layers (Dropout -> Linear -> Activation -> BatchNorm) after the `pooling layer` and the `final fc layer`  to the classifier of the model - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxj7575oQOMU"
      },
      "source": [
        "class xResModelv2(nn.Sequential):\n",
        "    def __init__(self, num_outputs: int, act_cls=nn.ReLU, sa=False):\n",
        "        conv = nn.Sequential(\n",
        "            ConvBnDropBlock(in_chans=3,  out_chans=32, kernel_size=3, stride=2, act_cls=act_cls, bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=32, kernel_size=3, stride=1, act_cls=act_cls, bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=64, kernel_size=3, stride=1, act_cls=act_cls, bias=False, use_bn=True),\n",
        "            )\n",
        "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        conv_stem  = nn.Sequential(conv, pool)     \n",
        "        block1 = xResBlock(in_chans=64, out_chans=64,  stride=1, kernel_size=3, act_cls=act_cls, sa=sa)\n",
        "        block2 = xResBlock(in_chans=64, out_chans=128, stride=2, kernel_size=3, act_cls=act_cls, sa=sa)\n",
        "        block3 = xResBlock(in_chans=128,out_chans=256, stride=2, kernel_size=3, act_cls=act_cls, sa=sa)\n",
        "        pool_flatten = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
        "        fc1 = nn.Sequential(nn.Dropout(0.25), nn.Linear(256, 512, bias=False), act_cls(inplace=True), nn.BatchNorm1d(512))\n",
        "        fc2 = nn.Sequential(nn.Dropout(0.5), nn.Linear(512, num_outputs, bias=False)) \n",
        "        layers = OrderedDict(stem=conv_stem, block1=block1, block2=block2, block3=block3, pool_flatten=pool_flatten, fc1=fc1, fc2=fc2)\n",
        "        super(xResModelv2, self).__init__(layers)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNDZOkCsQOVN"
      },
      "source": [
        "Training the model to evaluate performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d9q87zmQOcz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "outputId": "69c62779-f39a-4399-fb09-022c6c5bbabd"
      },
      "source": [
        "exp_name = \"stage-01-xresblockv2\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "# instantiate the model\n",
        "model = xResModelv2(num_outputs=len(CLASS_MAP), act_cls=nn.ReLU, sa=False)\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | xResModelv2      | 1.4 M \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.663     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [620/620 03:27, Epoch 9 {'loss': '0.265', 'v_num': 8}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>5.038570</td>\n",
              "      <td>4.086416</td>\n",
              "      <td>20.705600</td>\n",
              "      <td>3.767100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>3.903433</td>\n",
              "      <td>3.173642</td>\n",
              "      <td>20.594900</td>\n",
              "      <td>3.787300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.072581</td>\n",
              "      <td>7.380940</td>\n",
              "      <td>2.370653</td>\n",
              "      <td>20.853100</td>\n",
              "      <td>3.740400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.417339</td>\n",
              "      <td>2.091654</td>\n",
              "      <td>1.421726</td>\n",
              "      <td>20.571000</td>\n",
              "      <td>3.791700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.655242</td>\n",
              "      <td>1.220579</td>\n",
              "      <td>1.190635</td>\n",
              "      <td>20.618400</td>\n",
              "      <td>3.783000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.661290</td>\n",
              "      <td>1.148891</td>\n",
              "      <td>0.483984</td>\n",
              "      <td>20.917800</td>\n",
              "      <td>3.728900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.842742</td>\n",
              "      <td>0.507935</td>\n",
              "      <td>0.334965</td>\n",
              "      <td>20.883100</td>\n",
              "      <td>3.735100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.854839</td>\n",
              "      <td>0.485718</td>\n",
              "      <td>0.177109</td>\n",
              "      <td>20.675500</td>\n",
              "      <td>3.772600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.864919</td>\n",
              "      <td>0.434257</td>\n",
              "      <td>0.453302</td>\n",
              "      <td>20.634100</td>\n",
              "      <td>3.780200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.872984</td>\n",
              "      <td>0.428266</td>\n",
              "      <td>0.171896</td>\n",
              "      <td>20.725600</td>\n",
              "      <td>3.763500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.971**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.873**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlvijK6rQOpQ"
      },
      "source": [
        "This time I will replace the classifier and pool layer of the Model with the model head that is created from by `fast.ai` s cnn_learner. The head begins with fastai's AdaptiveConcatPool2d. Then it uses a Flatten layer before going on blocks of BatchNorm, Dropout and Linear layers.\n",
        "\n",
        "[Source](https://docs.fast.ai/vision.learner.html#create_body)\n",
        "\n",
        "Let's build & train the model after adding these tweaks -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHPhIMGQauEY"
      },
      "source": [
        "class AdaptiveConcatPool2d(nn.Module):\n",
        "    \"Layer that concats `AdaptiveAvgPool2d` and `AdaptiveMaxPool2d`\"\n",
        "    def __init__(self, size=None):\n",
        "        super(AdaptiveConcatPool2d, self).__init__()\n",
        "        self.size = size or 1\n",
        "        self.ap = nn.AdaptiveAvgPool2d(self.size)\n",
        "        self.mp = nn.AdaptiveMaxPool2d(self.size)\n",
        "    def forward(self, x): \n",
        "        return torch.cat([self.mp(x), self.ap(x)], 1)\n",
        "\n",
        "\n",
        "class FastaiHead(nn.Sequential):\n",
        "    def __init__(self, nf: int, num_outputs: int, hiddens: int = 512, act_cls=nn.ReLU):\n",
        "        l1 = nn.BatchNorm1d(nf)\n",
        "        l2 = nn.Dropout(p=0.25)\n",
        "        l3 = nn.Linear(in_features=nf, out_features=hiddens, bias=False)\n",
        "        l4 = act_cls(inplace=True)\n",
        "        l5 = nn.BatchNorm1d(hiddens)\n",
        "        l6 = nn.Dropout(p=0.5)\n",
        "        fc = nn.Linear(in_features=hiddens, out_features=num_outputs, bias=False)\n",
        "\n",
        "        layers = OrderedDict(fc1=nn.Sequential(l1, l2, l3, l4, l5, l6), fc2=fc)\n",
        "        super(FastaiHead, self).__init__(layers)\n",
        "\n",
        "\n",
        "# Incorporate these into our xResNet based model :\n",
        "class xResModelv3(nn.Sequential):\n",
        "    def __init__(self, num_outputs: int, act_cls=nn.ReLU, sa=False):\n",
        "        conv = nn.Sequential(\n",
        "            ConvBnDropBlock(in_chans=3,  out_chans=32, kernel_size=3, stride=2, \n",
        "                            act_cls=act_cls, bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=32, kernel_size=3, stride=1, \n",
        "                            act_cls=act_cls, bias=False, use_bn=False),\n",
        "            ConvBnDropBlock(in_chans=32, out_chans=64, kernel_size=3, stride=1, \n",
        "                            act_cls=act_cls, bias=False, use_bn=False),\n",
        "            )\n",
        "        pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        conv_stem  = nn.Sequential(conv, pool)     \n",
        "        block1 = xResBlock(in_chans=64, out_chans=64,  stride=1, kernel_size=3, act_cls=act_cls, sa=sa)\n",
        "        block2 = xResBlock(in_chans=64, out_chans=128, stride=2, kernel_size=3, act_cls=act_cls, sa=sa)\n",
        "        block3 = xResBlock(in_chans=128,out_chans=256, stride=2, kernel_size=3, act_cls=act_cls, sa=sa)\n",
        "        pool = nn.Sequential(AdaptiveConcatPool2d(1), nn.Flatten())\n",
        "        fc = FastaiHead(nf=2*256, num_outputs=num_outputs, act_cls=act_cls, hiddens=256)\n",
        "        layers = OrderedDict(stem=conv_stem, block1=block1, block2=block2, block3=block3, pool_flatten=pool, fc=fc)\n",
        "        super(xResModelv3, self).__init__(layers)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "id": "elv7lwyL0UfJ",
        "outputId": "d85b88c8-5494-45f9-c40c-31c93ed7212f"
      },
      "source": [
        "exp_name = \"stage-01-xresblockv3\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "# instantiate the model\n",
        "model = xResModelv3(num_outputs=len(CLASS_MAP), act_cls=nn.ReLU, sa=False)\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | xResModelv3      | 1.4 M \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.601     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [620/620 03:26, Epoch 9 {'loss': '0.261', 'v_num': 10}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>6.530828</td>\n",
              "      <td>3.954202</td>\n",
              "      <td>20.482000</td>\n",
              "      <td>3.808200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.030242</td>\n",
              "      <td>15.868407</td>\n",
              "      <td>2.806798</td>\n",
              "      <td>20.512200</td>\n",
              "      <td>3.802600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>9.161619</td>\n",
              "      <td>1.817714</td>\n",
              "      <td>20.442900</td>\n",
              "      <td>3.815500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.056452</td>\n",
              "      <td>20.170443</td>\n",
              "      <td>1.167696</td>\n",
              "      <td>20.895000</td>\n",
              "      <td>3.732900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.052419</td>\n",
              "      <td>6.413023</td>\n",
              "      <td>0.946135</td>\n",
              "      <td>20.629900</td>\n",
              "      <td>3.780900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.427419</td>\n",
              "      <td>2.351232</td>\n",
              "      <td>0.877230</td>\n",
              "      <td>20.677300</td>\n",
              "      <td>3.772300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.455645</td>\n",
              "      <td>1.880538</td>\n",
              "      <td>0.710208</td>\n",
              "      <td>20.754500</td>\n",
              "      <td>3.758200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.743952</td>\n",
              "      <td>0.800156</td>\n",
              "      <td>0.282973</td>\n",
              "      <td>20.648500</td>\n",
              "      <td>3.777500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.850806</td>\n",
              "      <td>0.472166</td>\n",
              "      <td>0.306539</td>\n",
              "      <td>20.593800</td>\n",
              "      <td>3.787600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.866935</td>\n",
              "      <td>0.455058</td>\n",
              "      <td>0.238443</td>\n",
              "      <td>20.821800</td>\n",
              "      <td>3.746100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.974**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.867**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxZ16TND0PdC"
      },
      "source": [
        "try:\n",
        "    del model, trainer, task\n",
        "except:\n",
        "    pass\n",
        "try:\n",
        "    del train_dl, valid_dl\n",
        "except: \n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ3rsDtRz9h0"
      },
      "source": [
        "The previous classifier performs better so let's take that and incorporate Mish + SelfAttention in that model and check for performace ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "id": "zCriQSVBz_T1",
        "outputId": "aadd5377-3e0d-494f-ba9b-fb120677f3b2"
      },
      "source": [
        "exp_name = \"stage-02-xresblockv2\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "# instantiate the model\n",
        "model = xResModelv2(num_outputs=len(CLASS_MAP), act_cls=Mish, sa=True)\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=12, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | xResModelv2      | 1.5 M \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "1.5 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.5 M     Total params\n",
            "6.007     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='744' max='744' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [744/744 04:13, Epoch 11 {'loss': '0.104', 'v_num': 11}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.020161</td>\n",
              "      <td>6.012567</td>\n",
              "      <td>3.646578</td>\n",
              "      <td>21.022800</td>\n",
              "      <td>3.710300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.070565</td>\n",
              "      <td>8.797585</td>\n",
              "      <td>2.639690</td>\n",
              "      <td>20.994700</td>\n",
              "      <td>3.715200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.024194</td>\n",
              "      <td>16.169146</td>\n",
              "      <td>1.999533</td>\n",
              "      <td>20.847000</td>\n",
              "      <td>3.741500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.028226</td>\n",
              "      <td>69.084694</td>\n",
              "      <td>0.998649</td>\n",
              "      <td>20.896700</td>\n",
              "      <td>3.732700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.056452</td>\n",
              "      <td>29.266550</td>\n",
              "      <td>0.894572</td>\n",
              "      <td>20.868700</td>\n",
              "      <td>3.737600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.151210</td>\n",
              "      <td>36.136978</td>\n",
              "      <td>1.109990</td>\n",
              "      <td>21.384400</td>\n",
              "      <td>3.647500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.066532</td>\n",
              "      <td>19.022072</td>\n",
              "      <td>0.563144</td>\n",
              "      <td>21.754800</td>\n",
              "      <td>3.585400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.717742</td>\n",
              "      <td>0.954129</td>\n",
              "      <td>0.224662</td>\n",
              "      <td>21.331800</td>\n",
              "      <td>3.656500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.768145</td>\n",
              "      <td>0.821934</td>\n",
              "      <td>0.354794</td>\n",
              "      <td>20.918500</td>\n",
              "      <td>3.728800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.836694</td>\n",
              "      <td>0.521264</td>\n",
              "      <td>0.124756</td>\n",
              "      <td>20.973700</td>\n",
              "      <td>3.718900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.852823</td>\n",
              "      <td>0.456191</td>\n",
              "      <td>0.160299</td>\n",
              "      <td>21.037200</td>\n",
              "      <td>3.707700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.856855</td>\n",
              "      <td>0.438399</td>\n",
              "      <td>0.164595</td>\n",
              "      <td>20.920500</td>\n",
              "      <td>3.728400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.991**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.857**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxn2WDQpz_bq"
      },
      "source": [
        "Let's try with only Mish activation -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "id": "-r4SVgn01Vg7",
        "outputId": "b07c2648-1cb1-451e-9ff1-73841ba1affb"
      },
      "source": [
        "exp_name = \"stage-03-xresblockv2\"\n",
        "train_dl, valid_dl = get_data(base_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "# instantiate the model\n",
        "model = xResModelv2(num_outputs=len(CLASS_MAP), act_cls=Mish, sa=False)\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=12, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | xResModelv2      | 1.4 M \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.663     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='744' max='744' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [744/744 04:12, Epoch 11 {'loss': '0.126', 'v_num': 12}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>6.430037</td>\n",
              "      <td>3.540694</td>\n",
              "      <td>20.765000</td>\n",
              "      <td>3.756300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.058468</td>\n",
              "      <td>8.318975</td>\n",
              "      <td>2.739634</td>\n",
              "      <td>21.019800</td>\n",
              "      <td>3.710800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.046371</td>\n",
              "      <td>11.670553</td>\n",
              "      <td>1.184826</td>\n",
              "      <td>21.226800</td>\n",
              "      <td>3.674600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.018145</td>\n",
              "      <td>25.858728</td>\n",
              "      <td>1.067604</td>\n",
              "      <td>20.820800</td>\n",
              "      <td>3.746300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.352823</td>\n",
              "      <td>3.574652</td>\n",
              "      <td>1.179317</td>\n",
              "      <td>20.780500</td>\n",
              "      <td>3.753500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.040323</td>\n",
              "      <td>14.726781</td>\n",
              "      <td>0.734755</td>\n",
              "      <td>20.982000</td>\n",
              "      <td>3.717500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.419355</td>\n",
              "      <td>2.501947</td>\n",
              "      <td>0.588100</td>\n",
              "      <td>20.904400</td>\n",
              "      <td>3.731300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.467742</td>\n",
              "      <td>2.379369</td>\n",
              "      <td>0.558247</td>\n",
              "      <td>21.151200</td>\n",
              "      <td>3.687700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.794355</td>\n",
              "      <td>0.640727</td>\n",
              "      <td>0.221576</td>\n",
              "      <td>20.968700</td>\n",
              "      <td>3.719800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.862903</td>\n",
              "      <td>0.462746</td>\n",
              "      <td>0.143309</td>\n",
              "      <td>21.018600</td>\n",
              "      <td>3.711000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.866935</td>\n",
              "      <td>0.418976</td>\n",
              "      <td>0.166169</td>\n",
              "      <td>21.214100</td>\n",
              "      <td>3.676800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.877016</td>\n",
              "      <td>0.409923</td>\n",
              "      <td>0.157784</td>\n",
              "      <td>21.177200</td>\n",
              "      <td>3.683200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.991**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.877**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmhPCWVx110q"
      },
      "source": [
        "try:\n",
        "    del model, trainer, task\n",
        "except:\n",
        "    pass\n",
        "try:\n",
        "    del train_dl, valid_dl\n",
        "except: \n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iQjOe802mUf"
      },
      "source": [
        "Let's the take the above model. To summarize the architecture of our current best model is as follows -\n",
        "- *x*ResNet based feature extractor with Mish activation. 3 *x*ResNet blocks.\n",
        "- classifier consits of : \n",
        "```\n",
        "(pool_flatten): Sequential(\n",
        "    (0): AdaptiveAvgPool2d(output_size=1)\n",
        "    (1): Flatten(start_dim=1, end_dim=-1)\n",
        "  )\n",
        "  (fc1): Sequential(\n",
        "    (0): Dropout(p=0.25, inplace=False)\n",
        "    (1): Linear(in_features=256, out_features=512, bias=False)\n",
        "    (2): Mish()\n",
        "    (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  )\n",
        "  (fc2): Sequential(\n",
        "    (0): Dropout(p=0.5, inplace=False)\n",
        "    (1): Linear(in_features=512, out_features=62, bias=False)\n",
        "  )\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDs5CVKi19Om"
      },
      "source": [
        "### Regularization Techniques - \n",
        "> Reducing overfitting on training data via regularization\n",
        "\n",
        "In this section I will attempt to reduce the overfitting of our model using a common regularizing technique *data augmentation*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD66Vvyw2hiX"
      },
      "source": [
        "What is *data augmentation*? \n",
        "\n",
        "Data augmentation artificially increases the size of the training set by generating many realistic variants of each training instance. Data augmentation is a technique used for introducing variety in training data thereby helping to mitigate overfitting.\n",
        "\n",
        "\n",
        "Let's apply some *data augmentation* data input pipeline. We will only apply data augmentation to the training data. The validation data is not augmented. \n",
        "\n",
        "\n",
        "> Note: For the current task given we have to be very carefull while applying *data augmentation*. Since our Images our images can change depending on the rotation, I won't be applying any `rotation` or `flip` transforms. The only transforms i will be using are intorducing noise and changing the lighting of the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvgnvah64F35"
      },
      "source": [
        "base_tfms = A.Compose([\n",
        "    A.Resize(PRESIZE, PRESIZE, p=1.0),\n",
        "    A.CenterCrop(IMG_SIZE, IMG_SIZE, p=1.0),\n",
        "    A.ToFloat(max_value=255, p=1.0),\n",
        "    ToTensorV2(p=1.0),])\n",
        "\n",
        "aug_tfms = A.Compose([\n",
        "    A.Resize(PRESIZE, PRESIZE, p=1.0),\n",
        "    A.MultiplicativeNoise(multiplier=[0.5, 1.5], per_channel=True, p=0.5),\n",
        "    A.CLAHE(p=0.5),\n",
        "    A.Blur(blur_limit=10, always_apply=False, p=0.5),\n",
        "    A.JpegCompression(quality_lower=0, quality_upper=1, always_apply=False, p=0.5),\n",
        "    A.CenterCrop(IMG_SIZE, IMG_SIZE, p=1.0),\n",
        "    A.Cutout(num_holes=3, always_apply=False, p=0.5),\n",
        "    A.ToFloat(max_value=255, p=1.0),\n",
        "    ToTensorV2(p=1.0),])"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DMqPaqM5IhU"
      },
      "source": [
        "Let's view the augmentations on the Images -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tprO_F9F475O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "599fc6ab-4313-4aee-e8c0-02789a185d11"
      },
      "source": [
        "train_dl, valid_dl = get_data(transforms=aug_tfms, valid_transforms=base_tfms, bs=32)\n",
        "\n",
        "ims, lbls = next(iter(train_dl))\n",
        "ims, lbls = ims[:8], lbls[:8]\n",
        "\n",
        "grid = make_grid(ims, normalize=True).permute(1, 2, 0)\n",
        "fig = plt.figure(figsize=(13, 13))\n",
        "plt.imshow(grid) \n",
        "plt.title([CLASS_MAP[o] for o in lbls.data.cpu().numpy()])\n",
        "plt.axis(\"off\");"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 936x936 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqstDxh55MJ0"
      },
      "source": [
        "Another common technique to reduce overfitting is to increase the *weight_decay*, so let's try it also.\n",
        "\n",
        "Let's train on the dataset and explore the performance. Since this time we are using heavy data augmentations we probably would need to train more to get good results - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujmZorMj5MOd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ab16b54-894d-45c5-c648-3b999cde8d3e"
      },
      "source": [
        "exp_name = \"stage-01-xResModelv2-aug-wd\"\n",
        "train_dl, valid_dl = get_data(transforms=aug_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "# instantiate the model\n",
        "model = xResModelv2(num_outputs=len(CLASS_MAP), act_cls=Mish, sa=False)\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03, wd=1e-01)\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=20, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | model     | xResModelv2      | 1.4 M \n",
            "1 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.663     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='1240' max='1240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1240/1240 07:25, Epoch 19 {'loss': '0.234', 'v_num': 14}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>4.523890</td>\n",
              "      <td>4.230780</td>\n",
              "      <td>22.431600</td>\n",
              "      <td>3.477200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.046371</td>\n",
              "      <td>4.788090</td>\n",
              "      <td>4.019662</td>\n",
              "      <td>22.171200</td>\n",
              "      <td>3.518100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.048387</td>\n",
              "      <td>9.301105</td>\n",
              "      <td>3.082513</td>\n",
              "      <td>22.146900</td>\n",
              "      <td>3.521900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>5.002597</td>\n",
              "      <td>2.828414</td>\n",
              "      <td>22.245800</td>\n",
              "      <td>3.506300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.268145</td>\n",
              "      <td>2.936829</td>\n",
              "      <td>2.340194</td>\n",
              "      <td>22.292200</td>\n",
              "      <td>3.499000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.233871</td>\n",
              "      <td>4.198449</td>\n",
              "      <td>1.365895</td>\n",
              "      <td>22.238700</td>\n",
              "      <td>3.507400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.195565</td>\n",
              "      <td>4.353131</td>\n",
              "      <td>1.281465</td>\n",
              "      <td>22.194800</td>\n",
              "      <td>3.514300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.627016</td>\n",
              "      <td>1.190452</td>\n",
              "      <td>0.929793</td>\n",
              "      <td>22.199000</td>\n",
              "      <td>3.513700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.618952</td>\n",
              "      <td>1.227076</td>\n",
              "      <td>1.123493</td>\n",
              "      <td>22.164300</td>\n",
              "      <td>3.519200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.764113</td>\n",
              "      <td>0.774195</td>\n",
              "      <td>0.890450</td>\n",
              "      <td>22.460700</td>\n",
              "      <td>3.472700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.804435</td>\n",
              "      <td>0.651170</td>\n",
              "      <td>1.023472</td>\n",
              "      <td>22.601100</td>\n",
              "      <td>3.451200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.802419</td>\n",
              "      <td>0.592425</td>\n",
              "      <td>0.530447</td>\n",
              "      <td>22.316500</td>\n",
              "      <td>3.495200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.719758</td>\n",
              "      <td>0.947803</td>\n",
              "      <td>0.734046</td>\n",
              "      <td>22.287800</td>\n",
              "      <td>3.499700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.822581</td>\n",
              "      <td>0.581834</td>\n",
              "      <td>0.691289</td>\n",
              "      <td>22.412500</td>\n",
              "      <td>3.480200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.808468</td>\n",
              "      <td>0.575211</td>\n",
              "      <td>0.216023</td>\n",
              "      <td>22.437100</td>\n",
              "      <td>3.476400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.846774</td>\n",
              "      <td>0.494391</td>\n",
              "      <td>0.803129</td>\n",
              "      <td>22.198000</td>\n",
              "      <td>3.513800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.844758</td>\n",
              "      <td>0.506107</td>\n",
              "      <td>0.303950</td>\n",
              "      <td>22.486100</td>\n",
              "      <td>3.468800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.844758</td>\n",
              "      <td>0.485745</td>\n",
              "      <td>0.291726</td>\n",
              "      <td>22.298200</td>\n",
              "      <td>3.498000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.834677</td>\n",
              "      <td>0.509185</td>\n",
              "      <td>0.311555</td>\n",
              "      <td>22.044200</td>\n",
              "      <td>3.538300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.838710</td>\n",
              "      <td>0.494257</td>\n",
              "      <td>0.451326</td>\n",
              "      <td>22.215500</td>\n",
              "      <td>3.511100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.948**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.847**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7iY6lNR4LJ_"
      },
      "source": [
        "try:\n",
        "    del model, trainer, task\n",
        "except: \n",
        "    pass\n",
        "try:\n",
        "    del train_dl, valid_dl\n",
        "except: \n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MldJzAA27iSm"
      },
      "source": [
        "Our model is more robust now it is not overfitting on the training data let's see if we can improve furthur..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH_pziTJcO22"
      },
      "source": [
        "In this next part, I will be using label smoothing. Label smoothing is known to known to increase the generalization of the models. It reduces the ability of the model of the model to adapt to the training data by adding noise the original labels of the model on the training data.\n",
        "\n",
        "\n",
        "The effectiveness of label smoothing was explored in this [paper](https://arxiv.org/abs/1906.02629#:~:text=Smoothing%20the%20labels%20in%20this,language%20translation%20and%20speech%20recognition.)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5d3yMIxdQla"
      },
      "source": [
        "Here's how we can intorduce Label smoothing with CrossEntropy loss in pytorch -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHCIm40TdNJ1"
      },
      "source": [
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"Cross Entropy Loss with Label Smoothing\"\n",
        "    def __init__(self, eps=0.1, reduction: str = \"mean\", weight=None):\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        store_attr(\"eps, reduction, weight\")\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        c = input.size()[1]\n",
        "        log_preds = F.log_softmax(input, dim=1)\n",
        "        if self.reduction == \"sum\":\n",
        "            loss = -log_preds.sum()\n",
        "        else:\n",
        "            loss = -log_preds.sum(dim=1)\n",
        "            if self.reduction == \"mean\":\n",
        "                loss = loss.mean()\n",
        "        loss = loss * self.eps / c + (1 - self.eps) * F.nll_loss(log_preds, target.long(), weight=self.weight, reduction=self.reduction)\n",
        "        return loss"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYbwR9a7iM-Y"
      },
      "source": [
        "With label smoothing we generally need to train more epochs to get decent results, so I will train increasing the number of epochs -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lo6XVn1KgaNg",
        "outputId": "9759413b-0c90-4217-b4e7-42c2a6c6b72a"
      },
      "source": [
        "exp_name = \"stage-01-xResModelv2-aug-wd-labelsmooth\"\n",
        "train_dl, valid_dl = get_data(transforms=aug_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "# instantiate the model\n",
        "model = MxResModelv2(num_outputs=len(CLASS_MAP), act_cls=Mish)\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03, wd=1e-01, criterion=LabelSmoothingCrossEntropy())\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=30, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type                       | Params\n",
            "---------------------------------------------------------\n",
            "0 | model     | MxResModelv2               | 1.4 M \n",
            "1 | criterion | LabelSmoothingCrossEntropy | 0     \n",
            "---------------------------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.663     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='1860' max='1860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1860/1860 11:10, Epoch 29 {'loss': '0.961', 'v_num': 15}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>4.348478</td>\n",
              "      <td>4.349501</td>\n",
              "      <td>22.369300</td>\n",
              "      <td>3.486900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.050403</td>\n",
              "      <td>4.108320</td>\n",
              "      <td>4.202126</td>\n",
              "      <td>22.818500</td>\n",
              "      <td>3.418300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.018145</td>\n",
              "      <td>7.528708</td>\n",
              "      <td>3.873045</td>\n",
              "      <td>22.458500</td>\n",
              "      <td>3.473100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.080645</td>\n",
              "      <td>4.971634</td>\n",
              "      <td>3.532441</td>\n",
              "      <td>22.341800</td>\n",
              "      <td>3.491200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.114919</td>\n",
              "      <td>4.402682</td>\n",
              "      <td>2.760030</td>\n",
              "      <td>22.191100</td>\n",
              "      <td>3.514900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.086694</td>\n",
              "      <td>7.028906</td>\n",
              "      <td>2.763040</td>\n",
              "      <td>22.295200</td>\n",
              "      <td>3.498500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.225806</td>\n",
              "      <td>4.075784</td>\n",
              "      <td>2.738441</td>\n",
              "      <td>22.115500</td>\n",
              "      <td>3.526900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.282258</td>\n",
              "      <td>3.424745</td>\n",
              "      <td>1.986200</td>\n",
              "      <td>22.026100</td>\n",
              "      <td>3.541200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.600806</td>\n",
              "      <td>2.140024</td>\n",
              "      <td>2.239122</td>\n",
              "      <td>22.276900</td>\n",
              "      <td>3.501400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.338710</td>\n",
              "      <td>3.013408</td>\n",
              "      <td>1.498490</td>\n",
              "      <td>22.195300</td>\n",
              "      <td>3.514300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.512097</td>\n",
              "      <td>2.415886</td>\n",
              "      <td>1.710688</td>\n",
              "      <td>22.399600</td>\n",
              "      <td>3.482200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.540323</td>\n",
              "      <td>2.263605</td>\n",
              "      <td>1.416689</td>\n",
              "      <td>23.052100</td>\n",
              "      <td>3.383600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.709677</td>\n",
              "      <td>1.644592</td>\n",
              "      <td>1.767893</td>\n",
              "      <td>22.660500</td>\n",
              "      <td>3.442100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.760081</td>\n",
              "      <td>1.551725</td>\n",
              "      <td>1.353355</td>\n",
              "      <td>22.060400</td>\n",
              "      <td>3.535700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.782258</td>\n",
              "      <td>1.528853</td>\n",
              "      <td>1.256292</td>\n",
              "      <td>22.261100</td>\n",
              "      <td>3.503900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.772177</td>\n",
              "      <td>1.549260</td>\n",
              "      <td>1.312977</td>\n",
              "      <td>22.385200</td>\n",
              "      <td>3.484400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.822581</td>\n",
              "      <td>1.425406</td>\n",
              "      <td>1.318811</td>\n",
              "      <td>22.019000</td>\n",
              "      <td>3.542400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.830645</td>\n",
              "      <td>1.396279</td>\n",
              "      <td>1.377984</td>\n",
              "      <td>22.048900</td>\n",
              "      <td>3.537600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.802419</td>\n",
              "      <td>1.403610</td>\n",
              "      <td>1.181906</td>\n",
              "      <td>21.916300</td>\n",
              "      <td>3.559000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.838710</td>\n",
              "      <td>1.331786</td>\n",
              "      <td>1.210897</td>\n",
              "      <td>22.389200</td>\n",
              "      <td>3.483800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.834677</td>\n",
              "      <td>1.333761</td>\n",
              "      <td>0.979657</td>\n",
              "      <td>23.084300</td>\n",
              "      <td>3.378900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.822581</td>\n",
              "      <td>1.344080</td>\n",
              "      <td>1.119964</td>\n",
              "      <td>22.477300</td>\n",
              "      <td>3.470200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.832661</td>\n",
              "      <td>1.326540</td>\n",
              "      <td>1.002003</td>\n",
              "      <td>22.288300</td>\n",
              "      <td>3.499600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.840726</td>\n",
              "      <td>1.332387</td>\n",
              "      <td>0.988811</td>\n",
              "      <td>22.252500</td>\n",
              "      <td>3.505200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.842742</td>\n",
              "      <td>1.323753</td>\n",
              "      <td>1.075783</td>\n",
              "      <td>22.151100</td>\n",
              "      <td>3.521300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.846774</td>\n",
              "      <td>1.325418</td>\n",
              "      <td>0.942842</td>\n",
              "      <td>22.135100</td>\n",
              "      <td>3.523800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.834677</td>\n",
              "      <td>1.330165</td>\n",
              "      <td>1.011587</td>\n",
              "      <td>21.868200</td>\n",
              "      <td>3.566800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.844758</td>\n",
              "      <td>1.323250</td>\n",
              "      <td>0.967449</td>\n",
              "      <td>22.411300</td>\n",
              "      <td>3.480400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.842742</td>\n",
              "      <td>1.324941</td>\n",
              "      <td>0.945278</td>\n",
              "      <td>22.307800</td>\n",
              "      <td>3.496500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.840726</td>\n",
              "      <td>1.317780</td>\n",
              "      <td>0.960689</td>\n",
              "      <td>22.386500</td>\n",
              "      <td>3.484200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.988**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.847**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxxAQlWx-Uva"
      },
      "source": [
        "def init_cnn(m):\n",
        "    \"apply kaiming_normal initialization to the weights of a cnn\"\n",
        "    if getattr(m, 'bias', None) is not None: \n",
        "        nn.init.constant_(m.bias, 0)\n",
        "    if isinstance(m, (nn.Conv1d,nn.Conv2d,nn.Conv3d,nn.Linear)): \n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "    for l in m.children(): \n",
        "        init_cnn(l)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mvChMKzl9zEl",
        "outputId": "9b354f7d-3f8d-4d74-c22f-6e2b8357bf58"
      },
      "source": [
        "exp_name = \"stage-02-xResModelv2-aug-wd-labelsmooth\"\n",
        "train_dl, valid_dl = get_data(transforms=aug_tfms, valid_transforms=base_tfms)\n",
        "\n",
        "# instantiate the model\n",
        "model = MxResModelv2(num_outputs=len(CLASS_MAP), act_cls=nn.ReLU)\n",
        "init_cnn(model)\n",
        "\n",
        "# Put the model into Lightning-Task\n",
        "task = ClassificationTask(model, lr=3e-03, wd=1e-01, criterion=LabelSmoothingCrossEntropy())\n",
        "\n",
        "cbs = [\n",
        "    ModelCheckpoint(monitor=\"val_acc\", filename=exp_name, dirpath=os.getcwd()), \n",
        "    NotebookProgressCallback(),\n",
        "]\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=30, callbacks=cbs, gpus=1, precision=16, deterministic=True)\n",
        "trainer.fit(task, train_dataloader=train_dl, val_dataloaders=valid_dl)\n",
        "\n",
        "# Evalute the final performance of the Model\n",
        "tst_res = trainer.test(ckpt_path=\"best\", test_dataloaders=[train_dl, valid_dl], verbose=False)\n",
        "trn_acc, val_acc = tst_res[0][\"test_acc/dataloader_idx_0\"], tst_res[1][\"test_acc/dataloader_idx_1\"]\n",
        "\n",
        "display(Markdown(f\"**Final Training Accuracy: {round(trn_acc, 3)}**\"))\n",
        "display(Markdown(f\"**Final Validation Accuracy: {round(val_acc, 3)}**\"))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "Using native 16bit precision.\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name      | Type                       | Params\n",
            "---------------------------------------------------------\n",
            "0 | model     | MxResModelv2               | 1.4 M \n",
            "1 | criterion | LabelSmoothingCrossEntropy | 0     \n",
            "---------------------------------------------------------\n",
            "1.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "1.4 M     Total params\n",
            "5.663     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            progress {\n",
              "                border: none;\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      Training\n",
              "      <progress value='1860' max='1860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1860/1860 11:12, Epoch 29 {'loss': '1.09', 'v_num': 16}]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>val_acc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>time</th>\n",
              "      <th>samples/s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.014113</td>\n",
              "      <td>4.265422</td>\n",
              "      <td>5.825770</td>\n",
              "      <td>22.397100</td>\n",
              "      <td>3.482600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.024194</td>\n",
              "      <td>4.374311</td>\n",
              "      <td>5.736481</td>\n",
              "      <td>22.086200</td>\n",
              "      <td>3.531600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.038306</td>\n",
              "      <td>4.938657</td>\n",
              "      <td>4.928933</td>\n",
              "      <td>22.436900</td>\n",
              "      <td>3.476400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.054435</td>\n",
              "      <td>4.110444</td>\n",
              "      <td>4.495432</td>\n",
              "      <td>22.148000</td>\n",
              "      <td>3.521800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>4.404708</td>\n",
              "      <td>4.163874</td>\n",
              "      <td>22.426200</td>\n",
              "      <td>3.478100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.094758</td>\n",
              "      <td>4.556996</td>\n",
              "      <td>4.219239</td>\n",
              "      <td>22.090400</td>\n",
              "      <td>3.530900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>3.668754</td>\n",
              "      <td>3.526898</td>\n",
              "      <td>22.396000</td>\n",
              "      <td>3.482800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.235887</td>\n",
              "      <td>3.101495</td>\n",
              "      <td>3.047855</td>\n",
              "      <td>22.860000</td>\n",
              "      <td>3.412100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.318548</td>\n",
              "      <td>2.821846</td>\n",
              "      <td>2.892351</td>\n",
              "      <td>22.138000</td>\n",
              "      <td>3.523400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.459677</td>\n",
              "      <td>2.389835</td>\n",
              "      <td>2.602592</td>\n",
              "      <td>22.673200</td>\n",
              "      <td>3.440200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.332661</td>\n",
              "      <td>2.817755</td>\n",
              "      <td>1.984826</td>\n",
              "      <td>22.569200</td>\n",
              "      <td>3.456000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.701613</td>\n",
              "      <td>1.746209</td>\n",
              "      <td>2.242778</td>\n",
              "      <td>22.504800</td>\n",
              "      <td>3.465900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.576613</td>\n",
              "      <td>2.065934</td>\n",
              "      <td>1.960177</td>\n",
              "      <td>22.326900</td>\n",
              "      <td>3.493500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.585303</td>\n",
              "      <td>1.430712</td>\n",
              "      <td>22.087900</td>\n",
              "      <td>3.531400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.727823</td>\n",
              "      <td>1.712558</td>\n",
              "      <td>1.667583</td>\n",
              "      <td>22.397600</td>\n",
              "      <td>3.482500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.816532</td>\n",
              "      <td>1.412966</td>\n",
              "      <td>1.532561</td>\n",
              "      <td>22.435400</td>\n",
              "      <td>3.476700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.820565</td>\n",
              "      <td>1.431967</td>\n",
              "      <td>1.550118</td>\n",
              "      <td>22.583900</td>\n",
              "      <td>3.453800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.629032</td>\n",
              "      <td>1.929563</td>\n",
              "      <td>1.418655</td>\n",
              "      <td>22.365700</td>\n",
              "      <td>3.487500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.810484</td>\n",
              "      <td>1.416453</td>\n",
              "      <td>1.498399</td>\n",
              "      <td>22.535800</td>\n",
              "      <td>3.461200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.832661</td>\n",
              "      <td>1.389648</td>\n",
              "      <td>1.407748</td>\n",
              "      <td>22.415200</td>\n",
              "      <td>3.479800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.844758</td>\n",
              "      <td>1.291006</td>\n",
              "      <td>1.492410</td>\n",
              "      <td>22.582600</td>\n",
              "      <td>3.454000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.850806</td>\n",
              "      <td>1.304508</td>\n",
              "      <td>1.269582</td>\n",
              "      <td>22.763400</td>\n",
              "      <td>3.426500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.854839</td>\n",
              "      <td>1.258499</td>\n",
              "      <td>1.167731</td>\n",
              "      <td>22.453700</td>\n",
              "      <td>3.473800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.846774</td>\n",
              "      <td>1.255285</td>\n",
              "      <td>1.056723</td>\n",
              "      <td>22.650100</td>\n",
              "      <td>3.443700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.848790</td>\n",
              "      <td>1.246837</td>\n",
              "      <td>1.082078</td>\n",
              "      <td>22.503000</td>\n",
              "      <td>3.466200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.854839</td>\n",
              "      <td>1.252134</td>\n",
              "      <td>1.122165</td>\n",
              "      <td>22.636700</td>\n",
              "      <td>3.445700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.858871</td>\n",
              "      <td>1.240015</td>\n",
              "      <td>1.056244</td>\n",
              "      <td>22.239400</td>\n",
              "      <td>3.507300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.858871</td>\n",
              "      <td>1.236651</td>\n",
              "      <td>1.054304</td>\n",
              "      <td>22.065900</td>\n",
              "      <td>3.534900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.856855</td>\n",
              "      <td>1.233776</td>\n",
              "      <td>1.003035</td>\n",
              "      <td>22.018500</td>\n",
              "      <td>3.542500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.858871</td>\n",
              "      <td>1.235082</td>\n",
              "      <td>1.217275</td>\n",
              "      <td>22.420200</td>\n",
              "      <td>3.479000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Training Accuracy: 0.973**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Final Validation Accuracy: 0.859**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Bm0HrZBck5H"
      },
      "source": [
        "try:\n",
        "    del model, trainer, task\n",
        "except: \n",
        "    pass\n",
        "try:\n",
        "    del train_dl, valid_dl\n",
        "except: \n",
        "    pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ensl6dWVBeQk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}